{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50584e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torchaudio in /opt/conda/lib/python3.8/site-packages (0.10.2)\n",
      "Requirement already satisfied: torch==1.10.2 in /opt/conda/lib/python3.8/site-packages (from torchaudio) (1.10.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.10.2->torchaudio) (4.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee7e43ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pydub in /opt/conda/lib/python3.8/site-packages (0.25.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.8/site-packages (0.19.2)\n",
      "Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.8/site-packages (from scikit-image) (2.7)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.8/site-packages (from scikit-image) (2022.2.9)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from scikit-image) (9.0.1)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /opt/conda/lib/python3.8/site-packages (from scikit-image) (2.16.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.8/site-packages (from scikit-image) (1.21.4)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from scikit-image) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from scikit-image) (21.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from scikit-image) (1.6.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->scikit-image) (3.0.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pydub\n",
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc9dce9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../lib'))\n",
    "#import config\n",
    "import config_DK_AST\n",
    "#import config\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "from torchvision import datasets\n",
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "import skimage.util\n",
    "import pickle\n",
    "import math\n",
    "import collections\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from scipy import signal\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter , filtfilt\n",
    "import scipy.io as sio\n",
    "import scipy.io.wavfile\n",
    "import copy\n",
    "from PyTorch import config_pytorch\n",
    "from PyTorch.runTorch_AST_DK import ASTModel\n",
    "from evaluate import get_results\n",
    "from pydub import AudioSegment\n",
    "import math\n",
    "import pathlib\n",
    "import random\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f42705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cd3fcef",
   "metadata": {},
   "source": [
    "## Few Global params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "593a141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.path.pardir, 'data', 'audio')\n",
    "plot_dir = os.path.join(os.path.pardir, 'outputs',  'plots')\n",
    "model_dir = os.path.join(os.path.pardir, 'outputs', 'models') # Model sub-directory created in config_keras or config_pytorch\n",
    "# this is the batch_size of the dataloader\n",
    "loader_bat_size = 4\n",
    "\n",
    "# set this to True if you want to call the bandpass filter\n",
    "call_filter = False\n",
    "# This is the duration of the max. len train file. Due to randomization the length should decrease.\n",
    "max_train_mel_len = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b323b08b",
   "metadata": {},
   "source": [
    "## Class Definitions for Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "908a0023",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://jonathanbgn.com/2021/08/30/audio-augmentation.html\n",
    "class RandomBackgroundNoise:\n",
    "    def __init__(self,  noise_dir ,sample_rate=config_DK_AST.rate, min_snr_db=0, max_snr_db=10):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.min_snr_db = min_snr_db\n",
    "        self.max_snr_db = max_snr_db\n",
    "\n",
    "        if not os.path.exists(noise_dir):\n",
    "            raise IOError(f'Noise directory `{noise_dir}` does not exist')\n",
    "        # find all WAV files including in sub-folders:\n",
    "        self.noise_files_list = list(pathlib.Path(noise_dir).glob('**/*.wav'))\n",
    "        if len(self.noise_files_list) == 0:\n",
    "            raise IOError(f'No .wav file found in the noise directory `{noise_dir}`')\n",
    "\n",
    "    def __call__(self, audio_data):\n",
    "        random_noise_file = random.choice(self.noise_files_list)\n",
    "        effects = [\n",
    "            ['remix', '1'], # convert to mono\n",
    "            ['rate', str(self.sample_rate)], # resample\n",
    "        ]\n",
    "        noise, _ = torchaudio.sox_effects.apply_effects_file(random_noise_file, effects, normalize=True)\n",
    "        audio_length = audio_data.shape[-1]\n",
    "        noise_length = noise.shape[-1]\n",
    "        if noise_length > audio_length:\n",
    "            offset = random.randint(0, noise_length-audio_length)\n",
    "            noise = noise[..., offset:offset+audio_length]\n",
    "        elif noise_length < audio_length:\n",
    "            noise = torch.cat([noise, torch.zeros((noise.shape[0], audio_length-noise_length))], dim=-1)\n",
    "\n",
    "        snr_db = random.randint(self.min_snr_db, self.max_snr_db)\n",
    "        snr = math.exp(snr_db / 10)\n",
    "        audio_power = audio_data.norm(p=2)\n",
    "        noise_power = noise.norm(p=2)\n",
    "        scale = snr * noise_power / audio_power\n",
    "\n",
    "        return (scale * audio_data + noise ) / 2\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb2e445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSpeedChange:\n",
    "    def __init__(self, sample_rate = config_DK_AST.rate):\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "    def __call__(self, audio_data):\n",
    "        speed_factor = random.choice([0.9, 1.0, 1.1])\n",
    "        if speed_factor == 1.0: # no change\n",
    "            return audio_data\n",
    "\n",
    "        # change speed and resample to original rate:\n",
    "        sox_effects = [\n",
    "            [\"speed\", str(speed_factor)],\n",
    "            [\"rate\", str(self.sample_rate)],\n",
    "        ]\n",
    "        transformed_audio, _ = torchaudio.sox_effects.apply_effects_tensor(\n",
    "            audio_data, self.sample_rate, sox_effects)\n",
    "        return transformed_audio\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e362c85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComposeTransform:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, audio_data):\n",
    "        for t in self.transforms:\n",
    "            audio_data = t(audio_data)\n",
    "        return audio_data\n",
    "\n",
    "# 2 kind of transformations on the incoming training data\n",
    "compose_transform_aug_noise = ComposeTransform([\n",
    "    #RandomClip(sample_rate=sample_rate, sequence_length=64000),\n",
    "    RandomSpeedChange(config_DK_AST.rate  ),\n",
    "    RandomBackgroundNoise(noise_dir = \"../data/audio/noise\" )])\n",
    "\n",
    "compose_transform_aug_signal = ComposeTransform([\n",
    "    #RandomClip(sample_rate=sample_rate, sequence_length=64000),\n",
    "    RandomSpeedChange(config_DK_AST.rate  ),\n",
    "    RandomBackgroundNoise(noise_dir = \"../data/audio/signal\" , min_snr_db=5, max_snr_db=20)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04408ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioUtil(ComposeTransform,RandomSpeedChange,RandomBackgroundNoise):\n",
    "## Function to apply Spectograms    \n",
    "    @staticmethod\n",
    "    def spec_augment(feat, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "        print(\"......APplying spec augment.....\")\n",
    "        _, n_mels, n_steps = feat.shape\n",
    "        #print(\"n_mels = \" + str(n_mels))\n",
    "        #print(\"n_steps = \" + str(n_steps))\n",
    "        mask_value = feat.mean()\n",
    "        #print(\"mask_val = \" + str(mask_value))\n",
    "        feat_aug = feat\n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "        \n",
    "        for _ in range(n_freq_masks):\n",
    "            feat_aug = T.FrequencyMasking(freq_mask_param)(feat_aug, mask_value)\n",
    "            \n",
    "        \n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "        for _ in range(n_time_masks):\n",
    "            feat_aug = T.TimeMasking(time_mask_param)(feat_aug, mask_value)\n",
    "            \n",
    "        return feat_aug\n",
    "    \n",
    "    # This function gets the expected number of spectograms for a file.\n",
    "    # THis is needed because randomization/length and padding at times add an extra spectogram and we need to compare this expected number with the \n",
    "    # one that is returned from reshape\n",
    "    \n",
    "    ## Function that takes a file names and returns a list of spectograms\n",
    "    @staticmethod\n",
    "    def get_train_feat_list(filename_str,label, min_duration = config_DK_AST.min_duration):\n",
    "        \n",
    "        filename = os.path.join(data_dir, filename_str)\n",
    "        print(\"inside get feat list , processing ->\" + str(filename))\n",
    "        label_duration = librosa.get_duration(filename = filename)\n",
    "        if label_duration >= min_duration:\n",
    "            #print(\"filename = \" + str(filename))\n",
    "            #call the function to process the sound file\n",
    "            feat_aug, y_temp , bugs = AudioUtil.process_wav (filename,label_duration,label )\n",
    "            if (y_temp != \"FAIL\" ):\n",
    "                #print(\"File processeing successful . SHape being returned is   \" + str(feat_aug.shape))\n",
    "                final_feat_list = reshape_feat_pt_no_pad(feat_aug,label)\n",
    "                #print(\"final_feat_list length = \" + str(len(final_feat_list)))\n",
    "                return (final_feat_list,\"success\")\n",
    "                    \n",
    "                           \n",
    "            #when process_wav has processing issues    \n",
    "            else:\n",
    "                print(\"a failure has occured during processing, return a tensor of zeros\")\n",
    "                feat_aug = torch.zeros(1, config_DK_AST.win_size,config_DK_AST.n_feat)\n",
    "                #print(\"Shape being returned from process_wav = \" + str(feat_aug.shape))\n",
    "                #final_feat_list = reshape_feat_pt_no_pad(feat_aug,max_length = max_length)\n",
    "                #print(\"final_feat_list length , returning from reshape = \" + str(len(final_feat_list)))\n",
    "                final_feat_list = list(feat_aug)\n",
    "                return final_feat_list,999\n",
    "                \n",
    "                \n",
    "        #when there is padding needed and         \n",
    "        else:\n",
    "            print(\"inside shorter files\")\n",
    "            size_to_pad = min_duration - label_duration \n",
    "            #print(\"size_to_pad = \" + str(size_to_pad))\n",
    "            #calling process_wav first to ensure that the 'signal' is centered\n",
    "            feat, y_temp , bugs = AudioUtil.process_wav (filename,label_duration,label,pad = True,filter_signal = call_filter )\n",
    "            #print(\"shape of returned file from process_wav -> \" + str(feat.shape))\n",
    "            # adding tolerance because the randomization from augmentation sometimes reduces the file size < window size.\n",
    "            # This causes downstream problem in reshape\n",
    "            size = config_DK_AST.rate*(size_to_pad + .25*label_duration) + feat.shape[1]\n",
    "            #print(\"size with which the file will be appende -> \" + str(size))\n",
    "            #print(\"about to apply padding\")\n",
    "            feat_padded = librosa.util.pad_center(feat, size=size , axis = 1,mode = 'mean')\n",
    "            #print(\"shape of padded file = \" + str(feat_padded.shape) + \"and its type = \" + str(type(feat_padded)))\n",
    "            #convert to mel scale\n",
    "            mel_spectrogram = T.MelSpectrogram(sample_rate=16000, n_fft=config_DK_AST.NFFT,win_length=config_DK_AST.win_size, \n",
    "                                                       hop_length=config_DK_AST.n_hop, center=True,\n",
    "                                                       pad_mode=\"reflect\", power=2.0, norm='slaney', onesided=True, n_mels=config_DK_AST.n_feat,\n",
    "                                                       mel_scale=\"htk\")\n",
    "            \n",
    "            melspec_temp = mel_spectrogram(torch.tensor(feat_padded))\n",
    "            #print(\"After melspectrogram . Shape of feat ->\" + str(melspec_temp.shape))\n",
    "            try:\n",
    "                db_obj= torchaudio.transforms.AmplitudeToDB(stype= 'power')\n",
    "                feat = db_obj(melspec_temp)\n",
    "                #print(\"after converting to mel scale , shape of feat = \" + str(feat.shape))\n",
    "                #print(\" feat[2] =\" + str(feat.shape[2]))\n",
    "                    \n",
    "                if feat.shape[2] < config_DK_AST.win_size :\n",
    "                    #print(\"After power_to_db . Shape of feat ->\" + str(feat.shape))\n",
    "                    raise ValueError\n",
    "                        \n",
    "                else:\n",
    "                    feat_aug = AudioUtil.spec_augment(feat, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1)\n",
    "                    #print(\"POst spec_augment ...for filename = \" + str(filename) + \"shape of feat_aug = \" + str(feat_aug.shape))\n",
    "                    #librosa.display.specshow(feat)\n",
    "                    #when the file is shorter than min duration then we'd return only 1 spectogram\n",
    "                    final_feat_list = reshape_feat_pt_no_pad(feat_aug,label)\n",
    "                    return final_feat_list,\"success\"\n",
    "                    \n",
    "            except ValueError:\n",
    "                print(\"!!!!ValueError!!!!!!for filename \" + str(filename) + \"FILE SIZE < Win size \")\n",
    "                final_feat_list = torch.zeros(1, config_DK_AST.win_size,config_DK_AST.n_feat)\n",
    "                #print(\"final_feat_list length , returning from reshape = \" + str(len(final_feat_list)))\n",
    "                final_feat_list = list(final_feat_list)\n",
    "                return final_feat_list,999\n",
    "    \n",
    "    ### Function to get the list of test features\n",
    "    @staticmethod\n",
    "    def get_test_fest_list(filename_str,label,min_duration = config_DK_AST.min_duration):\n",
    "        filename = os.path.join(data_dir, filename_str)\n",
    "        print(\"....inside get_test_fest_list....\")\n",
    "        label_duration = librosa.get_duration(filename = filename)\n",
    "        if label_duration > min_duration:\n",
    "            #print(\"AUdio duration of File \" + str(filename) + \"is more than min duration\")\n",
    "            signal_8k, rate = torchaudio.load(filename , normalize = True)\n",
    "            #print(\"Signal 8K shape = \" + str(signal_8k.shape))\n",
    "            # resampling to 16 K as needed by AST\n",
    "            transform = T.Resample(orig_freq = 8000,new_freq = 16000)\n",
    "            signal_file = transform(signal_8k)\n",
    "            #print(\"Signal 16K shape = \" + str(signal_file.shape))\n",
    "            #converting to mel scale\n",
    "            mel_spectrogram = T.MelSpectrogram(sample_rate=16000, n_fft=config_DK_AST.NFFT,win_length=config_DK_AST.win_size, \n",
    "                                                   hop_length=config_DK_AST.n_hop, center=True,\n",
    "                                                   pad_mode=\"reflect\", power=2.0, norm='slaney', onesided=True, n_mels=config_DK_AST.n_feat,\n",
    "                                                   mel_scale=\"htk\")\n",
    "            melspec = mel_spectrogram(signal_file)\n",
    "            db_obj= torchaudio.transforms.AmplitudeToDB(stype= 'power')\n",
    "            feat = db_obj(melspec)\n",
    "            #print(\"After melspectrogram . Shape of feat ->\" + str(melspec.shape))\n",
    "            #feat = librosa.power_to_db(feat, ref=np.max)\n",
    "            #print(\"After power_to_db . Shape of feat ->\" + str(feat.shape))\n",
    "                            \n",
    "            # split into winsize x labels\n",
    "            if feat.shape[2] < config_DK_AST.win_size:\n",
    "                print(\"File->\" + str(filename) + \"is smaller than win size. Returning a 999 label\")\n",
    "                temp_feat = torch.zeros(1, config_DK_AST.win_size,config_DK_AST.n_feat)\n",
    "                test_feat_list = list(temp_feat)\n",
    "                return test_feat_list,999\n",
    "                \n",
    "            else:\n",
    "                test_feat_list = reshape_feat_pt_no_pad(feat, label,size = config_DK_AST.win_size, step = config_DK_AST.step_size )\n",
    "                #print(\"length of list from reshape = \" + str(len(test_feat_list)))\n",
    "                print(\"file \" + str(filename) + \"will have \" + str(len(test_feat_list)) + \" specgrams\")\n",
    "                return test_feat_list,\"success\"\n",
    "                        \n",
    "                \n",
    "        else:            \n",
    "            #print(\"Audio duration of File \" + str(filename) + \"is less than min duration\")\n",
    "            signal_8k, rate = torchaudio.load(filename , normalize = True)\n",
    "            # resampling to 16 K as needed by AST\n",
    "            transform = T.Resample(orig_freq = 8000,new_freq = 16000)\n",
    "            signal_file = transform(signal_8k)\n",
    "            #print(\"shape of signal file = \" + str(signal_file.shape))\n",
    "            num_rows, sig_len = signal_file.shape\n",
    "            # the code below is to pad with 0\n",
    "            len_begin = int(round(min_duration*1000 - label_duration*1000 ))\n",
    "            pad_begin_len = random.randint(0,len_begin)\n",
    "            #print(\"pad_begin_len = \" + str(pad_begin_len))\n",
    "            pad_end_len = (min_duration*1000 - label_duration*1000 - pad_begin_len)*(config_DK_AST.sr/1000)\n",
    "            #print(\"pad_end_len = \" + str(pad_end_len))\n",
    "            # Pad with 0s\n",
    "            pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "            #print(\"pad_begin = \" +str(pad_begin))\n",
    "            pad_end = torch.zeros((num_rows, int(pad_end_len)))\n",
    "            #print(\"pad_end = \" +str(pad_end))\n",
    "            signal_appended = torch.cat((pad_begin, signal_file, pad_end), 1)\n",
    "            #print(\"signal_appended shape = \" + str(signal_appended.shape))\n",
    "            \n",
    "            # now covert to mel scale\n",
    "            mel_spectrogram = T.MelSpectrogram(sample_rate=16000, n_fft=config_DK_AST.NFFT,win_length=config_DK_AST.win_size, \n",
    "                                                   hop_length=config_DK_AST.n_hop, center=True,\n",
    "                                                   pad_mode=\"reflect\", power=2.0, norm='slaney', onesided=True, n_mels=config_DK_AST.n_feat,\n",
    "                                                   mel_scale=\"htk\")\n",
    "            melspec = mel_spectrogram(signal_appended)\n",
    "            db_obj= torchaudio.transforms.AmplitudeToDB(stype= 'power')\n",
    "            feat = db_obj(melspec)\n",
    "            \n",
    "            try:\n",
    "                if feat.shape[2] < config_DK_AST.win_size:\n",
    "                    #print(\"After melspectrogram . Shape of feat ->\" + str(melspec.shape))\n",
    "                    #print(\"File->\" + str(filename) + \"being skipped because its < config_size\")\n",
    "                    #print return with a 999 label indicating that this is an invalid input\n",
    "                    raise ValueError                 \n",
    "                else:\n",
    "                    #print(\"After power_to_db . Shape of feat ->\" + str(feat.shape))\n",
    "                    test_feat_list = reshape_feat_pt_no_pad(feat, label,size = config_DK_AST.win_size, step = config_DK_AST.step_size )\n",
    "                    #print(\"file \" + str(filename) + \"will have \" + str(len(test_feat_list)) + \" specgrams\")\n",
    "                    test_feat_list = test_feat_list[0:1]\n",
    "                    return test_feat_list,\"success\"\n",
    "                    \n",
    "                    \n",
    "            except ValueError:\n",
    "                print(\" ValueError .for filename \" + str(filename) + \"FILE SIZE < Win size. Returning 999 \")\n",
    "                test_feat_list = torch.zeros(1, config_DK_AST.win_size,config_DK_AST.n_feat)\n",
    "                #test_feat_list = reshape_feat_pt_no_pad(feat, size = config_DK_AST.win_size, step = config_DK_AST.step_size,max_length = None )\n",
    "                test_feat_list = list(test_feat_list)\n",
    "                return test_feat_list,999\n",
    "                \n",
    "                \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### Function to process the wav file #########\n",
    "    @staticmethod\n",
    "    def process_wav (filename,label_duration,label,pad = False,filter_signal = call_filter ):\n",
    "        print(\"****** ENTERING PROCESS_WAV for file = \" + str(filename) + \"*****\")\n",
    "        #length = librosa.get_duration(filename = filename)\n",
    "        bugs = 0    \n",
    "        #to resamp to 16Khz\n",
    "        # The below condition check any mismatch in labels\n",
    "        #print(\"length from librosa = \" + str(length) )\n",
    "        #  assert math.isclose(length,label_duration, rel_tol=0.01), \"File: %s label duration (%.4f) does not match audio length (%.4f)\" % (row['path'], label_duration, length)\n",
    "        signal_8k, rate = torchaudio.load(filename , normalize = True)\n",
    "        #print(\"shape of signal_8k = \" +str(signal_8k.shape))\n",
    "        # resampling to 16 K as needed by AST\n",
    "        transform = T.Resample(orig_freq = 8000,new_freq = 16000)\n",
    "        signal_file = transform(signal_8k)\n",
    "        #print(\"Post Resampling signal_16k = \" +str(signal_file.shape))\n",
    "        # if the incoming training file is padded or a mosquito - transform it with a mosq sound\n",
    "        \n",
    "        if (label == 1) :\n",
    "            print(\"About to apply signal transform...\")\n",
    "            transformed_audio = compose_transform_aug_signal(signal_file)\n",
    "            #print(\"Filename->\" +str(filename) + \"shape of transformed audio = \" + str(transformed_audio.shape))\n",
    "        else:\n",
    "            print(\"About to apply background transform...\")\n",
    "            transformed_audio = compose_transform_aug_noise(signal_file)\n",
    "        \n",
    "        if pad == True :\n",
    "            # this indicates that the function is being called from padding and the file has to be appended\n",
    "                print(\"****** EXITING PROCESS_WAV (pad == TRUE)*****\")\n",
    "                return transformed_audio , label , None\n",
    "        else:#when pad == false\n",
    "            try:\n",
    "                mel_spectrogram = T.MelSpectrogram(sample_rate=16000, n_fft=config_DK_AST.NFFT,win_length=config_DK_AST.win_size, \n",
    "                                                   hop_length=config_DK_AST.n_hop, center=True,\n",
    "                                                   pad_mode=\"reflect\", power=2.0, norm='slaney', onesided=True, n_mels=config_DK_AST.n_feat,\n",
    "                                                   mel_scale=\"htk\")\n",
    "                melspec = mel_spectrogram(transformed_audio)\n",
    "                db_obj= torchaudio.transforms.AmplitudeToDB(stype= 'power')\n",
    "                feat = db_obj(melspec)\n",
    "                #print(\"After melspectrogram . Shape of feat ->\" + str(melspec.shape))\n",
    "                if feat.shape[2] < config_DK_AST.win_size:\n",
    "                    #print(\"Post Power_db. Shape = \" + str(feat.shape))\n",
    "                    print(\"RAISING VALUE ERROR..Power_db issue\")\n",
    "                    raise ValueError\n",
    "                else:\n",
    "                    feat_aug = AudioUtil.spec_augment(feat, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1)\n",
    "                    #print(\"post spec augment . Shape = \" + str(feat_aug.shape))\n",
    "                    bugs = None\n",
    "                    print(\"****** EXITING PROCESS_WAV*****\")\n",
    "                    return feat_aug ,label,bugs\n",
    "                        \n",
    "                        \n",
    "            except ValueError:\n",
    "                print(\"FILE SIZE TOOO SMALLLL ..error while processing wav..skipping..\")\n",
    "                bugs = filename\n",
    "                feat_aug = -1\n",
    "                y_temp = \"FAIL\"\n",
    "                print(\"****** EXITING PROCESS_WAV*****\")\n",
    "                return feat_aug, y_temp,bugs\n",
    "            \n",
    "  \n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "        \n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536e79cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6d4e29",
   "metadata": {},
   "source": [
    "## Pre Processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999bfb4a",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab9bb5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundDS(Dataset):\n",
    "    \n",
    "    def __init__(self,df,max_dim = None ,sr = config_DK_AST.rate ):\n",
    "        self.df = df\n",
    "        self.sr = sr\n",
    "        self.channel = 1\n",
    "        #self.feat_list = len(final_feat_list)\n",
    "        print(\"len = \" + str(len(self.df)))\n",
    "               \n",
    "  \n",
    "  # Number of items in dataset\n",
    "  # \n",
    "    def __len__(self):\n",
    "        #all_spec_gram = AudioUtil.num_specgrams(self.df)\n",
    "        #print(\"total number of specgram for the dataset is = \" +str(all_spec_gram))\n",
    "        \n",
    "        return len(self.df)\n",
    "    \n",
    "  # ----------------------------\n",
    "  # Get i'th item in dataset\n",
    "  # ----------------------------\n",
    "    def __getitem__(self, idx , win_size = config_DK_AST.win_size , step_size = config_DK_AST.step_size , min_duration = config_DK_AST.min_duration,sr = config_DK_AST.sr):\n",
    "        # Absolute file path of the audio file - concatenate the audio directory with\n",
    "        # the relative path\n",
    "        #get the file details from the idex passed from the train_loop\n",
    "        print(\"inside get_item for train . Idx = \" + str(idx))\n",
    "        file_id = self.df.loc[idx,'id']\n",
    "        label_df = self.df.loc[idx,'sound_type']\n",
    "        \n",
    "        # converting to numeric\n",
    "        if label_df == 'mosquito':\n",
    "            label  = 1\n",
    "        else:\n",
    "            label  = 0\n",
    "        #filetag is the \"name\"   column in the df\n",
    "        filetag = self.df.loc[idx,'name']\n",
    "        _, file_format = os.path.splitext(filetag)\n",
    "        filename_str = str(file_id) + file_format\n",
    "        filename = os.path.join(data_dir, str(file_id) + file_format)\n",
    "        label_duration = self.df.loc[idx,'length']\n",
    "        print(\"^^^^^^^^ Start processing filename = \" + str(filename_str) + \"^^^^^^^^\")\n",
    "        #length = librosa.get_duration(filename = filename)\n",
    "        #print(\"length from librosa = \" + str(length) )\n",
    "        return (filename_str , label)\n",
    "    \n",
    "    \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16762abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a338f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundDS_test(Dataset):\n",
    "    \n",
    "    def __init__(self,df ,sr = config_DK_AST.rate ):\n",
    "        self.df = df\n",
    "        self.sr = sr\n",
    "        self.channel = 1\n",
    "              \n",
    "  # \n",
    "  # Number of items in dataset\n",
    "  # \n",
    "    def __len__(self):\n",
    "        #all_spec_gram = AudioUtil.num_specgrams(self.df)\n",
    "        #print(\"total number of specgram for the dataset is = \" +str(all_spec_gram))\n",
    "        return len(self.df)\n",
    "    \n",
    "  # ----------------------------\n",
    "  # Get i'th item in dataset\n",
    "  # ----------------------------\n",
    "    def __getitem__(self, idx , win_size = config_DK_AST.win_size , step_size = config_DK_AST.step_size , min_duration = config_DK_AST.min_duration,sr = config_DK_AST.sr):\n",
    "        # Absolute file path of the audio file - concatenate the audio directory with\n",
    "        # the relative path\n",
    "        print(\"inside get_item for val/test. Idx = \" +str(idx))\n",
    "        file_id = self.df.loc[idx,'id']\n",
    "        label_df = self.df.loc[idx,'sound_type']\n",
    "        # converting to numeric\n",
    "        if label_df == 'mosquito':\n",
    "            label  = 1\n",
    "        else:\n",
    "            label  = 0\n",
    "           \n",
    "        filetag = self.df.loc[idx,'name']\n",
    "        _, file_format = os.path.splitext(filetag)\n",
    "        filename_str = str(file_id) + file_format\n",
    "        filename = os.path.join(data_dir, str(file_id) + file_format)\n",
    "        label_duration = self.df.loc[idx,'length']\n",
    "        print(\"Test File :- filename = \" + str(filename))\n",
    "        return(filename_str,label)\n",
    "        \n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "819feb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------AST Model Summary---------------\n",
      "ImageNet pretraining: True, AudioSet pretraining: False\n",
      "frequncey stride=10, time stride=10\n",
      "number of patches=84\n"
     ]
    }
   ],
   "source": [
    "def train_model_ast(train_dl,val_dl = None, model = ASTModel()):\n",
    "    # X-train is a list of tensor and y_train is a list\n",
    "    # we create a tuple of X-train and labels . This will help us to create a validation dataset.\n",
    "              \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Training on {device}')\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimiser = optim.Adam(model.parameters(), lr=config_pytorch.lr)\n",
    "    #print(\"Optim Device= \" +str(optimiser.device))\n",
    "    all_train_loss = []\n",
    "    all_train_acc = []\n",
    "    all_val_loss = []\n",
    "    all_val_acc = []\n",
    "    best_val_loss = np.inf\n",
    "    best_val_acc = -np.inf\n",
    "    best_train_acc = -np.inf\n",
    "    best_epoch = -1\n",
    "    checkpoint_name = None\n",
    "    overrun_counter = 0\n",
    "    for e in range(config_pytorch.epochs):\n",
    "        start_epoch_time = time.time()\n",
    "        print(\"epoch = \" +str(e))\n",
    "        train_loss = 0.0\n",
    "        model.to(device).train()\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        total_sgrams = 0\n",
    "        #our data is a list of dim = max_dims\n",
    "        #dl_length  = int(len(train_dl))\n",
    "        #loader_bat_size = min(bat_size,dl_length)\n",
    "        for idx,train_bat in enumerate(train_dl):\n",
    "            start_time = time.time()\n",
    "            print(\"TRain_dl length is \" + str(len(train_dl)))\n",
    "            print(\"idx in train_ast = \" +str(idx))\n",
    "            file_bat = train_bat[0]\n",
    "            label_bat = train_bat[1]\n",
    "            print(\" file bat = \" + str(file_bat))\n",
    "            print(\" label bat = \" + str(label_bat))\n",
    "            for i in range(len(file_bat)):\n",
    "                filename = file_bat[i]\n",
    "                label = label_bat[i]\n",
    "                print(\"Processing the file in train_ast->\" + str(filename))\n",
    "                print(\"filename = \" +str(filename))\n",
    "                print(\"label = \" +str(label))\n",
    "                #print(\"batch number ->\" + str(i) + \"of.. \" + str(loader_bat_size))\n",
    "                spec_list,status = AudioUtil.get_train_feat_list(filename,label)\n",
    "                print(\"(inside train_ast) = len of of spec_list = \" + str(len(spec_list)))\n",
    "                if status == 999:\n",
    "                    print(\"inside y = 999\")\n",
    "                    print(\"Failure occured for file and skipping this rec\")\n",
    "                    all_y.append(status)\n",
    "                    continue\n",
    "                \n",
    "                total_sgrams+=int(len(spec_list))\n",
    "                for j in range(len(spec_list)):\n",
    "                    #print(\"index of spec_list = \" + str(i))\n",
    "                    y = torch.tensor(label).reshape(-1,1).to(device).float()\n",
    "                    #print(\"%%%%%% INSIDE TRAIN LOOP(when there is no 999) TO GET A LOSS%%%%%%%%\")\n",
    "                    #print(\"index of spec_list = \" + str(i))\n",
    "                    x_temp,_ = spec_list[j]\n",
    "                    #print(\"x_temp SHAPE = \" + str(x_temp[0].shape))\n",
    "                    x_temp_new = x_temp[0]\n",
    "                    x_temp_new = x_temp_new.unsqueeze(dim = 0)\n",
    "                    #print(\"shpe of x_temp_new = \" + str(x_temp_new.shape))\n",
    "                    #print(\"y SHAPE = \" + str(y.shape))\n",
    "                    # a zero tensor indicates a padding\n",
    "                    # this is recommended in paper.\n",
    "                    #print(x_reshaped.shape)\n",
    "                    x_temp_new.cuda()\n",
    "                    #print(\"x_temp_new device = \" +str(x_reshaped.device))\n",
    "                    y.cuda()\n",
    "                    #print(\"Y device = \" +str(y.device))\n",
    "                    y_pred = model(x_temp_new.to(device).detach()).float()\n",
    "                    #print(\"Y Pred Device = \" +str(y_pred.device))\n",
    "                    #print(\"Y  shape = \" +str(y.shape))\n",
    "                    #print(\"i = \" + str(i))\n",
    "                    #print(\"@@@@Y_pred = >\" + str(y_pred))\n",
    "                    #print(\"sigmoid of Y_pred = \" +str(m(y_pred)))\n",
    "                    loss = criterion(y_pred, y)\n",
    "                    #print(\"loss  = \" +str(loss.device))\n",
    "                    optimiser.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimiser.step()\n",
    "                    #print(\"Current train loss before adding the loss = \" + str(train_loss))\n",
    "                    train_loss += loss.detach().item()\n",
    "                    #print(\"train loss - >\" + str(loss.item()))\n",
    "                    #print(\"train loss/len(train_loader)-> \" + str(train_loss/len(train_loader)))\n",
    "                    all_y.append(y.cpu().detach())\n",
    "                    all_y_pred.append(y_pred.cpu().detach())\n",
    "                    # if bat_size%100 == 0 :\n",
    "                     #print(\"Inside Epoch \" + str(e) + \" & inside batch \" + str(idx) + \"specgram \" + str(i) + \" of \" + str(len(tup_list))  )\n",
    "                    del x_temp_new\n",
    "                    del y\n",
    "                    del y_pred\n",
    "\n",
    "                    #print(\"Finished training a  batch\")\n",
    "                    finish_time = time.time()\n",
    "                    duration = (finish_time - start_time )/60\n",
    "                    if duration//120 > 1 :\n",
    "                        print(\"Duration of training this file = \" + str(duration)) \n",
    "                    #total_sgrams = len(train_dl) + item_in_bat\n",
    "                    #print(\"total_sgrams = \" + str(total_sgrams))\n",
    "                    all_train_loss.append(train_loss/total_sgrams)    \n",
    "                    #removing all instances of 999\n",
    "\n",
    "        \n",
    "        all_y_999_removed = all_y\n",
    "        while 999 in all_y_999_removed: all_y_999_removed.remove(999)\n",
    "        all_y_999_removed = torch.cat(all_y_999_removed)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        train_acc = accuracy_score(all_y_999_removed.numpy(), (all_y_pred.numpy() > 0.5).astype(float))\n",
    "        all_train_acc.append(train_acc)\n",
    "        \n",
    "        # Can add more conditions to support loss instead of accuracy. Use *-1 for loss inequality instead of acc\n",
    "        if val_dl is not None:\n",
    "            val_loss, val_acc,total_val_size,invalid_file_flag = test_model(model, val_dl, criterion, 0.5, device=device)\n",
    "            #when the val_data returns np.inf as loss and accuracy\n",
    "            if invalid_file_flag == True:\n",
    "                acc_metric = train_acc\n",
    "                best_acc_metric = best_train_acc\n",
    "            else:\n",
    "                all_val_loss.append(val_loss)\n",
    "                all_val_acc.append(val_acc)\n",
    "                acc_metric = val_acc\n",
    "                best_acc_metric = best_val_acc\n",
    "        else:\n",
    "            acc_metric = train_acc\n",
    "            best_acc_metric = best_train_acc\n",
    "        end_epoch_time = time.time()\n",
    "        duration_epoch = (end_epoch_time -start_epoch_time )/60\n",
    "        print(\"duration to train epoch \" + str(e) + \"= \" + str(duration_epoch))\n",
    "        if acc_metric > best_acc_metric:\n",
    "            checkpoint_name = f'model_e{e}_{datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")}.pth'\n",
    "            torch.save(model.state_dict(), os.path.join(config_DK_AST.model_dir, 'pytorch', checkpoint_name))\n",
    "            print('Saving model to:', os.path.join(config_DK_AST.model_dir, 'pytorch', checkpoint_name)) \n",
    "            best_epoch = e\n",
    "            best_train_acc = train_acc\n",
    "            best_train_loss = train_loss\n",
    "            best_val_acc = val_acc\n",
    "            best_val_loss = val_loss\n",
    "            overrun_counter = -1\n",
    "\n",
    "        overrun_counter += 1\n",
    "        if (val_dl is not None) and (invalid_file_flag == False):\n",
    "            print('Epoch: %d, Train Loss: %.8f, Train Acc: %.8f, Val Loss: %.8f, Val Acc: %.8f, overrun_counter %i' % (e, train_loss/(total_sgrams), train_acc, val_loss/(total_val_size), val_acc,  overrun_counter))\n",
    "        else:\n",
    "            print('Epoch: %d, Train Loss: %.8f, Train Acc: %.8f, overrun_counter %i' % (e, train_loss/(total_sgrams), train_acc, overrun_counter))\n",
    "        if overrun_counter > config_pytorch.max_overrun:\n",
    "            break\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ad876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446df191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee83636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e7cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fe731c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3ee65c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49066565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7ebb9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_df):\n",
    "    \n",
    "    test_obj = SoundDS_test(test_df)\n",
    "    test_dl = torch.utils.data.DataLoader(test_df, batch_size=16, shuffle=False)\n",
    "    n_samples = len(test_df)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Evaluating on {device}')\n",
    "    print(\"n_samples = \" +str(n_samples))\n",
    "\n",
    "    #x_test,label = torch.tensor(X_test).float()\n",
    "                \n",
    "    y_preds_all = np.zeros([n_samples, len(y_test), 2])\n",
    "    #print(\"Shape of y_preds_all = \" + str(y_preds_all.shape))\n",
    "    model.eval() # Important to not leak info from batch norm layers and cause other issues\n",
    "\n",
    "    for n in range(n_samples):\n",
    "        all_y_pred = []\n",
    "        all_y = []\n",
    "        \n",
    "        for idx,(x, labels ) in enumerate(test_dl):\n",
    "            # The model needs input in the form of(batchsize, time, freq . Reshaping the features below)           \n",
    "            print(\"bat_idx = \" + str(idx))\n",
    "            feat_temp , labels  =  x, labels\n",
    "            #print(\"shape of feat in the batch = \" + str(feat_temp.shape))\n",
    "            #print(\"Calling reshape ....\")\n",
    "            feat_list =  reshape_feat_pt_temp(feat_temp, labels, size = config_DK_AST.win_size, step = config_DK_AST.step_size)\n",
    "            #This is of the form [1,var,80,128]\n",
    "            #print(\"Batch#->\" + str(idx) + \"will have \" + str(len(feat_list)) + \" elements\")          \n",
    "            one_image_loss = 0\n",
    "            for i in range(0,len(feat_list)):\n",
    "                x_temp,y_elem = feat_list[i]\n",
    "                bat_size = x_temp.shape[0]\n",
    "                time_dim = x_temp.shape[1]\n",
    "                freq_dim = x_temp.shape[2]\n",
    "                                \n",
    "                #print(\"X-temp Shape = \" +str(x_temp.shape))\n",
    "                #x_reshaped = x_temp.reshape(bat_size,time_dim,freq_dim)\n",
    "                \n",
    "            # this is recommended in paper.\n",
    "            #x_reshaped = ((x_reshaped - train_data_mean)/(train_data_sd)).cuda()\n",
    "                y = y_elem.reshape(-1,1).to(device).float()\n",
    "                #print(\"Lables = \" +str(y))\n",
    "                #print(\"Type of Y = \" + str(type(y)))\n",
    "                #print(\"Unsqueeze Y = \" + str(y))\n",
    "                #print(x_reshaped.shape)\n",
    "                optimiser.zero_grad()\n",
    "                x_reshaped.cuda()\n",
    "                #print(\"X_REshaped device = \" +str(x_reshaped.device))\n",
    "                y.cuda()\n",
    "            #print(\"Y device = \" +str(y.device))\n",
    "                y_pred = model(x_reshaped.to(device).detach()).float()\n",
    "                all_y.append(y.cpu().detach())\n",
    "                all_y_pred.append(y_pred.cpu().detach())\n",
    "                del x\n",
    "                del y\n",
    "                del y_pred\n",
    "                all_y_pred = torch.cat(all_y_pred)\n",
    "                all_y = torch.cat(all_y)\n",
    "\n",
    "        y_preds_all[n,:,1] = np.array(all_y_pred)\n",
    "        y_preds_all[n,:,0] = 1-np.array(all_y_pred) # Check ordering of classes (yes/no)\n",
    "        test_acc = accuracy_score(all_y.numpy(), (all_y_pred.numpy() > 0.5).astype(float))\n",
    "        # print(test_acc)\n",
    "    return y_preds_all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c2b525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_aggregated(model, X_test, y_test, n_samples):\n",
    "    ''' Generate predictions for VGGish features (Feat. A) rescaled to time window of 1.92 second features (Feat. B)'''\n",
    "    preds_aggregated_by_mean = []\n",
    "    y_aggregated_prediction_by_mean = []\n",
    "    y_target_aggregated = []\n",
    "    \n",
    "    for idx, recording in enumerate(X_test):\n",
    "        n_target_windows = len(recording)//2  # Calculate expected length: discard edge\n",
    "        y_target = np.repeat(y_test[idx],n_target_windows) # Create y array of correct length\n",
    "        preds = evaluate_model(model, recording, np.repeat(y_test[idx],len(recording)),n_samples) # Sample BNN\n",
    "#         preds = np.mean(preds, axis=0) # Average across BNN samples\n",
    "#         print(np.shape(preds))\n",
    "        preds = preds[:,:n_target_windows*2,:] # Discard edge case\n",
    "#         print(np.shape(preds))\n",
    "#         print('reshaping')\n",
    "        preds = np.mean(preds.reshape(len(preds),-1,2,2), axis=2) # Average every 2 elements, keep samples in first dim\n",
    "#         print(np.shape(preds))\n",
    "        preds_y = np.argmax(preds)  # Append argmax prediction (label output)\n",
    "        y_aggregated_prediction_by_mean.append(preds_y)\n",
    "        preds_aggregated_by_mean.append(preds)  # Append prob (or log-prob/other space)\n",
    "        y_target_aggregated.append(y_target)  # Append y_target\n",
    "#     return preds_aggregated_by_mean, y_aggregated_prediction_by_mean, y_target_aggregated\n",
    "    return np.hstack(preds_aggregated_by_mean), np.concatenate(y_target_aggregated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "704bb255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_feat_pt_no_pad(feats,label,size = config_DK_AST.win_size, step = config_DK_AST.step_size):\n",
    "    #tup_list will hold the values in the form of an image(x,label)\n",
    "    print(\"&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\")\n",
    "    #print(\"initial shape of feat = \" + str(feats.shape))\n",
    "    feat_list = []\n",
    "    feat_first_tpose = torch.transpose(feats,1,2)\n",
    "    print(\"shape after first transpose = \" + str(feat_first_tpose.shape))\n",
    "    feat_new = feat_first_tpose.unfold(1,size  = config_DK_AST.win_size, step = config_DK_AST.step_size)\n",
    "    #print(\"shape after unfolding = \" + str(feat_new.shape))\n",
    "    feat_squee = feat_new.squeeze()\n",
    "    #print(\"shape post squeeze = \" + str(feat_squee.shape))\n",
    "    len_squeeze = list(feat_squee.shape)\n",
    "    if len(len_squeeze) == 2:\n",
    "        if feat_squee.shape[1] < config_DK_AST.win_size :\n",
    "            # 999 indicated error condition\n",
    "            #print(\"inside feat_squee.shape[1] < config_DK_AST.win_size\")\n",
    "            feature_zero = list(torch.zeros(1,config_DK_AST.win_size,config_DK_AST.n_feat))\n",
    "            print(\"returning zeros and 999\")\n",
    "            tup = (feature_zero,999)\n",
    "            feat_list.append(tup)\n",
    "            print(\"returning a feat_list with elements = \" + str(len(feat_list)))\n",
    "            return feat_list\n",
    "            \n",
    "        else:\n",
    "            #print(\"already in desired shape\")\n",
    "            t = torch.transpose(feat_squee,1,0)\n",
    "            t = torch.unsqueeze(t,dim = 0)            \n",
    "            #print(\"shape of feat = \" + str(t.shape))\n",
    "            feature = list(t)\n",
    "            tup = (feature,label)\n",
    "            print(\"returning a feat_list with elements = \" + str(len(feat_list)))\n",
    "            feat_list.append(tup)\n",
    "            return feat_list\n",
    "              \n",
    "    else:\n",
    "        num_chunk = feat_squee.shape[0]\n",
    "        #print(\"Num chunks to be generated = \" + str(num_chunk))\n",
    "        chunk_list = torch.chunk(feat_squee , num_chunk , 0)\n",
    "        #print(\"One sound file being split into  \" + str(len(chunk_list)))\n",
    "        #print(\"LEngth of list of tensors = \" + str(len(chunk_list)))\n",
    "        \n",
    "        for s_gram in (chunk_list):\n",
    "            #print(\"sgram shape =\" + str(torch.transpose(s_gram,2,1).shape))\n",
    "            #print(\"ctr =\" +str(ctr))\n",
    "            feature = torch.transpose(s_gram,2,1)\n",
    "            #print(\"before appending feat_list = \" + str(len(feat_list)))\n",
    "            tup = (list(feature),label)\n",
    "            feat_list.append(tup)\n",
    "            #print(\"POST appending feat_list = \" + str(len(feat_list)))\n",
    "            \n",
    "        print(\"returning a feat_list with elements = \" + str(len(feat_list)))\n",
    "        return feat_list\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b58bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6a5f66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a282909a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1fc459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, val_dl, criterion, class_threshold=0.5, device=None):\n",
    "        \n",
    "    test_loss = 0.0\n",
    "    model.eval()\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        total_sgram_val = 0\n",
    "        print(\"^^^^^ INSIDE VAL DATA^^^^^^^^^\")\n",
    "        for idx,val_batch in enumerate(val_dl):\n",
    "            #print(\"filename without de-listing \" +str(filename))           \n",
    "            print(\"val_dl length is = \" +str(len(val_dl)))\n",
    "            print(\"idx in val_dl = \" +str(idx))\n",
    "            file_bat = val_batch[0]\n",
    "            label_bat = val_batch[1]\n",
    "            print(\" file bat = \" + str(file_bat))\n",
    "            print(\" label bat = \" + str(label_bat))\n",
    "            for i in range(len(file_bat)):\n",
    "                filename = file_bat[i]\n",
    "                label = label_bat[i]\n",
    "                print(\"Processing the file in train_ast->\" + str(filename))\n",
    "                print(\"filename = \" +str(filename))\n",
    "                print(\"label = \" +str(label))\n",
    "                start_time = time.time()\n",
    "                total_sgram_list_val,status = AudioUtil.get_test_fest_list(filename,label)\n",
    "            #print(\"Total num os spec grams returned = \" + str(total_sgram_val))\n",
    "                if status == 999 :\n",
    "                    print(\"999 encountered .Error file..skipping\")\n",
    "                    all_y.append(status)\n",
    "                    print(\"Length of all_y = \" +str(len(all_y)))\n",
    "                    continue\n",
    "                \n",
    "                total_sgram_val+=(len(total_sgram_list_val))\n",
    "                #print(\"Num items in this list = \" + str(len(x_el_val)))\n",
    "                for i in range(len(total_sgram_list_val)):\n",
    "                    y = label.reshape(-1,1).to(device).float()\n",
    "                    x_temp_val,_ = total_sgram_list_val[i]\n",
    "                    x_temp_new = x_temp_val[0].unsqueeze(dim=0)\n",
    "                    #print(\"x_temp SHAPE = \" + str(x_temp_val.shape))\n",
    "                    #print(\"y SHAPE = \" + str(y.shape))\n",
    "                    # a zero tensor indicates a padding\n",
    "                    #bat_size = x_temp.shape[1]\n",
    "                    #time_dim = x_temp.shape[2]\n",
    "                    #freq_dim = x_temp.shape[3]\n",
    "                    #x_reshaped = x_temp.reshape(bat_size,time_dim,freq_dim)\n",
    "                    #print(\"X_REshaped shape = \" +str(x_reshaped.shape))\n",
    "                    x_temp_new.to(device)\n",
    "                    #print(\"X_REshaped device = \" +str(x_reshaped.device))\n",
    "                    #y.to(device)\n",
    "                    #print(\"Y device = \" +str(y.device))\n",
    "                    y_pred = model(x_temp_new.to(device).detach()).float()\n",
    "                    #print(\"Y Pred Device = \" +str(y_pred.device))\n",
    "                    #print(\"Y Pred shape = \" +str(y_pred.shape))\n",
    "                    #print(\"Y  shape = \" +str(y.shape))\n",
    "                    #print(\"@@@@Y_pred in val= >\" + str(y_pred))\n",
    "                    loss = criterion(y_pred, y)\n",
    "                    test_loss += loss.detach().item()\n",
    "                    #print(\"loss  = \" +str(loss.device))\n",
    "                    #print(\"train loss/len(train_loader)-> \" + str(train_loss/len(train_loader)))\n",
    "                    all_y.append(y.cpu().detach())\n",
    "                    all_y_pred.append(y_pred.cpu().detach())\n",
    "                    #if bat_size%100 == 0 :\n",
    "                    #    print(\"Inside Epoch \" + str(e) + \" & inside batch \" + str(idx) + \"specgram \" + str(i) + \" of \" + str(len(item_in_bat))  )\n",
    "                    del x_temp_val\n",
    "                    del x_temp_new\n",
    "                    del y\n",
    "                    del y_pred\n",
    "\n",
    "\n",
    "                #removing instances of999\n",
    "            \n",
    "        all_y_999_removed = all_y\n",
    "        while 999 in all_y_999_removed: all_y_999_removed.remove(999)\n",
    "        #print(\"all_y_999_removed = \" + str(all_y_999_removed))\n",
    "        #print(\"all_y = \" + str(all_y))             \n",
    "        #all_y = torch.cat(all_y)\n",
    "        #all_y_999_removed = torch.cat(all_y_999_removed)\n",
    "        #all_y_pred = torch.cat(all_y_pred)\n",
    "            \n",
    "        if(len(all_y_999_removed) == 0):\n",
    "            test_loss = np.inf\n",
    "            test_acc = -np.inf\n",
    "            total_val_size = 1\n",
    "            invalid_file_flag = True\n",
    "            return test_loss, test_acc,total_val_size,invalid_file_flag\n",
    "                \n",
    "                \n",
    "                \n",
    "        else:\n",
    "            test_loss = test_loss/(total_sgram_val)\n",
    "            all_y_999_removed = torch.cat(all_y_999_removed)\n",
    "            all_y_pred = torch.cat(all_y_pred)\n",
    "            #test_acc = accuracy_score(all_y_999_removed.numpy(), (all_y_pred.numpy() > class_threshold).astype(float))\n",
    "            #print(\"all_y_999_removed = \" + str(np.array(all_y_999_removed)))\n",
    "            #print(\"all_y_pred = \" + str(np.array(all_y_pred)))\n",
    "            test_acc = accuracy_score(all_y_999_removed.numpy(), all_y_pred.numpy() > class_threshold).astype(float)\n",
    "            #               test_acc = accuracy_score(np.array(all_y_999_removed), np.array(all_y_pred)).astype(float)\n",
    "            \n",
    "            total_val_size = total_sgram_val\n",
    "            invalid_file_flag = False\n",
    "            #print(\"total_val_size= \" + str(total_val_size))\n",
    "\n",
    "\n",
    "            return test_loss, test_acc,total_val_size,invalid_file_flag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d489c8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff34272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "841f74ab",
   "metadata": {},
   "source": [
    "## Data Load and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d768d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "library = 'PyTorch'\n",
    "\n",
    "if library == 'PyTorch':\n",
    "    from PyTorch.runTorch_AST_DK import (ASTModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c64cae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(config_DK_AST.data_df_dk_1)\n",
    "\n",
    "# To be kept: please do not edit the test set: these paths select test set A, test set B as described in the paper\n",
    "idx_test_A = np.logical_and(df['country'] == 'Tanzania', df['location_type'] == 'field')\n",
    "idx_test_B = np.logical_and(df['country'] == 'UK', df['location_type'] == 'culture')\n",
    "idx_train = np.logical_not(np.logical_or(idx_test_A, idx_test_B))\n",
    "df_test_A = df[idx_test_A]\n",
    "df_test_B = df[idx_test_B]\n",
    "\n",
    "\n",
    "df_train = df[idx_train]\n",
    "\n",
    "# Modify by addition or sub-sampling of df_train here\n",
    "\n",
    "\n",
    "# Assertion to check that train does NOT appear in test:\n",
    "assert len(np.where(pd.concat([df_train,df_test_A,\n",
    "                               df_test_B]).duplicated())[0]) == 0, 'Train dataframe contains overlap with Test A, Test B'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ee8b303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train) = 18\n",
      "len(val) = 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, val = train_test_split(df_train, test_size=0.1,shuffle = True)\n",
    "print(\"len(train) = \"  +str(len(train)))\n",
    "print(\"len(val) = \"  +str(len(val)))\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "val =  val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f561a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>record_datetime</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>species</th>\n",
       "      <th>gender</th>\n",
       "      <th>fed</th>\n",
       "      <th>plurality</th>\n",
       "      <th>age</th>\n",
       "      <th>method</th>\n",
       "      <th>mic_type</th>\n",
       "      <th>device_type</th>\n",
       "      <th>country</th>\n",
       "      <th>district</th>\n",
       "      <th>province</th>\n",
       "      <th>place</th>\n",
       "      <th>location_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74</td>\n",
       "      <td>0.127687</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>219424</td>\n",
       "      <td>5.376000</td>\n",
       "      <td>r2015-12-17_18.30.08.828.wav_u2016-04-08_14.22...</td>\n",
       "      <td>8000</td>\n",
       "      <td>17-12-15 18:30</td>\n",
       "      <td>background</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4015X</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nairobi</td>\n",
       "      <td>USAMRU-K</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63</td>\n",
       "      <td>0.080395</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    length                                               name  \\\n",
       "0      74  0.127687                    CDC_Ae-aegypti_labelled_800.wav   \n",
       "1  219424  5.376000  r2015-12-17_18.30.08.828.wav_u2016-04-08_14.22...   \n",
       "2      63  0.080395                    CDC_Ae-aegypti_labelled_800.wav   \n",
       "\n",
       "   sample_rate record_datetime  sound_type     species gender  fed plurality  \\\n",
       "0         8000   8/9/2016 8:00    mosquito  ae aegypti    NaN  NaN    Single   \n",
       "1         8000  17-12-15 18:30  background         NaN    NaN  NaN       NaN   \n",
       "2         8000   8/9/2016 8:00    mosquito  ae aegypti    NaN  NaN    Single   \n",
       "\n",
       "   age method mic_type    device_type country district province  \\\n",
       "0  NaN    NaN    phone  Alcatel 4009X     USA  Georgia  Atlanta   \n",
       "1  NaN    NaN    phone  Alcatel 4015X   Kenya      NaN  Nairobi   \n",
       "2  NaN    NaN    phone  Alcatel 4009X     USA  Georgia  Atlanta   \n",
       "\n",
       "                          place location_type  \n",
       "0  CDC insect cultures, Atlanta       culture  \n",
       "1                      USAMRU-K       culture  \n",
       "2  CDC insect cultures, Atlanta       culture  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a75a01b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc37c27b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba2d98b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b614994d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "baf8bb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len = 18\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_obj = SoundDS(train )\n",
    "val_obj = SoundDS_test(val)\n",
    "\n",
    "\n",
    "# Create training and validation data loaders\n",
    "#train_dl = torch.utils.data.DataLoader(train_obj, batch_size=16, shuffle=True,num_workers = 2 , collate_fn = collate_data,pin_memory = True)\n",
    "train_dl = torch.utils.data.DataLoader(train_obj, batch_size= loader_bat_size, shuffle=True,pin_memory = True)\n",
    "val_dl = torch.utils.data.DataLoader(val_obj, batch_size= loader_bat_size, shuffle=False,pin_memory = True)\n",
    "\n",
    "# train_dl = torch.utils.data.DataLoader(train_obj, batch_size= 1, shuffle=True, collate_fn = collate_data,pin_memory = True)\n",
    "# val_dl = torch.utils.data.DataLoader(val_obj, batch_size=1, shuffle=False,collate_fn = collate_data,pin_memory = True)\n",
    "\n",
    "# print(\"Len train DL = \" + str(len(train_dl)))\n",
    "# print(\"Len VAL DL = \" + str(len(val_dl)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fc89bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside get_item for train . Idx = 6\n",
      "^^^^^^^^ Start processing filename = 219432.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 11\n",
      "^^^^^^^^ Start processing filename = 54.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 14\n",
      "^^^^^^^^ Start processing filename = 86.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 1\n",
      "^^^^^^^^ Start processing filename = 72.wav^^^^^^^^\n",
      "['219432.wav', '54.wav', '86.wav', '72.wav']\n",
      "tensor([0, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_dl)\n",
    "data = dataiter.next()\n",
    "a,b = data\n",
    "print(a)\n",
    "print(b)\n",
    "# data_x,data_y = data\n",
    "# print((data_x))\n",
    "# print((data_y[0]))\n",
    "\n",
    "# dl_length  = len(train_dl)\n",
    "# loader_bat_size = min(4,dl_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c92ca",
   "metadata": {},
   "source": [
    "### Get the max dims for the val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5483284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b8771bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af5c5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39e961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3dcf71b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar  6 06:40:14 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   31C    P0    40W / 300W |   1373MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87497db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:0\n",
      "epoch = 0\n",
      "inside get_item for train . Idx = 8\n",
      "^^^^^^^^ Start processing filename = 73.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 13\n",
      "^^^^^^^^ Start processing filename = 218851.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 11\n",
      "^^^^^^^^ Start processing filename = 54.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 10\n",
      "^^^^^^^^ Start processing filename = 218964.wav^^^^^^^^\n",
      "TRain_dl length is 5\n",
      "idx in train_ast = 0\n",
      " file bat = ['73.wav', '218851.wav', '54.wav', '218964.wav']\n",
      " label bat = tensor([1, 1, 1, 0])\n",
      "Processing the file in train_ast->73.wav\n",
      "filename = 73.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/73.wav\n",
      "inside shorter files\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/73.wav*****\n",
      "About to apply signal transform...\n",
      "****** EXITING PROCESS_WAV (pad == TRUE)*****\n",
      "......APplying spec augment.....\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 83, 128])\n",
      "returning a feat_list with elements = 0\n",
      "(inside train_ast) = len of of spec_list = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2104/2368998103.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(label).reshape(-1,1).to(device).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the file in train_ast->218851.wav\n",
      "filename = 218851.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/218851.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/218851.wav*****\n",
      "About to apply signal transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 101, 128])\n",
      "returning a feat_list with elements = 5\n",
      "(inside train_ast) = len of of spec_list = 5\n",
      "Processing the file in train_ast->54.wav\n",
      "filename = 54.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/54.wav\n",
      "inside shorter files\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/54.wav*****\n",
      "About to apply signal transform...\n",
      "****** EXITING PROCESS_WAV (pad == TRUE)*****\n",
      "......APplying spec augment.....\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 81, 128])\n",
      "returning a feat_list with elements = 0\n",
      "(inside train_ast) = len of of spec_list = 1\n",
      "Processing the file in train_ast->218964.wav\n",
      "filename = 218964.wav\n",
      "label = tensor(0)\n",
      "inside get feat list , processing ->../data/audio/218964.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/218964.wav*****\n",
      "About to apply background transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 169, 128])\n",
      "returning a feat_list with elements = 18\n",
      "(inside train_ast) = len of of spec_list = 18\n",
      "inside get_item for train . Idx = 9\n",
      "^^^^^^^^ Start processing filename = 219429.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 15\n",
      "^^^^^^^^ Start processing filename = 218850.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 5\n",
      "^^^^^^^^ Start processing filename = 219427.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 4\n",
      "^^^^^^^^ Start processing filename = 62.wav^^^^^^^^\n",
      "TRain_dl length is 5\n",
      "idx in train_ast = 1\n",
      " file bat = ['219429.wav', '218850.wav', '219427.wav', '62.wav']\n",
      " label bat = tensor([0, 1, 0, 1])\n",
      "Processing the file in train_ast->219429.wav\n",
      "filename = 219429.wav\n",
      "label = tensor(0)\n",
      "inside get feat list , processing ->../data/audio/219429.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/219429.wav*****\n",
      "About to apply background transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 1542, 128])\n",
      "returning a feat_list with elements = 293\n",
      "(inside train_ast) = len of of spec_list = 293\n",
      "Processing the file in train_ast->218850.wav\n",
      "filename = 218850.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/218850.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/218850.wav*****\n",
      "About to apply signal transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 143, 128])\n",
      "returning a feat_list with elements = 13\n",
      "(inside train_ast) = len of of spec_list = 13\n",
      "Processing the file in train_ast->219427.wav\n",
      "filename = 219427.wav\n",
      "label = tensor(0)\n",
      "inside get feat list , processing ->../data/audio/219427.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/219427.wav*****\n",
      "About to apply background transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 1618, 128])\n",
      "returning a feat_list with elements = 308\n",
      "(inside train_ast) = len of of spec_list = 308\n",
      "Processing the file in train_ast->62.wav\n",
      "filename = 62.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/62.wav\n",
      "inside shorter files\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/62.wav*****\n",
      "About to apply signal transform...\n",
      "****** EXITING PROCESS_WAV (pad == TRUE)*****\n",
      "......APplying spec augment.....\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 83, 128])\n",
      "returning a feat_list with elements = 0\n",
      "(inside train_ast) = len of of spec_list = 1\n",
      "inside get_item for train . Idx = 0\n",
      "^^^^^^^^ Start processing filename = 219431.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 1\n",
      "^^^^^^^^ Start processing filename = 72.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 14\n",
      "^^^^^^^^ Start processing filename = 86.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 12\n",
      "^^^^^^^^ Start processing filename = 219428.wav^^^^^^^^\n",
      "TRain_dl length is 5\n",
      "idx in train_ast = 2\n",
      " file bat = ['219431.wav', '72.wav', '86.wav', '219428.wav']\n",
      " label bat = tensor([0, 1, 1, 0])\n",
      "Processing the file in train_ast->219431.wav\n",
      "filename = 219431.wav\n",
      "label = tensor(0)\n",
      "inside get feat list , processing ->../data/audio/219431.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/219431.wav*****\n",
      "About to apply background transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 2063, 128])\n",
      "returning a feat_list with elements = 397\n",
      "(inside train_ast) = len of of spec_list = 397\n",
      "Processing the file in train_ast->72.wav\n",
      "filename = 72.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/72.wav\n",
      "inside shorter files\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/72.wav*****\n",
      "About to apply signal transform...\n",
      "****** EXITING PROCESS_WAV (pad == TRUE)*****\n",
      "......APplying spec augment.....\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 84, 128])\n",
      "returning a feat_list with elements = 0\n",
      "(inside train_ast) = len of of spec_list = 1\n",
      "Processing the file in train_ast->86.wav\n",
      "filename = 86.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/86.wav\n",
      "inside shorter files\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/86.wav*****\n",
      "About to apply signal transform...\n",
      "****** EXITING PROCESS_WAV (pad == TRUE)*****\n",
      "......APplying spec augment.....\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 86, 128])\n",
      "returning a feat_list with elements = 2\n",
      "(inside train_ast) = len of of spec_list = 2\n",
      "Processing the file in train_ast->219428.wav\n",
      "filename = 219428.wav\n",
      "label = tensor(0)\n",
      "inside get feat list , processing ->../data/audio/219428.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/219428.wav*****\n",
      "About to apply background transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 1857, 128])\n",
      "returning a feat_list with elements = 356\n",
      "(inside train_ast) = len of of spec_list = 356\n",
      "inside get_item for train . Idx = 6\n",
      "^^^^^^^^ Start processing filename = 219432.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 16\n",
      "^^^^^^^^ Start processing filename = 219976.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 7\n",
      "^^^^^^^^ Start processing filename = 58.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 17\n",
      "^^^^^^^^ Start processing filename = 71.wav^^^^^^^^\n",
      "TRain_dl length is 5\n",
      "idx in train_ast = 3\n",
      " file bat = ['219432.wav', '219976.wav', '58.wav', '71.wav']\n",
      " label bat = tensor([0, 0, 1, 1])\n",
      "Processing the file in train_ast->219432.wav\n",
      "filename = 219432.wav\n",
      "label = tensor(0)\n",
      "inside get feat list , processing ->../data/audio/219432.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/219432.wav*****\n",
      "About to apply background transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 1697, 128])\n",
      "returning a feat_list with elements = 324\n",
      "(inside train_ast) = len of of spec_list = 324\n",
      "Processing the file in train_ast->219976.wav\n",
      "filename = 219976.wav\n",
      "label = tensor(0)\n",
      "inside get feat list , processing ->../data/audio/219976.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/219976.wav*****\n",
      "About to apply background transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 9369, 128])\n",
      "returning a feat_list with elements = 1858\n",
      "(inside train_ast) = len of of spec_list = 1858\n",
      "Processing the file in train_ast->58.wav\n",
      "filename = 58.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/58.wav\n",
      "inside shorter files\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/58.wav*****\n",
      "About to apply signal transform...\n",
      "****** EXITING PROCESS_WAV (pad == TRUE)*****\n",
      "......APplying spec augment.....\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 82, 128])\n",
      "returning a feat_list with elements = 0\n",
      "(inside train_ast) = len of of spec_list = 1\n",
      "Processing the file in train_ast->71.wav\n",
      "filename = 71.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/71.wav\n",
      "inside shorter files\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/71.wav*****\n",
      "About to apply signal transform...\n",
      "****** EXITING PROCESS_WAV (pad == TRUE)*****\n",
      "......APplying spec augment.....\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 86, 128])\n",
      "returning a feat_list with elements = 2\n",
      "(inside train_ast) = len of of spec_list = 2\n",
      "inside get_item for train . Idx = 2\n",
      "^^^^^^^^ Start processing filename = 4221.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 3\n",
      "^^^^^^^^ Start processing filename = 219430.wav^^^^^^^^\n",
      "TRain_dl length is 5\n",
      "idx in train_ast = 4\n",
      " file bat = ['4221.wav', '219430.wav']\n",
      " label bat = tensor([1, 0])\n",
      "Processing the file in train_ast->4221.wav\n",
      "filename = 4221.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/4221.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/4221.wav*****\n",
      "About to apply signal transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 978, 128])\n",
      "returning a feat_list with elements = 180\n",
      "(inside train_ast) = len of of spec_list = 180\n",
      "Processing the file in train_ast->219430.wav\n",
      "filename = 219430.wav\n",
      "label = tensor(0)\n",
      "inside get feat list , processing ->../data/audio/219430.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/219430.wav*****\n",
      "About to apply background transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 632, 128])\n",
      "returning a feat_list with elements = 111\n",
      "(inside train_ast) = len of of spec_list = 111\n",
      "^^^^^ INSIDE VAL DATA^^^^^^^^^\n",
      "inside get_item for val/test. Idx = 0\n",
      "Test File :- filename = ../data/audio/74.wav\n",
      "inside get_item for val/test. Idx = 1\n",
      "Test File :- filename = ../data/audio/219424.wav\n",
      "inside get_item for val/test. Idx = 2\n",
      "Test File :- filename = ../data/audio/63.wav\n",
      "val_dl length is = 1\n",
      "idx in val_dl = 0\n",
      " file bat = ['74.wav', '219424.wav', '63.wav']\n",
      " label bat = tensor([1, 0, 1])\n",
      "Processing the file in train_ast->74.wav\n",
      "filename = 74.wav\n",
      "label = tensor(1)\n",
      "....inside get_test_fest_list....\n",
      " ValueError .for filename ../data/audio/74.wavFILE SIZE < Win size. Returning 999 \n",
      "999 encountered .Error file..skipping\n",
      "Length of all_y = 1\n",
      "Processing the file in train_ast->219424.wav\n",
      "filename = 219424.wav\n",
      "label = tensor(0)\n",
      "....inside get_test_fest_list....\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 169, 128])\n",
      "returning a feat_list with elements = 18\n",
      "file ../data/audio/219424.wavwill have 18 specgrams\n",
      "Processing the file in train_ast->63.wav\n",
      "filename = 63.wav\n",
      "label = tensor(1)\n",
      "....inside get_test_fest_list....\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 80, 128])\n",
      "returning a feat_list with elements = 0\n",
      "duration to train epoch 0= 3.220042065779368\n",
      "Saving model to: ../outputs/models/pytorch/model_e0_2022_03_06_06_43_28.pth\n",
      "Epoch: 0, Train Loss: 0.03711540, Train Acc: 0.99457645, Val Loss: 0.11114958, Val Acc: 0.94736842, overrun_counter 0\n",
      "epoch = 1\n",
      "inside get_item for train . Idx = 8\n",
      "^^^^^^^^ Start processing filename = 73.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 13\n",
      "^^^^^^^^ Start processing filename = 218851.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 5\n",
      "^^^^^^^^ Start processing filename = 219427.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 3\n",
      "^^^^^^^^ Start processing filename = 219430.wav^^^^^^^^\n",
      "TRain_dl length is 5\n",
      "idx in train_ast = 0\n",
      " file bat = ['73.wav', '218851.wav', '219427.wav', '219430.wav']\n",
      " label bat = tensor([1, 1, 0, 0])\n",
      "Processing the file in train_ast->73.wav\n",
      "filename = 73.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/73.wav\n",
      "inside shorter files\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/73.wav*****\n",
      "About to apply signal transform...\n",
      "****** EXITING PROCESS_WAV (pad == TRUE)*****\n",
      "......APplying spec augment.....\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 84, 128])\n",
      "returning a feat_list with elements = 0\n",
      "(inside train_ast) = len of of spec_list = 1\n",
      "Processing the file in train_ast->218851.wav\n",
      "filename = 218851.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/218851.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/218851.wav*****\n",
      "About to apply signal transform...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2104/2368998103.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(label).reshape(-1,1).to(device).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 83, 128])\n",
      "returning a feat_list with elements = 0\n",
      "(inside train_ast) = len of of spec_list = 1\n",
      "Processing the file in train_ast->219427.wav\n",
      "filename = 219427.wav\n",
      "label = tensor(0)\n",
      "inside get feat list , processing ->../data/audio/219427.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/219427.wav*****\n",
      "About to apply background transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 1457, 128])\n",
      "returning a feat_list with elements = 276\n",
      "(inside train_ast) = len of of spec_list = 276\n",
      "Processing the file in train_ast->219430.wav\n",
      "filename = 219430.wav\n",
      "label = tensor(0)\n",
      "inside get feat list , processing ->../data/audio/219430.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/219430.wav*****\n",
      "About to apply background transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 569, 128])\n",
      "returning a feat_list with elements = 98\n",
      "(inside train_ast) = len of of spec_list = 98\n",
      "inside get_item for train . Idx = 2\n",
      "^^^^^^^^ Start processing filename = 4221.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 4\n",
      "^^^^^^^^ Start processing filename = 62.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 16\n",
      "^^^^^^^^ Start processing filename = 219976.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 17\n",
      "^^^^^^^^ Start processing filename = 71.wav^^^^^^^^\n",
      "TRain_dl length is 5\n",
      "idx in train_ast = 1\n",
      " file bat = ['4221.wav', '62.wav', '219976.wav', '71.wav']\n",
      " label bat = tensor([1, 1, 0, 1])\n",
      "Processing the file in train_ast->4221.wav\n",
      "filename = 4221.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/4221.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/4221.wav*****\n",
      "About to apply signal transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 1196, 128])\n",
      "returning a feat_list with elements = 224\n",
      "(inside train_ast) = len of of spec_list = 224\n",
      "Processing the file in train_ast->62.wav\n",
      "filename = 62.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/62.wav\n",
      "inside shorter files\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/62.wav*****\n",
      "About to apply signal transform...\n",
      "****** EXITING PROCESS_WAV (pad == TRUE)*****\n",
      "......APplying spec augment.....\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 82, 128])\n",
      "returning a feat_list with elements = 0\n",
      "(inside train_ast) = len of of spec_list = 1\n",
      "Processing the file in train_ast->219976.wav\n",
      "filename = 219976.wav\n",
      "label = tensor(0)\n",
      "inside get feat list , processing ->../data/audio/219976.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/219976.wav*****\n",
      "About to apply background transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 9369, 128])\n",
      "returning a feat_list with elements = 1858\n",
      "(inside train_ast) = len of of spec_list = 1858\n",
      "Processing the file in train_ast->71.wav\n",
      "filename = 71.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/71.wav\n",
      "inside shorter files\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/71.wav*****\n",
      "About to apply signal transform...\n",
      "****** EXITING PROCESS_WAV (pad == TRUE)*****\n",
      "......APplying spec augment.....\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 88, 128])\n",
      "returning a feat_list with elements = 2\n",
      "(inside train_ast) = len of of spec_list = 2\n",
      "inside get_item for train . Idx = 11\n",
      "^^^^^^^^ Start processing filename = 54.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 6\n",
      "^^^^^^^^ Start processing filename = 219432.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 1\n",
      "^^^^^^^^ Start processing filename = 72.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 10\n",
      "^^^^^^^^ Start processing filename = 218964.wav^^^^^^^^\n",
      "TRain_dl length is 5\n",
      "idx in train_ast = 2\n",
      " file bat = ['54.wav', '219432.wav', '72.wav', '218964.wav']\n",
      " label bat = tensor([1, 0, 1, 0])\n",
      "Processing the file in train_ast->54.wav\n",
      "filename = 54.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/54.wav\n",
      "inside shorter files\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/54.wav*****\n",
      "About to apply signal transform...\n",
      "****** EXITING PROCESS_WAV (pad == TRUE)*****\n",
      "......APplying spec augment.....\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 81, 128])\n",
      "returning a feat_list with elements = 0\n",
      "(inside train_ast) = len of of spec_list = 1\n",
      "Processing the file in train_ast->219432.wav\n",
      "filename = 219432.wav\n",
      "label = tensor(0)\n",
      "inside get feat list , processing ->../data/audio/219432.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/219432.wav*****\n",
      "About to apply background transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 1885, 128])\n",
      "returning a feat_list with elements = 362\n",
      "(inside train_ast) = len of of spec_list = 362\n",
      "Processing the file in train_ast->72.wav\n",
      "filename = 72.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/72.wav\n",
      "inside shorter files\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/72.wav*****\n",
      "About to apply signal transform...\n",
      "****** EXITING PROCESS_WAV (pad == TRUE)*****\n",
      "......APplying spec augment.....\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 84, 128])\n",
      "returning a feat_list with elements = 0\n",
      "(inside train_ast) = len of of spec_list = 1\n",
      "Processing the file in train_ast->218964.wav\n",
      "filename = 218964.wav\n",
      "label = tensor(0)\n",
      "inside get feat list , processing ->../data/audio/218964.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/218964.wav*****\n",
      "About to apply background transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 187, 128])\n",
      "returning a feat_list with elements = 22\n",
      "(inside train_ast) = len of of spec_list = 22\n",
      "inside get_item for train . Idx = 12\n",
      "^^^^^^^^ Start processing filename = 219428.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 7\n",
      "^^^^^^^^ Start processing filename = 58.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 0\n",
      "^^^^^^^^ Start processing filename = 219431.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 9\n",
      "^^^^^^^^ Start processing filename = 219429.wav^^^^^^^^\n",
      "TRain_dl length is 5\n",
      "idx in train_ast = 3\n",
      " file bat = ['219428.wav', '58.wav', '219431.wav', '219429.wav']\n",
      " label bat = tensor([0, 1, 0, 0])\n",
      "Processing the file in train_ast->219428.wav\n",
      "filename = 219428.wav\n",
      "label = tensor(0)\n",
      "inside get feat list , processing ->../data/audio/219428.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/219428.wav*****\n",
      "About to apply background transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 1857, 128])\n",
      "returning a feat_list with elements = 356\n",
      "(inside train_ast) = len of of spec_list = 356\n",
      "Processing the file in train_ast->58.wav\n",
      "filename = 58.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/58.wav\n",
      "inside shorter files\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/58.wav*****\n",
      "About to apply signal transform...\n",
      "****** EXITING PROCESS_WAV (pad == TRUE)*****\n",
      "......APplying spec augment.....\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 81, 128])\n",
      "returning a feat_list with elements = 0\n",
      "(inside train_ast) = len of of spec_list = 1\n",
      "Processing the file in train_ast->219431.wav\n",
      "filename = 219431.wav\n",
      "label = tensor(0)\n",
      "inside get feat list , processing ->../data/audio/219431.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/219431.wav*****\n",
      "About to apply background transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 2063, 128])\n",
      "returning a feat_list with elements = 397\n",
      "(inside train_ast) = len of of spec_list = 397\n",
      "Processing the file in train_ast->219429.wav\n",
      "filename = 219429.wav\n",
      "label = tensor(0)\n",
      "inside get feat list , processing ->../data/audio/219429.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/219429.wav*****\n",
      "About to apply background transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 1697, 128])\n",
      "returning a feat_list with elements = 324\n",
      "(inside train_ast) = len of of spec_list = 324\n",
      "inside get_item for train . Idx = 15\n",
      "^^^^^^^^ Start processing filename = 218850.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 14\n",
      "^^^^^^^^ Start processing filename = 86.wav^^^^^^^^\n",
      "TRain_dl length is 5\n",
      "idx in train_ast = 4\n",
      " file bat = ['218850.wav', '86.wav']\n",
      " label bat = tensor([1, 1])\n",
      "Processing the file in train_ast->218850.wav\n",
      "filename = 218850.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/218850.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/218850.wav*****\n",
      "About to apply signal transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 157, 128])\n",
      "returning a feat_list with elements = 16\n",
      "(inside train_ast) = len of of spec_list = 16\n",
      "Processing the file in train_ast->86.wav\n",
      "filename = 86.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/86.wav\n",
      "inside shorter files\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/86.wav*****\n",
      "About to apply signal transform...\n",
      "****** EXITING PROCESS_WAV (pad == TRUE)*****\n",
      "......APplying spec augment.....\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 85, 128])\n",
      "returning a feat_list with elements = 2\n",
      "(inside train_ast) = len of of spec_list = 2\n",
      "^^^^^ INSIDE VAL DATA^^^^^^^^^\n",
      "inside get_item for val/test. Idx = 0\n",
      "Test File :- filename = ../data/audio/74.wav\n",
      "inside get_item for val/test. Idx = 1\n",
      "Test File :- filename = ../data/audio/219424.wav\n",
      "inside get_item for val/test. Idx = 2\n",
      "Test File :- filename = ../data/audio/63.wav\n",
      "val_dl length is = 1\n",
      "idx in val_dl = 0\n",
      " file bat = ['74.wav', '219424.wav', '63.wav']\n",
      " label bat = tensor([1, 0, 1])\n",
      "Processing the file in train_ast->74.wav\n",
      "filename = 74.wav\n",
      "label = tensor(1)\n",
      "....inside get_test_fest_list....\n",
      " ValueError .for filename ../data/audio/74.wavFILE SIZE < Win size. Returning 999 \n",
      "999 encountered .Error file..skipping\n",
      "Length of all_y = 1\n",
      "Processing the file in train_ast->219424.wav\n",
      "filename = 219424.wav\n",
      "label = tensor(0)\n",
      "....inside get_test_fest_list....\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 169, 128])\n",
      "returning a feat_list with elements = 18\n",
      "file ../data/audio/219424.wavwill have 18 specgrams\n",
      "Processing the file in train_ast->63.wav\n",
      "filename = 63.wav\n",
      "label = tensor(1)\n",
      "....inside get_test_fest_list....\n",
      " ValueError .for filename ../data/audio/63.wavFILE SIZE < Win size. Returning 999 \n",
      "999 encountered .Error file..skipping\n",
      "Length of all_y = 20\n",
      "duration to train epoch 1= 3.2324891448020936\n",
      "Epoch: 1, Train Loss: 0.07924676, Train Acc: 0.99365965, Val Loss: 0.78450526, Val Acc: 0.00000000, overrun_counter 1\n",
      "epoch = 2\n",
      "inside get_item for train . Idx = 2\n",
      "^^^^^^^^ Start processing filename = 4221.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 1\n",
      "^^^^^^^^ Start processing filename = 72.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 10\n",
      "^^^^^^^^ Start processing filename = 218964.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 15\n",
      "^^^^^^^^ Start processing filename = 218850.wav^^^^^^^^\n",
      "TRain_dl length is 5\n",
      "idx in train_ast = 0\n",
      " file bat = ['4221.wav', '72.wav', '218964.wav', '218850.wav']\n",
      " label bat = tensor([1, 1, 0, 1])\n",
      "Processing the file in train_ast->4221.wav\n",
      "filename = 4221.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/4221.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/4221.wav*****\n",
      "About to apply signal transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 978, 128])\n",
      "returning a feat_list with elements = 180\n",
      "(inside train_ast) = len of of spec_list = 180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2104/2368998103.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(label).reshape(-1,1).to(device).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the file in train_ast->72.wav\n",
      "filename = 72.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/72.wav\n",
      "inside shorter files\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/72.wav*****\n",
      "About to apply signal transform...\n",
      "****** EXITING PROCESS_WAV (pad == TRUE)*****\n",
      "......APplying spec augment.....\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 86, 128])\n",
      "returning a feat_list with elements = 2\n",
      "(inside train_ast) = len of of spec_list = 2\n",
      "Processing the file in train_ast->218964.wav\n",
      "filename = 218964.wav\n",
      "label = tensor(0)\n",
      "inside get feat list , processing ->../data/audio/218964.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/218964.wav*****\n",
      "About to apply background transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 153, 128])\n",
      "returning a feat_list with elements = 15\n",
      "(inside train_ast) = len of of spec_list = 15\n",
      "Processing the file in train_ast->218850.wav\n",
      "filename = 218850.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/218850.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/218850.wav*****\n",
      "About to apply signal transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 157, 128])\n",
      "returning a feat_list with elements = 16\n",
      "(inside train_ast) = len of of spec_list = 16\n",
      "inside get_item for train . Idx = 11\n",
      "^^^^^^^^ Start processing filename = 54.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 13\n",
      "^^^^^^^^ Start processing filename = 218851.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 4\n",
      "^^^^^^^^ Start processing filename = 62.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 3\n",
      "^^^^^^^^ Start processing filename = 219430.wav^^^^^^^^\n",
      "TRain_dl length is 5\n",
      "idx in train_ast = 1\n",
      " file bat = ['54.wav', '218851.wav', '62.wav', '219430.wav']\n",
      " label bat = tensor([1, 1, 1, 0])\n",
      "Processing the file in train_ast->54.wav\n",
      "filename = 54.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/54.wav\n",
      "inside shorter files\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/54.wav*****\n",
      "About to apply signal transform...\n",
      "****** EXITING PROCESS_WAV (pad == TRUE)*****\n",
      "......APplying spec augment.....\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 81, 128])\n",
      "returning a feat_list with elements = 0\n",
      "(inside train_ast) = len of of spec_list = 1\n",
      "Processing the file in train_ast->218851.wav\n",
      "filename = 218851.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/218851.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/218851.wav*****\n",
      "About to apply signal transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 91, 128])\n",
      "returning a feat_list with elements = 3\n",
      "(inside train_ast) = len of of spec_list = 3\n",
      "Processing the file in train_ast->62.wav\n",
      "filename = 62.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/62.wav\n",
      "inside shorter files\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/62.wav*****\n",
      "About to apply signal transform...\n",
      "****** EXITING PROCESS_WAV (pad == TRUE)*****\n",
      "......APplying spec augment.....\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 82, 128])\n",
      "returning a feat_list with elements = 0\n",
      "(inside train_ast) = len of of spec_list = 1\n",
      "Processing the file in train_ast->219430.wav\n",
      "filename = 219430.wav\n",
      "label = tensor(0)\n",
      "inside get feat list , processing ->../data/audio/219430.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/219430.wav*****\n",
      "About to apply background transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 569, 128])\n",
      "returning a feat_list with elements = 98\n",
      "(inside train_ast) = len of of spec_list = 98\n",
      "inside get_item for train . Idx = 7\n",
      "^^^^^^^^ Start processing filename = 58.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 12\n",
      "^^^^^^^^ Start processing filename = 219428.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 0\n",
      "^^^^^^^^ Start processing filename = 219431.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 16\n",
      "^^^^^^^^ Start processing filename = 219976.wav^^^^^^^^\n",
      "TRain_dl length is 5\n",
      "idx in train_ast = 2\n",
      " file bat = ['58.wav', '219428.wav', '219431.wav', '219976.wav']\n",
      " label bat = tensor([1, 0, 0, 0])\n",
      "Processing the file in train_ast->58.wav\n",
      "filename = 58.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/58.wav\n",
      "inside shorter files\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/58.wav*****\n",
      "About to apply signal transform...\n",
      "****** EXITING PROCESS_WAV (pad == TRUE)*****\n",
      "......APplying spec augment.....\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 82, 128])\n",
      "returning a feat_list with elements = 0\n",
      "(inside train_ast) = len of of spec_list = 1\n",
      "Processing the file in train_ast->219428.wav\n",
      "filename = 219428.wav\n",
      "label = tensor(0)\n",
      "inside get feat list , processing ->../data/audio/219428.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/219428.wav*****\n",
      "About to apply background transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 2063, 128])\n",
      "returning a feat_list with elements = 397\n",
      "(inside train_ast) = len of of spec_list = 397\n",
      "Processing the file in train_ast->219431.wav\n",
      "filename = 219431.wav\n",
      "label = tensor(0)\n",
      "inside get feat list , processing ->../data/audio/219431.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/219431.wav*****\n",
      "About to apply background transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 2063, 128])\n",
      "returning a feat_list with elements = 397\n",
      "(inside train_ast) = len of of spec_list = 397\n",
      "Processing the file in train_ast->219976.wav\n",
      "filename = 219976.wav\n",
      "label = tensor(0)\n",
      "inside get feat list , processing ->../data/audio/219976.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/219976.wav*****\n",
      "About to apply background transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 9369, 128])\n",
      "returning a feat_list with elements = 1858\n",
      "(inside train_ast) = len of of spec_list = 1858\n",
      "inside get_item for train . Idx = 17\n",
      "^^^^^^^^ Start processing filename = 71.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 8\n",
      "^^^^^^^^ Start processing filename = 73.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 9\n",
      "^^^^^^^^ Start processing filename = 219429.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 14\n",
      "^^^^^^^^ Start processing filename = 86.wav^^^^^^^^\n",
      "TRain_dl length is 5\n",
      "idx in train_ast = 3\n",
      " file bat = ['71.wav', '73.wav', '219429.wav', '86.wav']\n",
      " label bat = tensor([1, 1, 0, 1])\n",
      "Processing the file in train_ast->71.wav\n",
      "filename = 71.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/71.wav\n",
      "inside shorter files\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/71.wav*****\n",
      "About to apply signal transform...\n",
      "****** EXITING PROCESS_WAV (pad == TRUE)*****\n",
      "......APplying spec augment.....\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 86, 128])\n",
      "returning a feat_list with elements = 2\n",
      "(inside train_ast) = len of of spec_list = 2\n",
      "Processing the file in train_ast->73.wav\n",
      "filename = 73.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/73.wav\n",
      "inside shorter files\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/73.wav*****\n",
      "About to apply signal transform...\n",
      "****** EXITING PROCESS_WAV (pad == TRUE)*****\n",
      "......APplying spec augment.....\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 86, 128])\n",
      "returning a feat_list with elements = 2\n",
      "(inside train_ast) = len of of spec_list = 2\n",
      "Processing the file in train_ast->219429.wav\n",
      "filename = 219429.wav\n",
      "label = tensor(0)\n",
      "inside get feat list , processing ->../data/audio/219429.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/219429.wav*****\n",
      "About to apply background transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 1697, 128])\n",
      "returning a feat_list with elements = 324\n",
      "(inside train_ast) = len of of spec_list = 324\n",
      "Processing the file in train_ast->86.wav\n",
      "filename = 86.wav\n",
      "label = tensor(1)\n",
      "inside get feat list , processing ->../data/audio/86.wav\n",
      "inside shorter files\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/86.wav*****\n",
      "About to apply signal transform...\n",
      "****** EXITING PROCESS_WAV (pad == TRUE)*****\n",
      "......APplying spec augment.....\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 86, 128])\n",
      "returning a feat_list with elements = 2\n",
      "(inside train_ast) = len of of spec_list = 2\n",
      "inside get_item for train . Idx = 5\n",
      "^^^^^^^^ Start processing filename = 219427.wav^^^^^^^^\n",
      "inside get_item for train . Idx = 6\n",
      "^^^^^^^^ Start processing filename = 219432.wav^^^^^^^^\n",
      "TRain_dl length is 5\n",
      "idx in train_ast = 4\n",
      " file bat = ['219427.wav', '219432.wav']\n",
      " label bat = tensor([0, 0])\n",
      "Processing the file in train_ast->219427.wav\n",
      "filename = 219427.wav\n",
      "label = tensor(0)\n",
      "inside get feat list , processing ->../data/audio/219427.wav\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/219427.wav*****\n",
      "About to apply background transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape after first transpose = torch.Size([1, 1618, 128])\n",
      "returning a feat_list with elements = 308\n",
      "(inside train_ast) = len of of spec_list = 308\n"
     ]
    }
   ],
   "source": [
    "model = train_model_ast(train_dl , val_dl )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7e66f5",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "076e81f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dli/task/Capstone-HumBug/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a3513da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.690375"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x = reshape_feat_pt(torch.random(1,128,408))\n",
    "librosa.get_duration(filename = '../data/audio/71.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e38ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model = train_model_ast(X_train, y_train, X_val, y_val, model=ASTModel())\n",
    "#model = train_model_ast(train_data, val_data,model=ASTModel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be8b8da",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cbf7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dim_testA = get_max_file_dims(df_test_A )\n",
    "max_dim_testA = max_dim_testA + int(max_dim_testA*config_DK_AST.len_tol)\n",
    "\n",
    "\n",
    "max_dim_testB = get_max_file_dims(df_test_B )\n",
    "max_dim_testB = max_dim_testB + int(max_dim_testB*config_DK_AST.len_tol)\n",
    "\n",
    "\n",
    "testA_obj = SoundDS_test(df_test_A,max_dim = max_dim_testA)\n",
    "testB_obj = SoundDS_test(df_test_B , max_dim = max_dim_testA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5e6d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filepath, model=ASTModel()):\n",
    "    # Instantiate model to inspect\n",
    "    print(\"Filepath = \" + str(filepath))\n",
    "    print(\"model = \" +str(model))\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "    print(f'Training on {device}')\n",
    "        \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "    model = model.to(device)\n",
    "    # Load trained parameters from checkpoint (may need to download from S3 first)\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        map_location=lambda storage, loc: storage.cuda()\n",
    "    else:\n",
    "        map_location='cpu'\n",
    "        \n",
    "    checkpoint = model.load_state_dict(torch.load(filepath))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5395d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlp = MLP()\n",
    "#mlp.load_state_dict(torch.load(save_path))\n",
    "\n",
    "\n",
    "model_init=ASTModel()\n",
    "model_init.eval()\n",
    "path = '../outputs/models/pytorch/'\n",
    "model_name = 'model_e2_2022_02_14_21_57_53.pth'\n",
    "\n",
    "\n",
    "#checkpoint = model.load_state_dict(torch.load(filepath))\n",
    "trained_model = load_model(path + model_name , model = model_init)\n",
    "\n",
    "\n",
    "# Dummy data for evaluation\n",
    "\n",
    "#creating random data to test \n",
    "#f1 = np.random.randn(128,1075)\n",
    "#f2 = np.random.randn(128,242)\n",
    "#f3 = np.random.randn(128,234)\n",
    "#f4 = np.ranadom.randn(128,263)\n",
    "\n",
    "# replicates get_feat\n",
    "#feats = [f1,f2,f3,f4]\n",
    "#labels = [0,1,0,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cfe105",
   "metadata": {},
   "source": [
    "## Test A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964acc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_type = 'FeatB'\n",
    "if feat_type == 'FeatA':\n",
    "    p, yt = evaluate_model_aggregated(trained_model, X_test_A, y_test_A, 1)  # Aggregate windows from feature list (0.96->1.92 s)\n",
    "    PE, MI, log_prob = get_results(p, yt, filename = feat_type + '_' + model_name +'_Test_A')\n",
    "elif feat_type == 'FeatB':\n",
    "    y_preds_all = evaluate_model(trained_model, X_test_A, y_test_A, 1)  # Predict directly over feature windows (1.92 s)\n",
    "    PE, MI, log_prob = get_results(y_preds_all, y_test_A, filename = feat_type + '_' + model_name +'_Test_A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba7440e",
   "metadata": {},
   "source": [
    "## TEST B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f80037",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_type = 'FeatB'\n",
    "if feat_type == 'FeatA':\n",
    "    \n",
    "    p, yt = evaluate_model_aggregated(model, X_test_B, y_test_B, 1)  # Aggregate windows from feature list (0.96->1.92 s)\n",
    "    PE, MI, log_prob = get_results(p, yt, filename = feat_type + '_' + model_name +'_Test_B')\n",
    "elif feat_type == 'FeatB':\n",
    "    y_preds_all = evaluate_model(trained_model, X_test_B, y_test_B, 1)  # Predict directly over feature windows (1.92 s)\n",
    "    PE, MI, log_prob = get_results(y_preds_all, y_test_B, filename = feat_type + '_' + model_name +'_Test_B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea5c11b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c2cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce8570b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
