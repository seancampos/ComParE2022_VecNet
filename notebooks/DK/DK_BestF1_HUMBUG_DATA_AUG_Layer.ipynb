{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaef2784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".CodeMirror{\n",
       "font-size: 14px;\n",
       "</style>\n",
       "CUDA_LAUNCH_BLOCKING=1\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style type='text/css'>\n",
    ".CodeMirror{\n",
    "font-size: 14px;\n",
    "</style>\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c79de07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch_audiomentations in /opt/conda/lib/python3.8/site-packages (0.11.0)\n",
      "Requirement already satisfied: torchaudio>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (0.11.0+cu113)\n",
      "Requirement already satisfied: librosa>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (0.8.1)\n",
      "Requirement already satisfied: torch-pitch-shift>=1.2.2 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (1.2.2)\n",
      "Requirement already satisfied: torch>=1.7.0 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (1.11.0+cu113)\n",
      "Requirement already satisfied: julius<0.3,>=0.2.3 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (0.2.6)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (5.1.0)\n",
      "Requirement already satisfied: numba>=0.43.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (0.53.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (1.21.4)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (1.6.3)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (0.10.3.post1)\n",
      "Requirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (1.5.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (21.3)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (0.2.2)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (2.1.9)\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (0.24.0)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /opt/conda/lib/python3.8/site-packages (from numba>=0.43.0->librosa>=0.6.0->torch_audiomentations) (0.36.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from numba>=0.43.0->librosa>=0.6.0->torch_audiomentations) (59.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->librosa>=0.6.0->torch_audiomentations) (3.0.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (2.26.0)\n",
      "Requirement already satisfied: appdirs in /opt/conda/lib/python3.8/site-packages (from pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (1.4.4)\n",
      "Requirement already satisfied: six>=1.3 in /opt/conda/lib/python3.8/site-packages (from resampy>=0.2.2->librosa>=0.6.0->torch_audiomentations) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa>=0.6.0->torch_audiomentations) (3.0.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.8/site-packages (from soundfile>=0.10.2->librosa>=0.6.0->torch_audiomentations) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa>=0.6.0->torch_audiomentations) (2.21)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.7.0->torch_audiomentations) (4.0.1)\n",
      "Requirement already satisfied: primePy>=1.3 in /opt/conda/lib/python3.8/site-packages (from torch-pitch-shift>=1.2.2->torch_audiomentations) (1.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (2.0.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_audiomentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b244f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_audiomentations import Compose,AddBackgroundNoise , AddColoredNoise , ApplyImpulseResponse,PeakNormalization,TimeInversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2138e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had to find the right version of pytorch with the widget here https://pytorch.org/\n",
    "# I *think* this will work with AWS\n",
    "#!pip3 install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7be4429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other dependencies\n",
    "#!pip install timm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a49ed762",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nnAudio\n",
    "#!pip install git+https://github.com/KinWaiCheuk/nnAudio.git#subdirectory=Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26e59c5",
   "metadata": {},
   "source": [
    "### 1 Import the kitchen sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91ddabf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
     ]
    }
   ],
   "source": [
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8224e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug main imports\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../lib'))\n",
    "import config\n",
    "from evaluate import get_results\n",
    "import numpy as np\n",
    "\n",
    "# Troubleshooting and visualisation\n",
    "import IPython.display as ipd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37d1cfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug lib imports\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from PyTorch import config_pytorch\n",
    "from datetime import datetime\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a77f28a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional pytorch tools\n",
    "import random\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchvision.transforms as VT\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "import timm\n",
    "import timm.optim\n",
    "from timm.loss import BinaryCrossEntropy\n",
    "from timm.utils import NativeScaler\n",
    "from timm.models import model_parameters\n",
    "from glob import glob\n",
    "from torch_audiomentations import Compose, Gain, PolarityInversion,AddColoredNoise,ApplyImpulseResponse,PeakNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "101cb8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nnAudio\n",
    "from nnAudio import features\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82ece133",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Training variables \n",
    "USE_SHORT_AUDIO = True\n",
    "num_workers=4\n",
    "pin_memory=True\n",
    "#train_size = 100\n",
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    batch_size = 4\n",
    "    test_batch_size = 4\n",
    "    num_workers=1\n",
    "    \n",
    "     \n",
    "\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e093131a",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb43bfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7f59c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates 1.92 secs rows of audio in a data frame format\n",
    "def get_offsets_df(df, short_audio=False):\n",
    "    audio_offsets = []\n",
    "    min_length = config.win_size*config.NFFT/(((1/config.n_hop)*config.NFFT)*config.rate)\n",
    "    step_frac = config.step_size/config.win_size\n",
    "    for _,row in df.iterrows():\n",
    "        if row['length'] > min_length:\n",
    "            step_size = step_frac*min_length\n",
    "            audio_offsets.append({'id':row['id'], 'offset':0, 'length': row['length'],'specie_ind': row['specie_ind']})\n",
    "            for i in range(1, int((row['length']-min_length)//step_size)):\n",
    "                audio_offsets.append({'id': row['id'], 'offset':int(min_length+(i*step_size)*config.rate), 'length': row['length'],'specie_ind': row['specie_ind']})\n",
    "        elif short_audio:\n",
    "            audio_offsets.append({'id':row['id'], 'offset':0,'length': row['length'],'specie_ind': row['specie_ind']})\n",
    "    return pd.DataFrame(audio_offsets)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aaa64eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['an arabiensis','culex pipiens complex', 'ae aegypti','an funestus ss','an squamosus',\n",
    "               'an coustani','ma uniformis','ma africanus' ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81118aaa",
   "metadata": {},
   "source": [
    "### Read CSV and get train/test groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b082ecff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>record_datetime</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>species</th>\n",
       "      <th>gender</th>\n",
       "      <th>fed</th>\n",
       "      <th>plurality</th>\n",
       "      <th>age</th>\n",
       "      <th>method</th>\n",
       "      <th>mic_type</th>\n",
       "      <th>device_type</th>\n",
       "      <th>country</th>\n",
       "      <th>district</th>\n",
       "      <th>province</th>\n",
       "      <th>place</th>\n",
       "      <th>location_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>0.463456</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>08-09-16 08:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>0.170249</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>08-09-16 08:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>0.104041</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>08-09-16 08:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69</td>\n",
       "      <td>0.274290</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>08-09-16 08:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56</td>\n",
       "      <td>0.420894</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>08-09-16 08:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Plural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>3562</td>\n",
       "      <td>6.083093</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>01-07-18 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>3556</td>\n",
       "      <td>6.719908</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>01-07-18 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an maculatus</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9009</th>\n",
       "      <td>3553</td>\n",
       "      <td>6.128580</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>01-07-18 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an maculatus</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9011</th>\n",
       "      <td>3561</td>\n",
       "      <td>11.614280</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>01-07-18 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9012</th>\n",
       "      <td>3552</td>\n",
       "      <td>2.920249</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>01-07-18 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6008 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     length                             name  sample_rate  \\\n",
       "1       53   0.463456  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "2       57   0.170249  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "3       61   0.104041  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "4       69   0.274290  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "5       56   0.420894  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "...    ...        ...                              ...          ...   \n",
       "8999  3562   6.083093                    #988-1001.wav        44100   \n",
       "9000  3556   6.719908                    #988-1001.wav        44100   \n",
       "9009  3553   6.128580                    #988-1001.wav        44100   \n",
       "9011  3561  11.614280                    #988-1001.wav        44100   \n",
       "9012  3552   2.920249                    #988-1001.wav        44100   \n",
       "\n",
       "     record_datetime sound_type       species  gender  fed plurality  age  \\\n",
       "1     08-09-16 08:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "2     08-09-16 08:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "3     08-09-16 08:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "4     08-09-16 08:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "5     08-09-16 08:00   mosquito    ae aegypti     NaN  NaN    Plural  NaN   \n",
       "...              ...        ...           ...     ...  ...       ...  ...   \n",
       "8999  01-07-18 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "9000  01-07-18 12:00   mosquito  an maculatus  Female    t    Single  NaN   \n",
       "9009  01-07-18 12:00   mosquito  an maculatus  Female    t    Single  NaN   \n",
       "9011  01-07-18 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "9012  01-07-18 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "\n",
       "     method mic_type    device_type   country          district  \\\n",
       "1       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "2       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "3       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "4       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "5       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "...     ...      ...            ...       ...               ...   \n",
       "8999    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9000    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9009    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9011    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9012    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "\n",
       "                   province                            place location_type  \n",
       "1                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "2                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "3                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "4                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "5                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "...                     ...                              ...           ...  \n",
       "8999  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9000  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9009  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9011  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9012  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "\n",
       "[6008 rows x 19 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    df = pd.read_csv(config.data_df_msc_test)\n",
    "else:\n",
    "    df = pd.read_csv(config.data_df)\n",
    "\n",
    "#df = df.loc[df['Grade'].notnull()]\n",
    "df = df.loc[df['species'].notnull()]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89505b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a colum for specie encoding\n",
    "df['specie_ind'] = \"NULL_VAL\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2de8261f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specie = an arabiensisand its index = 0\n",
      "specie = culex pipiens complexand its index = 1\n",
      "specie = ae aegyptiand its index = 2\n",
      "specie = an funestus ssand its index = 3\n",
      "specie = an squamosusand its index = 4\n",
      "specie = an coustaniand its index = 5\n",
      "specie = ma uniformisand its index = 6\n",
      "specie = ma africanusand its index = 7\n"
     ]
    }
   ],
   "source": [
    "# Adding a new column to encode specie_index in the same order as the list \"classes\"\n",
    "ind = 0\n",
    "for specie in classes:\n",
    "    print(\"specie = \" + str(specie) + \"and its index = \" + str(ind) )\n",
    "    row_indexes=df[df['species']==specie].index \n",
    "    df.loc[row_indexes,'specie_ind']= ind\n",
    "    ind+=1\n",
    "\n",
    "    \n",
    "# other_df_ind = df[df['specie_ind'] == \"NULL_VAL\"].index\n",
    "# df.loc[other_df_ind,'specie_ind']= other_ind                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6a6acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['specie_ind'] == \"NULL_VAL\"].index, inplace=True)\n",
    "#other_df_ind = df[df['specie_ind'] == \"NULL_VAL\"].index\n",
    "#df.loc[other_df_ind,'specie_ind']= other_ind        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d14a2",
   "metadata": {},
   "source": [
    "At this stage we have all extracted the data with specie information and have encoded the specie encoding in a col = 'specie_ind'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4182faa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the TZ and Cup data- this is as per the humbug paper\n",
    "\n",
    "idx_multiclass = np.logical_and(df['country'] == 'Tanzania', df['location_type'] == 'cup')\n",
    "df_all = df[idx_multiclass]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc316ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d76fdea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>record_datetime</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>species</th>\n",
       "      <th>gender</th>\n",
       "      <th>fed</th>\n",
       "      <th>...</th>\n",
       "      <th>age</th>\n",
       "      <th>method</th>\n",
       "      <th>mic_type</th>\n",
       "      <th>device_type</th>\n",
       "      <th>country</th>\n",
       "      <th>district</th>\n",
       "      <th>province</th>\n",
       "      <th>place</th>\n",
       "      <th>location_type</th>\n",
       "      <th>specie_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1879</td>\n",
       "      <td>221103</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_24_664.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1880</td>\n",
       "      <td>221111</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_25_665.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1881</td>\n",
       "      <td>221110</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_25_665.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1882</td>\n",
       "      <td>221149</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_26_666.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an arabiensis</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1883</td>\n",
       "      <td>221150</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_26_666.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an arabiensis</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>4546</td>\n",
       "      <td>222615</td>\n",
       "      <td>30.72</td>\n",
       "      <td>IFA_86_39_3439.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2284</th>\n",
       "      <td>4547</td>\n",
       "      <td>222585</td>\n",
       "      <td>25.60</td>\n",
       "      <td>IFA_86_40_3440.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2285</th>\n",
       "      <td>4548</td>\n",
       "      <td>222586</td>\n",
       "      <td>40.90</td>\n",
       "      <td>IFA_87_10_3450.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2286</th>\n",
       "      <td>4549</td>\n",
       "      <td>222596</td>\n",
       "      <td>40.90</td>\n",
       "      <td>IFA_87_11_3451.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287</th>\n",
       "      <td>4550</td>\n",
       "      <td>222614</td>\n",
       "      <td>38.40</td>\n",
       "      <td>IFA_87_12_3452.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2288 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index      id  length                name  sample_rate record_datetime  \\\n",
       "0      1879  221103    2.56   IFA_17_24_664.wav        44100  30-01-20 00:00   \n",
       "1      1880  221111    2.56   IFA_17_25_665.wav        44100  30-01-20 00:00   \n",
       "2      1881  221110    2.56   IFA_17_25_665.wav        44100  30-01-20 00:00   \n",
       "3      1882  221149    2.56   IFA_17_26_666.wav        44100  30-01-20 00:00   \n",
       "4      1883  221150    2.56   IFA_17_26_666.wav        44100  30-01-20 00:00   \n",
       "...     ...     ...     ...                 ...          ...             ...   \n",
       "2283   4546  222615   30.72  IFA_86_39_3439.wav        44100  23-08-20 00:00   \n",
       "2284   4547  222585   25.60  IFA_86_40_3440.wav        44100  23-08-20 00:00   \n",
       "2285   4548  222586   40.90  IFA_87_10_3450.wav        44100  23-08-20 00:00   \n",
       "2286   4549  222596   40.90  IFA_87_11_3451.wav        44100  23-08-20 00:00   \n",
       "2287   4550  222614   38.40  IFA_87_12_3452.wav        44100  23-08-20 00:00   \n",
       "\n",
       "     sound_type         species  gender fed  ... age  method mic_type  \\\n",
       "0      mosquito    ma africanus  Female   f  ... NaN     HBN  telinga   \n",
       "1      mosquito    ma africanus  Female   f  ... NaN     HBN  telinga   \n",
       "2      mosquito    ma africanus  Female   f  ... NaN     HBN  telinga   \n",
       "3      mosquito   an arabiensis  Female   f  ... NaN     HBN  telinga   \n",
       "4      mosquito   an arabiensis  Female   f  ... NaN     HBN  telinga   \n",
       "...         ...             ...     ...  ..  ...  ..     ...      ...   \n",
       "2283   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2284   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2285   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2286   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2287   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "\n",
       "     device_type   country            district  province    place  \\\n",
       "0         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "1         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "3         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "4         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "...          ...       ...                 ...       ...      ...   \n",
       "2283      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2284      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2285      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2286      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2287      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "\n",
       "     location_type specie_ind  \n",
       "0              cup          7  \n",
       "1              cup          7  \n",
       "2              cup          7  \n",
       "3              cup          0  \n",
       "4              cup          0  \n",
       "...            ...        ...  \n",
       "2283           cup          3  \n",
       "2284           cup          3  \n",
       "2285           cup          3  \n",
       "2286           cup          3  \n",
       "2287           cup          3  \n",
       "\n",
       "[2288 rows x 21 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d92a613",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8668152a",
   "metadata": {},
   "source": [
    "### Train-Test split( avoiding sklearn )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0ce75da",
   "metadata": {},
   "outputs": [],
   "source": [
    "msk_test = np.random.rand(len(df_all)) < 0.2\n",
    "df_test = df_all[msk_test]\n",
    "df_train_temp  = df_all[~msk_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "242e89d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "msk_train = np.random.rand(len(df_train_temp)) < 0.2\n",
    "df_val = df_train_temp[msk_train]\n",
    "df_train  = df_train_temp[~msk_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff0ad48",
   "metadata": {},
   "source": [
    "## Let's verify for data leakage by performing an inner-join on id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4857517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, index_y, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_test,df_train, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cd9a838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, index_y, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_test,df_val, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c06bc6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, index_y, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_train,df_val, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c2fa00",
   "metadata": {},
   "source": [
    "We've confirmed that there is no recording that is common in Train,Test,val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fbcc7e",
   "metadata": {},
   "source": [
    "### Next, we perform \"offsets\", spliting each(long) recording into multiple 1.92 secs chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "557c360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_offset = get_offsets_df(df_train, short_audio=USE_SHORT_AUDIO)\n",
    "df_test_offset = get_offsets_df(df_test, short_audio=USE_SHORT_AUDIO)\n",
    "df_val_offset = get_offsets_df(df_val, short_audio=USE_SHORT_AUDIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02385964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train offset = 67822\n",
      "length of test offset = 18965\n",
      "length of val offset = 15793\n"
     ]
    }
   ],
   "source": [
    "print(\"length of train offset = \" +str(len(df_train_offset)))\n",
    "print(\"length of test offset = \" +str(len(df_test_offset)))\n",
    "print(\"length of val offset = \" +str(len(df_val_offset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d15c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e059135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd684ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "849d15ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temp.reset_index(inplace = True)\n",
    "df_train_offset.reset_index(inplace = True)\n",
    "df_test_offset.reset_index(inplace = True)\n",
    "df_val_offset.reset_index(inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7403aec",
   "metadata": {},
   "source": [
    "### Let's check for data leakage in offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c43011e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>offset_x</th>\n",
       "      <th>length_x</th>\n",
       "      <th>specie_ind_x</th>\n",
       "      <th>index_y</th>\n",
       "      <th>offset_y</th>\n",
       "      <th>length_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, offset_x, length_x, specie_ind_x, index_y, offset_y, length_y, specie_ind_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_train_offset , df_test_offset , on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df13067d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>offset_x</th>\n",
       "      <th>length_x</th>\n",
       "      <th>specie_ind_x</th>\n",
       "      <th>index_y</th>\n",
       "      <th>offset_y</th>\n",
       "      <th>length_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, offset_x, length_x, specie_ind_x, index_y, offset_y, length_y, specie_ind_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_train_offset , df_val_offset , on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc4877a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>offset_x</th>\n",
       "      <th>length_x</th>\n",
       "      <th>specie_ind_x</th>\n",
       "      <th>index_y</th>\n",
       "      <th>offset_y</th>\n",
       "      <th>length_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, offset_x, length_x, specie_ind_x, index_y, offset_y, length_y, specie_ind_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_test_offset , df_val_offset , on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c5a5cf",
   "metadata": {},
   "source": [
    "### At this stage we've a dataframe of recordin ids and each row corresponds to a 1.92 secs recording or shorter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f125f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specie_distri(df , classes , type_df = None):\n",
    "    \"\"\"This function takes a dataframe and provides a count of each specie class\"\"\"\n",
    "    for i in range(len(classes)):\n",
    "        print(\"DF type = \" + str(type_df))\n",
    "        df_temp = df[df['specie_ind'] == i]\n",
    "        print(\"i = \" +str(i))\n",
    "        print(len(df_temp))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71a38ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31193429 0.61620512 3.26067308 0.57896264 2.47453298 4.35202772\n",
      " 2.70768125 7.44969244]\n"
     ]
    }
   ],
   "source": [
    "#Class imbalance \n",
    "np.array(df_train_offset.specie_ind)\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',classes=np.unique(np.array(df_train_offset.specie_ind)),y=np.array(np.array(df_train_offset.specie_ind)))\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8db3c8",
   "metadata": {},
   "source": [
    "Let us now get the class distribution for each of the dataframes- train,test and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51732dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF type = train\n",
      "i = 0\n",
      "27178\n",
      "DF type = train\n",
      "i = 1\n",
      "13758\n",
      "DF type = train\n",
      "i = 2\n",
      "2600\n",
      "DF type = train\n",
      "i = 3\n",
      "14643\n",
      "DF type = train\n",
      "i = 4\n",
      "3426\n",
      "DF type = train\n",
      "i = 5\n",
      "1948\n",
      "DF type = train\n",
      "i = 6\n",
      "3131\n",
      "DF type = train\n",
      "i = 7\n",
      "1138\n"
     ]
    }
   ],
   "source": [
    "get_specie_distri(df_train_offset , classes , type_df = \"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3445560c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF type = Val\n",
      "i = 0\n",
      "6688\n",
      "DF type = Val\n",
      "i = 1\n",
      "3733\n",
      "DF type = Val\n",
      "i = 2\n",
      "438\n",
      "DF type = Val\n",
      "i = 3\n",
      "2722\n",
      "DF type = Val\n",
      "i = 4\n",
      "845\n",
      "DF type = Val\n",
      "i = 5\n",
      "344\n",
      "DF type = Val\n",
      "i = 6\n",
      "524\n",
      "DF type = Val\n",
      "i = 7\n",
      "499\n"
     ]
    }
   ],
   "source": [
    "get_specie_distri(df_val_offset , classes , type_df = \"Val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "79f54959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF type = test\n",
      "i = 0\n",
      "7244\n",
      "DF type = test\n",
      "i = 1\n",
      "4615\n",
      "DF type = test\n",
      "i = 2\n",
      "546\n",
      "DF type = test\n",
      "i = 3\n",
      "3421\n",
      "DF type = test\n",
      "i = 4\n",
      "1389\n",
      "DF type = test\n",
      "i = 5\n",
      "705\n",
      "DF type = test\n",
      "i = 6\n",
      "706\n",
      "DF type = test\n",
      "i = 7\n",
      "339\n"
     ]
    }
   ],
   "source": [
    "get_specie_distri(df_test_offset , classes , type_df = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2b194ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd1c7aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function pads a short-audio tensor with its mean to ensure that it becomes a 1.92 sec long audio equivalent\n",
    "def pad_mean(x_temp,rate = config.rate, min_length = config.min_duration ):\n",
    "    if DEBUG:\n",
    "        print(\"inside padding mean...\")\n",
    "    x_mean = torch.mean(x_temp)\n",
    "    #x_mean.cuda()\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"X_mean = \" + str(x_mean))\n",
    "    left_pad_amt = int((rate*min_length-x_temp.shape[1])//2)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_amt = \" + str(left_pad_amt))\n",
    "    left_pad = torch.zeros(1,left_pad_amt) #+ (0.1**0.5)*torch.randn(1, left_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad shape = \" + str(left_pad.shape))\n",
    "    left_pad_mean_add = left_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_mean shape = \" + str(left_pad_mean_add))\n",
    "        print(\"sum of left pad mean add = \" + str(torch.sum(left_pad_mean_add)))\n",
    "    \n",
    "    right_pad_amt = int(rate*min_length-x_temp.shape[1]-left_pad_amt)\n",
    "    right_pad = torch.zeros(1,right_pad_amt)# + (0.1**0.5)*torch.randn(1, right_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"right_pad shape = \" + str(right_pad.shape))\n",
    "    right_pad_mean_add = right_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"right_pad_mean shape = \" + str(right_pad_mean_add))\n",
    "        print(\"sum of right pad mean add = \"  + str(torch.sum(right_pad_mean_add)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    f = torch.cat([left_pad,x_temp,right_pad],dim=1)[0]\n",
    "    f = f.unsqueeze(dim = 0)\n",
    "    #print(\"returning a tensor of shape = \" + str(f.shape))\n",
    "    return(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fba928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a11e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_hat,y_true,classes):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_hat, y_true ,labels= range(len(classes)))\n",
    "    import seaborn as sns\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cellsplt.xticks(rotation=90)\n",
    "    ax.xaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    ax.xaxis.tick_bottom()\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.set_ylabel('True', fontsize=20)\n",
    "    ax.yaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b39580d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ebeacb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.92"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the min length based on config params\n",
    "min_length = (config.win_size * config.n_hop) / config.rate\n",
    "min_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec60adf2",
   "metadata": {},
   "source": [
    "### Class Defintions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "235edc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization():\n",
    "    \"\"\"This class is for normalizing the spectrograms batch by batch. The normalization used is min-max, two modes 'framewise' and 'imagewise' can be selected. In this paper, we found that 'imagewise' normalization works better than 'framewise'\"\"\"\n",
    "    def __init__(self, mode='framewise'):\n",
    "        if mode == 'framewise':\n",
    "            def normalize(x):\n",
    "                size = x.shape\n",
    "                x_max = x.max(1, keepdim=True)[0] # Finding max values for each frame\n",
    "                x_min = x.min(1, keepdim=True)[0]  \n",
    "                output = (x-x_min)/(x_max-x_min) # If there is a column with all zero, nan will occur\n",
    "                output[torch.isnan(output)]=0 # Making nan to 0\n",
    "                return output\n",
    "        elif mode == 'imagewise':\n",
    "            def normalize(x):\n",
    "                size = x.shape\n",
    "                x_max = x.reshape(size[0], size[1]*size[2]).max(1, keepdim=True)[0]\n",
    "                x_min = x.reshape(size[0], size[1]*size[2]).min(1, keepdim=True)[0]\n",
    "                x_max = x_max.unsqueeze(1) # Make it broadcastable\n",
    "                x_min = x_min.unsqueeze(1) # Make it broadcastable \n",
    "                return (x-x_min)/(x_max-x_min)\n",
    "        else:\n",
    "            print(f'please choose the correct mode')\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.normalize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6bcf19e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcen(x, eps=1e-6, s=0.025, alpha=0.98, delta=2, r=0.5, training=False):\n",
    "    frames = x.split(1, -2)\n",
    "    m_frames = []\n",
    "    last_state = None\n",
    "    for frame in frames:\n",
    "        if last_state is None:\n",
    "            last_state = s * frame\n",
    "            m_frames.append(last_state)\n",
    "            continue\n",
    "        if training:\n",
    "            m_frame = ((1 - s) * last_state).add_(s * frame)\n",
    "        else:\n",
    "            m_frame = (1 - s) * last_state + s * frame\n",
    "        last_state = m_frame\n",
    "        m_frames.append(m_frame)\n",
    "    M = torch.cat(m_frames, 1)\n",
    "    if training:\n",
    "        pcen_ = (x / (M + eps).pow(alpha) + delta).pow(r) - delta ** r\n",
    "    else:\n",
    "        pcen_ = x.div_(M.add_(eps).pow_(alpha)).add_(delta).pow_(r).sub_(delta ** r)\n",
    "    return pcen_\n",
    "\n",
    "\n",
    "class PCENTransform(nn.Module):\n",
    "\n",
    "    def __init__(self, eps=1e-6, s=0.025, alpha=0.98, delta=2, r=0.5, trainable=True):\n",
    "        super().__init__()\n",
    "        if trainable:\n",
    "            self.log_s = nn.Parameter(torch.log(torch.Tensor([s])))\n",
    "            self.log_alpha = nn.Parameter(torch.log(torch.Tensor([alpha])))\n",
    "            self.log_delta = nn.Parameter(torch.log(torch.Tensor([delta])))\n",
    "            self.log_r = nn.Parameter(torch.log(torch.Tensor([r])))\n",
    "        else:\n",
    "            self.s = s\n",
    "            self.alpha = alpha\n",
    "            self.delta = delta\n",
    "            self.r = r\n",
    "        self.eps = eps\n",
    "        self.trainable = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x.permute((0,2,1)).squeeze(dim=1)\n",
    "        if self.trainable:\n",
    "            x = pcen(x, self.eps, torch.exp(self.log_s), torch.exp(self.log_alpha), torch.exp(self.log_delta), torch.exp(self.log_r), self.training and self.trainable)\n",
    "        else:\n",
    "            x = pcen(x, self.eps, self.s, self.alpha, self.delta, self.r, self.training and self.trainable)\n",
    "#         x = x.unsqueeze(dim=1).permute((0,1,3,2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "baf3d035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>offset</th>\n",
       "      <th>length</th>\n",
       "      <th>specie_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>221111</td>\n",
       "      <td>0</td>\n",
       "      <td>2.56</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>221111</td>\n",
       "      <td>2561</td>\n",
       "      <td>2.56</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>221110</td>\n",
       "      <td>0</td>\n",
       "      <td>2.56</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>221110</td>\n",
       "      <td>2561</td>\n",
       "      <td>2.56</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>221149</td>\n",
       "      <td>0</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index      id  offset  length  specie_ind\n",
       "0      0  221111       0    2.56           7\n",
       "1      1  221111    2561    2.56           7\n",
       "2      2  221110       0    2.56           7\n",
       "3      3  221110    2561    2.56           7\n",
       "4      4  221149       0    2.56           0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_offset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09b62efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, loader, criterion,  classes = classes,device=None , call = \"val\"):\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"calling for ...\" +str(call))\n",
    "    with torch.no_grad():\n",
    "        if device is None:\n",
    "            torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        sigmoid = nn.Sigmoid()\n",
    "        test_loss = 0.0\n",
    "        model.eval()\n",
    "        if DEBUG:\n",
    "            print(\"inside test....\")\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        counter = 1\n",
    "        if DEBUG:\n",
    "            print(\"length of loader = \" + str(len(loader)))\n",
    "        for idx,(x,y) in enumerate(loader):\n",
    "            if DEBUG:\n",
    "                print(\"loader index = \" + str(idx))\n",
    "                            \n",
    "            x = x.to(device).float() \n",
    "            y = y.type(torch.LongTensor).to(device)\n",
    "            if DEBUG:\n",
    "                print(\"y = \" + str(y))\n",
    "            y_pred = model(x)['prediction']\n",
    "            preds = torch.argmax(y_pred, axis = 1)\n",
    "            y_pred_cpu = y_pred.cpu().detach()\n",
    "            if DEBUG:\n",
    "                print(\"y_pred_cpu = \" + str(y_pred_cpu))\n",
    "            preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "            if DEBUG:\n",
    "                print(\"preds = \" +str(preds))\n",
    "            all_y_pred.append(preds.cpu().detach())\n",
    "                                   \n",
    "            loss = criterion(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "            all_y.append(y.cpu().detach())\n",
    "            #all_y_pred.append(np.argmax(y_pred.cpu().detach().numpy()))\n",
    "            \n",
    "            del x\n",
    "            del y\n",
    "            del y_pred\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        if DEBUG:\n",
    "            print(\"inside test....\")\n",
    "            print(\"y = \" + str(all_y))\n",
    "            print(\"y_pred  = \" + str(all_y_pred))\n",
    "        \n",
    "        test_loss = test_loss/len(test_loader)\n",
    "        test_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')\n",
    "    \n",
    "    \n",
    "    return test_loss, test_f1 , all_y,all_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "971819f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(train_loader, val_loader, test_loader,model, classes ,class_weights ,num_epochs = num_epochs )\n",
    "def train_model(train_loader, val_loader,test_loader, model = None,  classes = classes,class_weights = class_weights,num_epochs = num_epochs ,n_channels = 1):\n",
    "    # Creates a GradScaler once at the beginning of training.\n",
    "    loss_scaler = NativeScaler()\n",
    "    global_step = 0\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Training on {device}')    \n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "\n",
    "    model = model.to(device)\n",
    "    weights_adj = torch.tensor(class_weights).type(torch.float).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights_adj)\n",
    "    optimiser = timm.optim.RAdam(model.parameters(), lr=config_pytorch.lr/10)\n",
    "    num_epochs = num_epochs\n",
    "    all_train_loss = []\n",
    "    all_train_f1 = []\n",
    "    all_val_loss = []\n",
    "    all_val_f1 = []\n",
    "    best_val_loss = np.inf\n",
    "    best_val_f1 = -np.inf\n",
    "    best_train_f1 = -np.inf\n",
    "    best_epoch = -1\n",
    "    checkpoint_name = None\n",
    "    overrun_counter = 0\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    lr_log = []\n",
    "    for e in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        tk0 = tqdm(train_loader, total=int(len(train_loader)))\n",
    "        for batch_i, inputs in enumerate(tk0):\n",
    "            if DEBUG:\n",
    "                print(\"inside train loop.. batch_ind = \" +str(batch_i))\n",
    "            if batch_i % 500 == 0:\n",
    "                bat_time = time.time()\n",
    "                durn = (bat_time - start_time)/60\n",
    "                print(\"epoch = \" +str(e) + \"batch = \" +str(batch_i) + \" of \" + str(len(train_loader)) + \"duraation = \" + str(durn))\n",
    "            x = inputs[0].to(device).float()\n",
    "            y = inputs[1].type(torch.LongTensor).to(device)\n",
    "            global_step += 1\n",
    "            optimiser.zero_grad()\n",
    "            # AMP\n",
    "            with autocast():\n",
    "                y_pred = model(x)['prediction']\n",
    "                preds = torch.argmax(y_pred, axis = 1)\n",
    "                if DEBUG:\n",
    "                    print(\"y_pred  = \" +str(y_pred))\n",
    "                    print(\"preds = \" +str(preds))\n",
    "                loss = criterion(y_pred, y)\n",
    "            loss_scaler(loss, optimiser,parameters=model_parameters(model))\n",
    "            train_loss += loss.item()\n",
    "            all_y.append(y.cpu().detach())\n",
    "            y_pred_cpu = y_pred.cpu().detach()\n",
    "            preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "            if DEBUG:\n",
    "                print(\"batch_ind = \" +str(batch_i))\n",
    "                print(\"y_pred_cpu = \" + str(y_pred_cpu))\n",
    "                \n",
    "            all_y_pred.append(preds.cpu().detach())\n",
    "            lr_log.append(optimiser.param_groups[0]['lr'])\n",
    "            tk0.set_postfix(training_loss=(train_loss / (batch_i+1)), lr=optimiser.param_groups[0]['lr'])\n",
    "            del x\n",
    "            del y\n",
    "            del y_pred,preds\n",
    "        \n",
    "        all_train_loss.append(train_loss/len(train_loader))\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        if DEBUG:\n",
    "            print(\"y = \" + str(all_y))\n",
    "            print(\"y_pred  = \" + str(all_y_pred))\n",
    "        \n",
    "        train_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')\n",
    "        if DEBUG:\n",
    "            print(\"train acc = \" +str(train_acc))\n",
    "        all_train_f1.append(train_f1)\n",
    "        val_loss, val_f1 , _,_ = test_model(model, val_loader, criterion = nn.CrossEntropyLoss(), classes = classes ,device=device, call = \"val\")\n",
    "        if DEBUG:\n",
    "            print(\"val F1 = \" + str(val_f1))\n",
    "        all_val_loss.append(val_loss)\n",
    "        all_val_f1.append(val_f1)\n",
    "        \n",
    "        acc_metric = val_f1\n",
    "        best_acc_metric = best_val_f1\n",
    "        if acc_metric > best_acc_metric:  \n",
    "            overrun_counter = -1\n",
    "            checkpoint_name = f'model_e{e}_{datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")}.pth'\n",
    "            torch.save(model.state_dict(), os.path.join(config.model_dir, 'pytorch', checkpoint_name))\n",
    "            print('Epoch: %d, Train Loss: %.8f, Train f1: %.8f, Val Loss: %.8f, Val f1: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader), train_f1, val_loss/len(val_loader), val_f1,  overrun_counter))\n",
    "            print('Saving model to:', os.path.join(config.model_dir, 'pytorch', checkpoint_name)) \n",
    "            print(\"Now printing classification rport... \")\n",
    "            print(\"********************************\")\n",
    "            from sklearn.metrics import classification_report\n",
    "            _, _ , all_y_test,all_y_pred_test = test_model(model, test_loader, criterion = nn.CrossEntropyLoss(), classes = classes ,device=device, call = \"test\")\n",
    "            print(classification_report(all_y_test.numpy(), all_y_pred_test.numpy(), target_names= classes))\n",
    "            print(\"********************************\")\n",
    "            plot_confusion_matrix(all_y_pred_test.numpy(), all_y_test.numpy() , classes)\n",
    "            best_epoch = e\n",
    "            best_val_f1 = val_f1\n",
    "            best_val_loss = val_loss\n",
    "            \n",
    "        else:\n",
    "            print(\"..Overrun....no improvement\")\n",
    "            overrun_counter += 1\n",
    "            print('Epoch: %d, Train Loss: %.8f, Train f1: %.8f, Val Loss: %.8f, Val f1: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader), train_f1, val_loss/len(val_loader), val_f1,  overrun_counter))\n",
    "        if overrun_counter > config_pytorch.max_overrun:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    return model, lr_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8a034b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_audiomentations import Compose, Gain, PolarityInversion,AddColoredNoise,ApplyImpulseResponse,PeakNormalization\n",
    "#apply_augmentation = Compose(transforms=[PolarityInversion(p=0.5 ,output_type = 'tensor'),AddColoredNoise(), PeakNormalization(apply_to=\"only_too_loud_sounds\"),TimeInversion(output_type = 'tensor')  ])\n",
    "\n",
    "\n",
    "apply_augmentation = Compose(transforms=[AddColoredNoise(p = 1) ,TimeInversion( p = 1) ,PolarityInversion(p = 1)])\n",
    "\n",
    "#apply_augmentation = Compose(transforms=[PolarityInversion(p=0.5 ,output_type = 'tensor'),AddColoredNoise(), PeakNormalization(apply_to=\"only_too_loud_sounds\"),TimeInversion(output_type = 'tensor')  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a95c8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MozTrainDataset(Dataset):\n",
    "\n",
    "    def __init__(self, audio_df, data_dir, min_length, cache=None, transform=apply_augmentation):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            noise_df (DataFrame): the df of noise files and lengths\n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        #self.noise_df = noise_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    \n",
    "    def _get_sample_(self, path, resample=None):\n",
    "        \n",
    "        waveform, inp_rate = torchaudio.load(path)\n",
    "        \n",
    "        if inp_rate != config.rate:\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_rate, config.rate, dtype=waveform.dtype)\n",
    "            waveform = resampler(waveform)\n",
    "    \n",
    "        \n",
    "        #waveform, rate = torchaudio.load(path)\n",
    "                \n",
    "        if waveform.shape[1] < config.rate*self.min_length:\n",
    "            #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "            f_out = pad_mean(waveform)\n",
    "        else:\n",
    "            f = waveform[0]\n",
    "            f_out = f.unsqueeze(0)\n",
    "            \n",
    "        return f_out\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #real_idx = idx % len(self.audio_df)\n",
    "        if DEBUG:\n",
    "            print(\"idx = \" + str(idx))\n",
    "        x = self._get_sample_(os.path.join(self.data_dir,f\"{int(self.audio_df.loc[idx]['id'])}.wav\"), resample=config.rate)\n",
    "        if DEBUG:\n",
    "            print(\"shape of x post augmentation = \" + str(x.shape))\n",
    "            \n",
    "        \n",
    "        # random noise on even number indexes\n",
    "        offset = int(self.audio_df.loc[idx]['offset'])\n",
    "        \n",
    "        return (x[:,offset:int(offset+config.rate*self.min_length)],self.audio_df.loc[idx]['specie_ind'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b102b27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MozTestDataset(Dataset):\n",
    "\n",
    "    def __init__(self, audio_df, data_dir, min_length, cache=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            noise_df (DataFrame): the df of noise files and lengths\n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        #self.noise_df = noise_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    \n",
    "    def _get_sample_(self, path, resample=None):\n",
    "        \n",
    "        waveform, inp_rate = torchaudio.load(path)\n",
    "        \n",
    "        if inp_rate != config.rate:\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_rate, config.rate, dtype=waveform.dtype)\n",
    "            waveform = resampler(waveform)\n",
    "    \n",
    "        \n",
    "        #waveform, rate = torchaudio.load(path)\n",
    "                \n",
    "        if waveform.shape[1] < config.rate*self.min_length:\n",
    "            #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "            f_out = pad_mean(waveform)\n",
    "        else:\n",
    "            f = waveform[0]\n",
    "            #mu = torch.std_mean(f)[1]\n",
    "            #st = torch.std_mean(f)[0]\n",
    "            # clip amplitudes\n",
    "            f_out = f.unsqueeze(0)\n",
    "            if self.cache is not None:\n",
    "                self.cache[path] = f_out\n",
    "        return f_out\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #real_idx = idx % len(self.audio_df)\n",
    "        if DEBUG:\n",
    "            print(\"idx = \" + str(idx))\n",
    "        x = self._get_sample_(os.path.join(self.data_dir,f\"{int(self.audio_df.loc[idx]['id'])}.wav\"), resample=config.rate)\n",
    "        \n",
    "        # random noise on even number indexes\n",
    "        offset = int(self.audio_df.loc[idx]['offset'])\n",
    "        \n",
    "        return (x[:,offset:int(offset+config.rate*self.min_length)],self.audio_df.loc[idx]['specie_ind'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3fbd8eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_aug(x,rate):\n",
    "        apply_augmentation = Compose(transforms=[AddColoredNoise(p = .75) ,TimeInversion( p = .75) ,PolarityInversion(p = .25)])\n",
    "        aug_audio = apply_augmentation(x,sample_rate = rate)\n",
    "        return(aug_audio)\n",
    "    \n",
    "\n",
    "class augment_audio(nn.Module):\n",
    "    \"\"\"This is a class to introduce randomness in the data.\n",
    "    We implement it as a layer in the NN to ensure that it learns from the propertis of the data\"\"\"\n",
    "    def __init__(self , trainable = True, sample_rate = config.rate):\n",
    "        super().__init__()\n",
    "        self.trainable = trainable\n",
    "        self.rate = sample_rate\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(dim = 1)\n",
    "           \n",
    "        if self.trainable:\n",
    "            x = apply_aug(x , self.rate)\n",
    "        else:\n",
    "            x = x\n",
    "#         x = x.unsqueeze(dim=1).permute((0,1,3,2))\n",
    "        return x.squeeze(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b578c459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass the pretrained model and make it a binary classification\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, model_name, image_size):\n",
    "        super().__init__()\n",
    "        # num_classes=0 removes the pretrained head\n",
    "        self.backbone = timm.create_model(model_name,\n",
    "                        pretrained=True, num_classes=8, in_chans=1, \n",
    "                        drop_path_rate=0.2, global_pool='max',\n",
    "                        drop_rate=0.2)\n",
    "        #####  This section is model specific\n",
    "        #### It freezes some fo the layers by name\n",
    "        #### you'll have to inspect the model to see the names\n",
    "                #### end layer freezing\n",
    "        self.spec_layer = features.STFT(n_fft=config.NFFT, freq_bins=None, hop_length=config.n_hop,\n",
    "                              window='hann', freq_scale='linear', center=True, pad_mode='reflect',\n",
    "                           sr=config.rate, output_format=\"Magnitude\", trainable=True,)\n",
    "        self.out = nn.Linear(self.backbone.num_features, 1)\n",
    "        self.sizer = VT.Resize((image_size,image_size))\n",
    "        self.timeMasking = T.TimeMasking(time_mask_param=int(config.win_size*0.4), iid_masks=True)\n",
    "        self.freqMasking = T.FrequencyMasking(freq_mask_param=int((config.NFFT//4)*0.15), iid_masks=True)\n",
    "        self.norm_layer = Normalization(mode='framewise')\n",
    "        self.pcen_layer = PCENTransform(eps=1e-6, s=0.025, alpha=0.6, delta=0.1, r=0.2, trainable=True)\n",
    "        self.augment_layer = augment_audio(trainable = True, sample_rate = config.rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # first compute spectrogram\n",
    "        spec = self.augment_layer(x.squeeze())\n",
    "        spec = self.spec_layer(x)  # (B, F, T)\n",
    "        # normalize\n",
    "#         spec = spec.transpose(1,2) # (B, T, F)\n",
    "        spec = self.pcen_layer(spec)\n",
    "        spec = self.norm_layer(spec)\n",
    "        \n",
    "#         if self.training:\n",
    "        spec = self.timeMasking(spec)\n",
    "        spec = self.freqMasking(spec)\n",
    "\n",
    "        # then size for CNN model\n",
    "        # and create a channel\n",
    "        spec = self.sizer(spec)\n",
    "        x = spec.unsqueeze(1)\n",
    "        # then repeat channels\n",
    "        x = self.backbone(x)\n",
    "        #print(\"x shape = \" + str(x.shape))\n",
    "        #print(\"x = \" +str(x))\n",
    "        #pred = nn.Softmax(x)\n",
    "        pred = x\n",
    "        #print(np.argmax(pred.detach().cpu().numpy()))\n",
    "        #print(pred)\n",
    "        output = {\"prediction\": pred,\n",
    "                  \"spectrogram\": spec}\n",
    "        #print(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b4a511db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MozTrainDataset(df_train_offset,  config.data_dir, min_length , transform = apply_augmentation)\n",
    "val_dataset = MozTestDataset(df_val_offset,  config.data_dir, min_length)\n",
    "test_dataset = MozTestDataset(df_test_offset,  config.data_dir, min_length)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, num_workers=num_workers,batch_size = batch_size,shuffle = True\n",
    "    , pin_memory=True )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size,\n",
    "        num_workers=num_workers, pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size,\n",
    "        num_workers= num_workers, pin_memory=pin_memory,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ba83d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f44598c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset = 67822\n",
      "Length of train loader = 1060\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of train dataset = \" +str(len(train_dataset)))\n",
    "print(\"Length of train loader = \" +str(len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2b5ef3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_itr = iter(train_loader)\n",
    "# a,b = train_itr.next()\n",
    "# print(a.shape)\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f55cb332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# a = torch.rand(32,15360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "599ebed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_mod = Model('convnext_small',224)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3c418674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_mod(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d74032",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2ff55adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling rate = 8000. Please make sure the sampling rate is correct in order toget a valid freq range\n",
      "STFT kernels created, time used = 0.0769 seconds\n"
     ]
    }
   ],
   "source": [
    "def load_model(filepath, model=Model('convnext_small',224)):\n",
    "    # Instantiate model to inspect\n",
    "    print(\"Filepath = \" + str(filepath))\n",
    "    print(\"model = \" +str(model))\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "    print(f'Training on {device}')\n",
    "        \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "    model = model.to(device)\n",
    "    # Load trained parameters from checkpoint (may need to download from S3 first)\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        map_location=lambda storage, loc: storage.cuda()\n",
    "    else:\n",
    "        map_location='cpu'\n",
    "        \n",
    "    checkpoint = model.load_state_dict(torch.load(filepath))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "38cfd21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling rate = 8000. Please make sure the sampling rate is correct in order toget a valid freq range\n",
      "STFT kernels created, time used = 0.1002 seconds\n",
      "Filepath = ../outputs/models/pytorch/model_e8_2022_09_20_14_16_42.pth\n",
      "model = Model(\n",
      "  (backbone): ConvNeXt(\n",
      "    (stem): Sequential(\n",
      "      (0): Conv2d(1, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "    (stages): Sequential(\n",
      "      (0): ConvNeXtStage(\n",
      "        (downsample): Identity()\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ConvNeXtStage(\n",
      "        (downsample): Sequential(\n",
      "          (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ConvNeXtStage(\n",
      "        (downsample): Sequential(\n",
      "          (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (3): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (4): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (5): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (6): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (7): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (8): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (9): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (10): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (11): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (12): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (13): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (14): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (15): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (16): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (17): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (18): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (19): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (20): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (21): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (22): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (23): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (24): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (25): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (26): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): ConvNeXtStage(\n",
      "        (downsample): Sequential(\n",
      "          (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm_pre): Identity()\n",
      "    (head): Sequential(\n",
      "      (global_pool): SelectAdaptivePool2d (pool_type=max, flatten=Identity())\n",
      "      (norm): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (drop): Dropout(p=0.2, inplace=False)\n",
      "      (fc): Linear(in_features=768, out_features=8, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (spec_layer): STFT(n_fft=2048, Fourier Kernel size=(1025, 1, 2048), iSTFT=False, trainable=True)\n",
      "  (out): Linear(in_features=768, out_features=1, bias=True)\n",
      "  (sizer): Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
      "  (timeMasking): TimeMasking()\n",
      "  (freqMasking): FrequencyMasking()\n",
      "  (pcen_layer): PCENTransform()\n",
      "  (augment_layer): augment_audio()\n",
      ")\n",
      "Training on cuda:0\n",
      "Training on cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3a6c82e5b240448d8748cb5377cff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0batch = 0 of 1035duraation = 0.03740012248357137\n",
      "epoch = 0batch = 500 of 1035duraation = 5.2474602063496905\n",
      "epoch = 0batch = 1000 of 1035duraation = 10.452250218391418\n",
      "Epoch: 0, Train Loss: 0.48683379, Train f1: 0.79962531, Val Loss: 0.00264806, Val f1: 0.74187082, overrun_counter -1\n",
      "Saving model to: ../outputs/models/pytorch/model_e0_2022_09_20_19_48_54.pth\n",
      "Now printing classification rport... \n",
      "********************************\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "        an arabiensis       0.89      0.66      0.75      8919\n",
      "culex pipiens complex       0.66      0.66      0.66      4194\n",
      "           ae aegypti       0.51      0.91      0.65       488\n",
      "       an funestus ss       0.68      0.79      0.73      4037\n",
      "         an squamosus       0.47      0.76      0.58       964\n",
      "          an coustani       0.40      0.65      0.50       327\n",
      "         ma uniformis       0.61      0.85      0.71       978\n",
      "         ma africanus       0.40      0.57      0.47       222\n",
      "\n",
      "             accuracy                           0.70     20129\n",
      "            macro avg       0.58      0.73      0.63     20129\n",
      "         weighted avg       0.74      0.70      0.71     20129\n",
      "\n",
      "********************************\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d3896f8cb24e378e3907395baf89d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1batch = 0 of 1035duraation = 0.04027980963389079\n",
      "epoch = 1batch = 500 of 1035duraation = 5.2506460269292194\n",
      "epoch = 1batch = 1000 of 1035duraation = 10.432986903190614\n",
      "..Overrun....no improvement\n",
      "Epoch: 1, Train Loss: 0.31946788, Train f1: 0.84183388, Val Loss: 0.00276559, Val f1: 0.73594971, overrun_counter 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470928a6aa7a452eaf3dd9cd18d61ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 2batch = 0 of 1035duraation = 0.03609832525253296\n",
      "epoch = 2batch = 500 of 1035duraation = 5.227116862932841\n",
      "epoch = 2batch = 1000 of 1035duraation = 10.431941576798756\n",
      "..Overrun....no improvement\n",
      "Epoch: 2, Train Loss: 0.26131036, Train f1: 0.86539258, Val Loss: 0.00289448, Val f1: 0.72117265, overrun_counter 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034977bc2c524f6b8f187286e643231c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 3batch = 0 of 1035duraation = 0.038141298294067386\n",
      "epoch = 3batch = 500 of 1035duraation = 5.241846493879954\n",
      "epoch = 3batch = 1000 of 1035duraation = 10.43714119195938\n",
      "..Overrun....no improvement\n",
      "Epoch: 3, Train Loss: 0.24411683, Train f1: 0.87825893, Val Loss: 0.00322460, Val f1: 0.70584471, overrun_counter 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d66474e50042b594bf622a464cd4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 4batch = 0 of 1035duraation = 0.03677437702814738\n",
      "epoch = 4batch = 500 of 1035duraation = 5.229526929060618\n",
      "epoch = 4batch = 1000 of 1035duraation = 10.432235666116078\n",
      "..Overrun....no improvement\n",
      "Epoch: 4, Train Loss: 0.22580362, Train f1: 0.88674179, Val Loss: 0.00384376, Val f1: 0.67719548, overrun_counter 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9cda955f37b436a894aee82140a3cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 5batch = 0 of 1035duraation = 0.03779154618581136\n",
      "epoch = 5batch = 500 of 1035duraation = 5.236344468593598\n",
      "epoch = 5batch = 1000 of 1035duraation = 10.419344727198283\n",
      "..Overrun....no improvement\n",
      "Epoch: 5, Train Loss: 0.20128962, Train f1: 0.89967387, Val Loss: 0.00373263, Val f1: 0.67583337, overrun_counter 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f28739275ee4e1eb85975313f6dd3f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 6batch = 0 of 1035duraation = 0.03668320178985596\n",
      "epoch = 6batch = 500 of 1035duraation = 5.250664754708608\n",
      "epoch = 6batch = 1000 of 1035duraation = 10.42459409236908\n",
      "..Overrun....no improvement\n",
      "Epoch: 6, Train Loss: 0.18401730, Train f1: 0.90924982, Val Loss: 0.00393801, Val f1: 0.66685652, overrun_counter 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b253eca0c944aeb9c5aab55bd51541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 7batch = 0 of 1035duraation = 0.037197371323903404\n",
      "epoch = 7batch = 500 of 1035duraation = 5.235582149028778\n",
      "epoch = 7batch = 1000 of 1035duraation = 10.428166230519613\n",
      "..Overrun....no improvement\n",
      "Epoch: 7, Train Loss: 0.17371991, Train f1: 0.91320124, Val Loss: 0.00416138, Val f1: 0.66368967, overrun_counter 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80fb064c9734acfb11746cd5e090f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 8batch = 0 of 1035duraation = 0.0397361159324646\n",
      "epoch = 8batch = 500 of 1035duraation = 5.211716898282369\n",
      "epoch = 8batch = 1000 of 1035duraation = 10.399002452691397\n",
      "..Overrun....no improvement\n",
      "Epoch: 8, Train Loss: 0.16071331, Train f1: 0.92013326, Val Loss: 0.00460465, Val f1: 0.63610601, overrun_counter 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc903b416bcd45cb9ff869aaf1b68451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 9batch = 0 of 1035duraation = 0.03695453405380249\n",
      "epoch = 9batch = 500 of 1035duraation = 5.219287526607514\n",
      "epoch = 9batch = 1000 of 1035duraation = 10.432139674822489\n",
      "..Overrun....no improvement\n",
      "Epoch: 9, Train Loss: 0.15441897, Train f1: 0.92372412, Val Loss: 0.00421091, Val f1: 0.66700407, overrun_counter 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c27ffd21e94ef3829716c2b0e2c6a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 10batch = 0 of 1035duraation = 0.036754930019378663\n",
      "epoch = 10batch = 500 of 1035duraation = 5.239653853575389\n",
      "epoch = 10batch = 1000 of 1035duraation = 10.43583385547002\n",
      "..Overrun....no improvement\n",
      "Epoch: 10, Train Loss: 0.12771745, Train f1: 0.93451058, Val Loss: 0.00415464, Val f1: 0.66909413, overrun_counter 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f773247d02c480ead1a2780a4a1ca3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 11batch = 0 of 1035duraation = 0.03902362585067749\n",
      "epoch = 11batch = 500 of 1035duraation = 5.240635279814402\n",
      "epoch = 11batch = 1000 of 1035duraation = 10.4499582529068\n",
      "..Overrun....no improvement\n",
      "Epoch: 11, Train Loss: 0.12303954, Train f1: 0.93857977, Val Loss: 0.00458928, Val f1: 0.66070964, overrun_counter 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2b847197694b6fba4108626e09ca75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 12batch = 0 of 1035duraation = 0.03842443227767944\n",
      "epoch = 12batch = 500 of 1035duraation = 5.255913313229879\n",
      "epoch = 12batch = 1000 of 1035duraation = 10.436446245511373\n",
      "..Overrun....no improvement\n",
      "Epoch: 12, Train Loss: 0.12525761, Train f1: 0.93820782, Val Loss: 0.00493833, Val f1: 0.63141349, overrun_counter 11\n"
     ]
    }
   ],
   "source": [
    "model =Model('convnext_small',224)\n",
    "filepath = \"../outputs/models/pytorch/model_e8_2022_09_20_14_16_42.pth\"\n",
    "model_epcoh_10 = load_model(filepath,model)\n",
    "model, lr_log = train_model(train_loader, val_loader, test_loader,model_epcoh_10, classes ,class_weights ,num_epochs = num_epochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aad1023a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 9, does not match size of target_names, 8. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2314897/1820390444.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprediction\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mLabel\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   1985\u001b[0m             )\n\u001b[1;32m   1986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1987\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1988\u001b[0m                 \u001b[0;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1989\u001b[0m                 \u001b[0;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of classes, 9, does not match size of target_names, 8. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "prediction  = [2.0, 3.0, 8.0, 2.0, 8.0, 8.0, 4.0, 0.0, 7.0, 8.0, 8.0, 2.0, 8.0, 0.0, 1.0, 3.0, 4.0, 8.0, 5.0, 8.0, 2.0, 2.0, 2.0, 0.0, 5.0, 5.0, 3.0, 8.0, 4.0, 1.0, 2.0, 5.0, 6.0, 1.0, 8.0, 0.0, 2.0, 4.0, 6.0, 8.0, 7.0, 0.0, 7.0, 0.0, 8.0, 5.0, 8.0, 2.0, 0.0, 0.0, 4.0, 7.0, 5.0, 4.0, 1.0, 2.0, 4.0, 1.0, 6.0, 4.0, 3.0, 7.0, 8.0, 8.0, 0.0, 6.0, 7.0, 1.0, 6.0, 5.0, 7.0, 0.0, 5.0, 0.0, 0.0, 2.0, 1.0, 5.0, 8.0, 1.0, 5.0, 7.0, 5.0, 7.0, 3.0, 6.0, 6.0, 6.0, 2.0, 6.0, 2.0, 6.0, 6.0, 3.0, 6.0, 6.0, 0.0, 4.0, 6.0, 6.0, 0.0, 8.0, 7.0, 1.0, 4.0, 1.0, 3.0, 0.0, 8.0, 6.0, 5.0, 7.0, 7.0, 3.0, 2.0, 0.0, 4.0, 3.0, 4.0, 2.0, 4.0, 2.0, 7.0, 3.0, 1.0, 3.0, 6.0, 5.0, 5.0, 2.0, 0.0, 2.0, 0.0, 6.0, 3.0, 0.0, 3.0, 4.0, 8.0, 6.0, 4.0, 6.0, 0.0, 4.0, 5.0, 2.0, 6.0, 1.0, 1.0, 5.0, 4.0, 6.0, 5.0, 8.0, 0.0, 3.0, 4.0, 4.0, 4.0, 8.0, 8.0, 5.0, 5.0, 0.0, 1.0, 3.0, 3.0, 7.0, 7.0, 1.0, 5.0, 7.0, 6.0, 5.0, 8.0, 3.0, 8.0, 5.0, 2.0, 3.0, 7.0, 3.0, 7.0, 8.0, 4.0, 2.0, 0.0, 6.0, 8.0, 1.0, 3.0, 6.0, 2.0, 2.0, 7.0, 7.0, 2.0, 0.0, 4.0, 8.0]\n",
    "Label   = [3.0, 5.0, 0.0, 0.0, 2.0, 0.0, 1.0, 3.0, 4.0, 7.0, 1.0, 3.0, 2.0, 1.0, 4.0, 1.0, 3.0, 8.0, 8.0, 8.0, 3.0, 4.0, 7.0, 6.0, 4.0, 1.0, 3.0, 0.0, 7.0, 7.0, 3.0, 1.0, 4.0, 3.0, 5.0, 3.0, 4.0, 2.0, 8.0, 8.0, 7.0, 4.0, 7.0, 2.0, 6.0, 1.0, 8.0, 4.0, 8.0, 0.0, 1.0, 4.0, 2.0, 2.0, 0.0, 8.0, 1.0, 7.0, 4.0, 1.0, 2.0, 1.0, 2.0, 3.0, 6.0, 3.0, 6.0, 5.0, 4.0, 0.0, 5.0, 8.0, 5.0, 7.0, 1.0, 1.0, 6.0, 8.0, 6.0, 6.0, 4.0, 6.0, 6.0, 6.0, 8.0, 4.0, 5.0, 8.0, 0.0, 7.0, 4.0, 1.0, 6.0, 1.0, 5.0, 5.0, 5.0, 3.0, 8.0, 7.0, 8.0, 1.0, 2.0, 7.0, 2.0, 0.0, 4.0, 3.0, 2.0, 5.0, 1.0, 1.0, 2.0, 3.0, 0.0, 4.0, 7.0, 5.0, 2.0, 2.0, 6.0, 7.0, 2.0, 2.0, 6.0, 4.0, 2.0, 5.0, 8.0, 3.0, 6.0, 1.0, 1.0, 6.0, 5.0, 0.0, 2.0, 5.0, 3.0, 3.0, 0.0, 3.0, 3.0, 3.0, 1.0, 4.0, 3.0, 0.0, 6.0, 3.0, 7.0, 1.0, 2.0, 7.0, 3.0, 6.0, 1.0, 3.0, 8.0, 3.0, 6.0, 1.0, 4.0, 4.0, 6.0, 2.0, 5.0, 3.0, 0.0, 5.0, 7.0, 3.0, 5.0, 6.0, 4.0, 7.0, 1.0, 1.0, 6.0, 1.0, 4.0, 5.0, 8.0, 4.0, 6.0, 4.0, 5.0, 1.0, 6.0, 0.0, 3.0, 3.0, 3.0, 4.0, 7.0, 5.0, 2.0, 6.0, 2.0, 4.0]\n",
    "print(classification_report(np.array(Label), np.array(prediction), target_names= classes))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "classes\n",
    "cm = confusion_matrix(prediction, Label ,labels= range(0,9))\n",
    "print(cm)\n",
    "import seaborn as sns\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cellsplt.xticks(rotation=90)\n",
    "ax.xaxis.set_ticklabels(classes, fontsize = 10)\n",
    "ax.xaxis.tick_bottom()\n",
    "plt.xticks(rotation=90)\n",
    "ax.set_ylabel('True', fontsize=20)\n",
    "ax.yaxis.set_ticklabels(classes, fontsize = 10)\n",
    "plt.yticks(rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94520c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "pred = []\n",
    "for i in range(10):\n",
    "    label.append(np.random.rand(9))\n",
    "    pred.append(np.random.rand(9))\n",
    "print(label)\n",
    "print(pred)\n",
    "print(classification_report(label, pred, target_names= classes, labels= classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e61d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.tensor(8, device = \"cuda\")\n",
    "print(label)\n",
    "label_cpu = label.cpu().detach()\n",
    "print(label_cpu)\n",
    "label_np = label_cpu.numpy()\n",
    "print(type(label_np))\n",
    "label_np_item = label_np.item()\n",
    "print(type(label_np_item))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98876894",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.randn(4,9)\n",
    "y_pred.shape\n",
    "#y_pred_np = y_pred.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761b3168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_np\n",
    "# y_pred_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a5ae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.argmax(y_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc25088",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2b6979",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcff378",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,(x,y) in enumerate(test_loader):\n",
    "    print(\"idx = \" + str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bfca5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46ada38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa7d35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
