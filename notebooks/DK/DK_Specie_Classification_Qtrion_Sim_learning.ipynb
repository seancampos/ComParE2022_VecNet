{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff63cba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".CodeMirror{\n",
       "font-size: 14px;\n",
       "</style>\n",
       "CUDA_LAUNCH_BLOCKING=1\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style type='text/css'>\n",
    ".CodeMirror{\n",
    "font-size: 14px;\n",
    "</style>\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a8838e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: quaterion in /opt/conda/lib/python3.8/site-packages (0.1.30)\n",
      "Requirement already satisfied: loguru<0.6.0,>=0.5.3 in /opt/conda/lib/python3.8/site-packages (from quaterion) (0.5.3)\n",
      "Requirement already satisfied: mmh3<4.0.0,>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from quaterion) (3.0.0)\n",
      "Requirement already satisfied: rich<13.0.0,>=12.4.4 in /opt/conda/lib/python3.8/site-packages (from quaterion) (12.5.1)\n",
      "Requirement already satisfied: quaterion-models<0.2.0,>=0.1.16 in /opt/conda/lib/python3.8/site-packages (from quaterion) (0.1.16)\n",
      "Requirement already satisfied: torch>=1.8.2 in /opt/conda/lib/python3.8/site-packages (from quaterion) (1.11.0+cu113)\n",
      "Requirement already satisfied: torchmetrics<=0.8.2 in /opt/conda/lib/python3.8/site-packages (from quaterion) (0.8.2)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.8/site-packages (from quaterion) (3.19.1)\n",
      "Requirement already satisfied: pytorch-lightning<2.0.0,>=1.6.4 in /opt/conda/lib/python3.8/site-packages (from quaterion) (1.7.4)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning<2.0.0,>=1.6.4->quaterion) (1.22.4)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning<2.0.0,>=1.6.4->quaterion) (2.10.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning<2.0.0,>=1.6.4->quaterion) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning<2.0.0,>=1.6.4->quaterion) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning<2.0.0,>=1.6.4->quaterion) (4.0.1)\n",
      "Requirement already satisfied: pyDeprecate>=0.3.1 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning<2.0.0,>=1.6.4->quaterion) (0.3.2)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning<2.0.0,>=1.6.4->quaterion) (6.0)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning<2.0.0,>=1.6.4->quaterion) (2021.11.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (2.26.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (3.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=17.0->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (3.0.6)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/conda/lib/python3.8/site-packages (from rich<13.0.0,>=12.4.4->quaterion) (2.10.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/conda/lib/python3.8/site-packages (from rich<13.0.0,>=12.4.4->quaterion) (0.9.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (1.0.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (0.37.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (2.3.3)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (1.42.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (3.3.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (59.4.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (2.0.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (1.8.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from absl-py>=0.4->tensorboard>=2.9.1->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (4.11.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (3.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (2022.6.15)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (3.1.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (1.8.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning<2.0.0,>=1.6.4->quaterion) (1.3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install quaterion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ae34896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had to find the right version of pytorch with the widget here https://pytorch.org/\n",
    "# I *think* this will work with AWS\n",
    "#!pip3 install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d1206d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: timm in /opt/conda/lib/python3.8/site-packages (0.6.7)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.0.1)\n",
      "Requirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.8/site-packages (from timm) (1.11.0+cu113)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (from timm) (0.12.0+cu113)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (6.6.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.1.0)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.22)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (4.11.1)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.5.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.4->timm) (4.0.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchvision->timm) (1.22.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from torchvision->timm) (2.26.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.8/site-packages (from torchvision->timm) (9.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->timm) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->timm) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->timm) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->timm) (2022.6.15)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# other dependencies\n",
    "!pip install timm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a59be2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting git+https://github.com/KinWaiCheuk/nnAudio.git#subdirectory=Installation\n",
      "  Cloning https://github.com/KinWaiCheuk/nnAudio.git to /tmp/pip-req-build-ofs927w2\n",
      "  Running command git clone -q https://github.com/KinWaiCheuk/nnAudio.git /tmp/pip-req-build-ofs927w2\n",
      "  Resolved https://github.com/KinWaiCheuk/nnAudio.git to commit 744fab12497a5316153978de2e97422c9c7389e0\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from nnAudio==0.3.1) (1.6.3)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /opt/conda/lib/python3.8/site-packages (from scipy->nnAudio==0.3.1) (1.22.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## nnAudio\n",
    "!pip install git+https://github.com/KinWaiCheuk/nnAudio.git#subdirectory=Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84446b0f",
   "metadata": {},
   "source": [
    "### 1 Import the kitchen sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f7860d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
     ]
    }
   ],
   "source": [
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "040a11cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug main imports\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../lib'))\n",
    "import config\n",
    "from evaluate import get_results\n",
    "import numpy as np\n",
    "\n",
    "# Troubleshooting and visualisation\n",
    "import IPython.display as ipd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37e13d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug lib imports\n",
    "from sklearn.metrics import accuracy_score\n",
    "from PyTorch import config_pytorch\n",
    "from datetime import datetime\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "396fac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional pytorch tools\n",
    "import random\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchvision.transforms as VT\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "import timm\n",
    "import timm.optim\n",
    "from timm.loss import BinaryCrossEntropy\n",
    "from timm.utils import NativeScaler\n",
    "from timm.models import model_parameters\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5deb383",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nnAudio\n",
    "from nnAudio import features\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba59fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarity learning\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from typing import Callable\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "from quaterion.dataset import (GroupSimilarityDataLoader, SimilarityGroupSample)\n",
    "import torchvision\n",
    "from quaterion_models.encoders import Encoder\n",
    "from quaterion_models.heads import EncoderHead, SkipConnectionHead\n",
    "from torch import nn\n",
    "from typing import Dict, Union, Optional, List\n",
    "\n",
    "from quaterion import TrainableModel\n",
    "from quaterion.eval.attached_metric import AttachedMetric\n",
    "from quaterion.eval.group import RetrievalRPrecision\n",
    "from quaterion.loss import SimilarityLoss, TripletLoss\n",
    "from quaterion.train.cache import CacheConfig, CacheType\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "from quaterion_models.encoders import Encoder\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelSummary\n",
    "\n",
    "from quaterion import Quaterion\n",
    "#from .data import get_dataloaders\n",
    "#from .models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b53b8d0",
   "metadata": {},
   "source": [
    "### Run all these function definition cells\n",
    "These have been extracted from the lib folder and are here to make them more easily editable.  Most of the action happens in *get_feat_torch*, which does feature extraction and *train_model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de2a7a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filepath, model):\n",
    "    # Instantiate model to inspect\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "    print(f'Training on {device}')\n",
    "        \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "    model = model.to(device)\n",
    "    # Load trained parameters from checkpoint (may need to download from S3 first)\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        map_location=lambda storage, loc: storage.cuda()\n",
    "    else:\n",
    "        map_location='cpu'\n",
    "        \n",
    "    checkpoint = model.load_state_dict(torch.load(filepath))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15cbc442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion, class_threshold=0.5, device=None):\n",
    "    with torch.no_grad():\n",
    "        if device is None:\n",
    "            torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        sigmoid = nn.Sigmoid()\n",
    "        test_loss = 0.0\n",
    "        model.eval()\n",
    "        \n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        counter = 1\n",
    "        for x, y, idx in tqdm(test_loader, desc='validation', leave=True):\n",
    "            \n",
    "            x, y = x.to(device), y.unsqueeze(1).float().to(device)\n",
    "            \n",
    "            y_pred = model(x)['prediction']\n",
    "                        \n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            all_y.append(y.cpu().detach())\n",
    "            all_y_pred.append(y_pred.cpu().detach())\n",
    "            \n",
    "            del x\n",
    "            del y\n",
    "            del y_pred\n",
    "            \n",
    "            counter +=1\n",
    "\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        \n",
    "        test_loss = test_loss/len(test_loader)\n",
    "        test_acc = accuracy_score(all_y.numpy(), (sigmoid(all_y_pred).numpy() > class_threshold).astype(float))\n",
    "    \n",
    "    \n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a535011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, val_loader, model = None, n_channels = 1):\n",
    "    # Creates a GradScaler once at the beginning of training.\n",
    "    loss_scaler = NativeScaler()\n",
    "    global_step = 0\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Training on {device}')    \n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Change compatibility to other loss function, cross-test with main.\n",
    "    #criterion = nn.BCEWithLogitsLoss()\n",
    "    criterion = BinaryCrossEntropy(smoothing=0.1)\n",
    "    #optimiser = optim.Adam(model.parameters(), lr=config_pytorch.lr)\n",
    "#     optimiser = timm.optim.AdamP(model.parameters(), lr=config_pytorch.lr/10)\n",
    "    optimiser = timm.optim.RAdam(model.parameters(), lr=config_pytorch.lr/10)\n",
    "    #optimiser = timm.optim.Lookahead(optimiser)\n",
    "    \n",
    "    num_steps = len(train_loader)\n",
    "    num_epochs = config_pytorch.epochs\n",
    "    \n",
    "    # 2 cycle CosineLR\n",
    "#     scheduler = scheduler = CosineLRScheduler(\n",
    "#             optimiser,\n",
    "#             t_initial= num_steps * (num_epochs // 2) + 1,\n",
    "#             lr_min=5e-7,\n",
    "#             noise_range_t=(0, num_steps*(num_epochs // 2)),\n",
    "#             noise_pct=0.1,\n",
    "#             warmup_lr_init=5e-7,\n",
    "#             warmup_t= num_steps * config_pytorch.n_warmup_epochs + 1, cycle_limit=20)\n",
    "    \n",
    "\n",
    "\n",
    "    all_train_loss = []\n",
    "    all_train_acc = []\n",
    "    all_val_loss = []\n",
    "    all_val_acc = []\n",
    "    best_val_loss = np.inf\n",
    "    best_val_acc = -np.inf\n",
    "\n",
    "    # best_train_loss = np.inf\n",
    "    best_train_acc = -np.inf\n",
    "\n",
    "    best_epoch = -1\n",
    "    checkpoint_name = None\n",
    "    overrun_counter = 0\n",
    "    \n",
    "    sigmoid = nn.Sigmoid()\n",
    "    lr_log = []\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        tk0 = tqdm(train_loader, total=int(len(train_loader)))\n",
    "        for batch_i, inputs in enumerate(tk0):\n",
    "#             scheduler.step(global_step)\n",
    "\n",
    "            x = inputs[0].to(device)\n",
    "            y = inputs[1].unsqueeze(1).float().to(device)\n",
    "\n",
    "            global_step += 1\n",
    "            \n",
    "            if len(x) == 1:\n",
    "                x = x[0]\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            # AMP\n",
    "            with autocast():\n",
    "                # outputs = model(inputs)\n",
    "                y_pred = model(x)['prediction']\n",
    "                loss = criterion(y_pred, y)\n",
    "                \n",
    "            loss_scaler(\n",
    "                loss, optimiser,\n",
    "                parameters=model_parameters(model))\n",
    "    \n",
    "#             scaler.scale(loss).backward()\n",
    "#             scaler.step(optimiser)\n",
    "#           # Updates the scale for next iteration.\n",
    "#             scaler.update()\n",
    "\n",
    "#             loss.backward()\n",
    "#             optimiser.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            all_y.append(y.cpu().detach())\n",
    "            all_y_pred.append(y_pred.cpu().detach())\n",
    "            \n",
    "            lr_log.append(optimiser.param_groups[0]['lr'])\n",
    "            \n",
    "            tk0.set_postfix(training_loss=(train_loss / (batch_i+1)), lr=optimiser.param_groups[0]['lr'])\n",
    "            \n",
    "            del x\n",
    "            del y\n",
    "        \n",
    "#         optimiser.sync_lookahead()\n",
    "        \n",
    "        all_train_loss.append(train_loss/len(train_loader))\n",
    "\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        train_acc = accuracy_score(all_y.numpy(), (all_y_pred.numpy() > 0.0).astype(float))\n",
    "        all_train_acc.append(train_acc)\n",
    "\n",
    "\n",
    "        # Can add more conditions to support loss instead of accuracy. Use *-1 for loss inequality instead of acc\n",
    "        val_loss, val_acc = test_model(model, val_loader, criterion, 0.5, device=device)\n",
    "        all_val_loss.append(val_loss)\n",
    "        all_val_acc.append(val_acc)\n",
    "\n",
    "        acc_metric = val_acc\n",
    "        best_acc_metric = best_val_acc\n",
    "        \n",
    "        if acc_metric > best_acc_metric:  \n",
    "            # if checkpoint_name is not None:\n",
    "                # os.path.join(os.path.pardir, 'models', 'pytorch', checkpoint_name)\n",
    "\n",
    "            checkpoint_name = f'model_e{e}_{datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")}.pth'\n",
    "\n",
    "            torch.save(model.state_dict(), os.path.join(config.model_dir, 'pytorch', checkpoint_name))\n",
    "            print('Saving model to:', os.path.join(config.model_dir, 'pytorch', checkpoint_name)) \n",
    "            best_epoch = e\n",
    "#             best_train_acc = train_acc\n",
    "#             best_train_loss = train_loss\n",
    "#             if x_val is not None:\n",
    "            best_val_acc = val_acc\n",
    "            best_val_loss = val_loss\n",
    "            overrun_counter = -1\n",
    "\n",
    "        overrun_counter += 1\n",
    "        #if x_val is not None:\n",
    "        print('Epoch: %d, Train Loss: %.8f, Train Acc: %.8f, Val Loss: %.8f, Val Acc: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader), train_acc, val_loss/len(val_loader), val_acc,  overrun_counter))\n",
    "#         else:\n",
    "#             print('Epoch: %d, Train Loss: %.8f, Train Acc: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader), train_acc, overrun_counter))\n",
    "        if overrun_counter > config_pytorch.max_overrun:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    return model, lr_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97bd2355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, n_samples, n_channels=3):\n",
    "    with torch.no_grad():\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f'Evaluating on {device}')\n",
    "\n",
    "\n",
    "        y_preds_all = np.zeros([n_samples, len(test_loader.dataset), 2])\n",
    "        model.eval() # Important to not leak info from batch norm layers and cause other issues\n",
    "        \n",
    "        sigmoid = nn.Sigmoid()\n",
    "\n",
    "        for n in range(n_samples):\n",
    "            all_y_pred = []\n",
    "            all_y = []\n",
    "            for x, y, idx in tqdm(test_loader):\n",
    "                x, y = x.to(device), y.unsqueeze(1).float().to(device)\n",
    "\n",
    "                y_pred = model(x)['prediction']\n",
    "                all_y.append(y.cpu().detach())\n",
    "\n",
    "                all_y_pred.append(y_pred.cpu().detach())\n",
    "\n",
    "                del x\n",
    "                del y\n",
    "                del y_pred\n",
    "\n",
    "            all_y_pred = torch.cat(all_y_pred)\n",
    "            all_y = torch.cat(all_y)\n",
    "\n",
    "            y_preds_all[n,:,1] = np.array(sigmoid(all_y_pred).squeeze())\n",
    "            y_preds_all[n,:,0] = 1-np.array(sigmoid(all_y_pred).squeeze()) # Check ordering of classes (yes/no)\n",
    "            test_acc = accuracy_score(all_y.numpy(), (sigmoid(all_y_pred).numpy() > 0.5).astype(float))\n",
    "            print(test_acc)\n",
    "    return y_preds_all, all_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97dcfea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offsets_df(df, short_audio=False):\n",
    "    audio_offsets = []\n",
    "    min_length = config.win_size*config.NFFT/(((1/config.n_hop)*config.NFFT)*config.rate)\n",
    "    step_frac = config.step_size/config.win_size\n",
    "    for _,row in df.iterrows():\n",
    "        if row['length'] > min_length:\n",
    "            step_size = step_frac*min_length\n",
    "            audio_offsets.append({'id':row['id'], 'offset':0,'sound_type': row['sound_type'], 'length': row['length'],'specie_ind': row['specie_ind']})\n",
    "            for i in range(1, int((row['length']-min_length)//step_size)):\n",
    "                audio_offsets.append({'id': row['id'], 'offset':int(min_length+(i*step_size)*config.rate),'sound_type':row['sound_type'], 'length': row['length'],'specie_ind': row['specie_ind']})\n",
    "        elif short_audio:\n",
    "            audio_offsets.append({'id':row['id'], 'offset':0,'sound_type': row['sound_type'], 'length': row['length'],'specie_ind': row['specie_ind']})\n",
    "    return pd.DataFrame(audio_offsets)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b815f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_df(df_offset, indices):\n",
    "    list_df_ind = []\n",
    "    #print(\"len of indices = \" + str(len(indices)))\n",
    "    for ind in indices :\n",
    "        df_name = \"df_\"+ str(ind)\n",
    "        df_name = df_offset[df_offset['specie_ind'] == ind]\n",
    "        list_df_ind.append(df_name)\n",
    "    df_offset_trimmed = pd.concat(list_df_ind)\n",
    "    return(df_offset_trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dbc1a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.92"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the min length based on config params\n",
    "min_length = (config.win_size * config.n_hop) / config.rate\n",
    "min_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9974df27",
   "metadata": {},
   "source": [
    "### 3 The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da6fcfb",
   "metadata": {},
   "source": [
    "### Read CSV and get train/test groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0df6c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['an arabiensis','culex pipiens complex', 'ae aegypti','an funestus ss','an squamosus',\n",
    "               'an coustani','ma uniformis','ma africanus' , 'others']\n",
    "classes_no_other = ['an arabiensis','culex pipiens complex', 'ae aegypti','an funestus ss','an squamosus',\n",
    "               'an coustani','ma uniformis','ma africanus' ]\n",
    "other_ind = classes.index('others')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad051f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11e56cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv, retain only those values where specie information is provided\n",
    "df_temp = pd.read_csv(config.data_df)\n",
    "bool_specie = df_temp['species'].notnull()\n",
    "df = df_temp[bool_specie]\n",
    "   \n",
    "# filtering data\n",
    "# displaying data only with Gender = NaN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf46a5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>record_datetime</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>species</th>\n",
       "      <th>gender</th>\n",
       "      <th>fed</th>\n",
       "      <th>plurality</th>\n",
       "      <th>age</th>\n",
       "      <th>method</th>\n",
       "      <th>mic_type</th>\n",
       "      <th>device_type</th>\n",
       "      <th>country</th>\n",
       "      <th>district</th>\n",
       "      <th>province</th>\n",
       "      <th>place</th>\n",
       "      <th>location_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>0.463456</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>08-09-16 08:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>0.170249</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>08-09-16 08:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>0.104041</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>08-09-16 08:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69</td>\n",
       "      <td>0.274290</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>08-09-16 08:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56</td>\n",
       "      <td>0.420894</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>08-09-16 08:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Plural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    length                             name  sample_rate record_datetime  \\\n",
       "1  53  0.463456  CDC_Ae-aegypti_labelled_800.wav         8000  08-09-16 08:00   \n",
       "2  57  0.170249  CDC_Ae-aegypti_labelled_800.wav         8000  08-09-16 08:00   \n",
       "3  61  0.104041  CDC_Ae-aegypti_labelled_800.wav         8000  08-09-16 08:00   \n",
       "4  69  0.274290  CDC_Ae-aegypti_labelled_800.wav         8000  08-09-16 08:00   \n",
       "5  56  0.420894  CDC_Ae-aegypti_labelled_800.wav         8000  08-09-16 08:00   \n",
       "\n",
       "  sound_type     species gender  fed plurality  age method mic_type  \\\n",
       "1   mosquito  ae aegypti    NaN  NaN    Single  NaN    NaN    phone   \n",
       "2   mosquito  ae aegypti    NaN  NaN    Single  NaN    NaN    phone   \n",
       "3   mosquito  ae aegypti    NaN  NaN    Single  NaN    NaN    phone   \n",
       "4   mosquito  ae aegypti    NaN  NaN    Single  NaN    NaN    phone   \n",
       "5   mosquito  ae aegypti    NaN  NaN    Plural  NaN    NaN    phone   \n",
       "\n",
       "     device_type country district province                         place  \\\n",
       "1  Alcatel 4009X     USA  Georgia  Atlanta  CDC insect cultures, Atlanta   \n",
       "2  Alcatel 4009X     USA  Georgia  Atlanta  CDC insect cultures, Atlanta   \n",
       "3  Alcatel 4009X     USA  Georgia  Atlanta  CDC insect cultures, Atlanta   \n",
       "4  Alcatel 4009X     USA  Georgia  Atlanta  CDC insect cultures, Atlanta   \n",
       "5  Alcatel 4009X     USA  Georgia  Atlanta  CDC insect cultures, Atlanta   \n",
       "\n",
       "  location_type  \n",
       "1       culture  \n",
       "2       culture  \n",
       "3       culture  \n",
       "4       culture  \n",
       "5       culture  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9c382e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-2417d4f27acd>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['specie_ind'] = \"NULL_VAL\"\n"
     ]
    }
   ],
   "source": [
    "# adding a colum for specie encoding\n",
    "df['specie_ind'] = \"NULL_VAL\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a79a2014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>record_datetime</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>species</th>\n",
       "      <th>gender</th>\n",
       "      <th>fed</th>\n",
       "      <th>plurality</th>\n",
       "      <th>age</th>\n",
       "      <th>method</th>\n",
       "      <th>mic_type</th>\n",
       "      <th>device_type</th>\n",
       "      <th>country</th>\n",
       "      <th>district</th>\n",
       "      <th>province</th>\n",
       "      <th>place</th>\n",
       "      <th>location_type</th>\n",
       "      <th>specie_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>0.463456</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>08-09-16 08:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "      <td>NULL_VAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>0.170249</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>08-09-16 08:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "      <td>NULL_VAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>0.104041</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>08-09-16 08:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "      <td>NULL_VAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69</td>\n",
       "      <td>0.274290</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>08-09-16 08:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "      <td>NULL_VAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56</td>\n",
       "      <td>0.420894</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>08-09-16 08:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Plural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "      <td>NULL_VAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>3562</td>\n",
       "      <td>6.083093</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>01-07-18 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "      <td>NULL_VAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>3556</td>\n",
       "      <td>6.719908</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>01-07-18 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an maculatus</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "      <td>NULL_VAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9009</th>\n",
       "      <td>3553</td>\n",
       "      <td>6.128580</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>01-07-18 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an maculatus</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "      <td>NULL_VAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9011</th>\n",
       "      <td>3561</td>\n",
       "      <td>11.614280</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>01-07-18 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "      <td>NULL_VAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9012</th>\n",
       "      <td>3552</td>\n",
       "      <td>2.920249</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>01-07-18 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "      <td>NULL_VAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6008 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     length                             name  sample_rate  \\\n",
       "1       53   0.463456  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "2       57   0.170249  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "3       61   0.104041  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "4       69   0.274290  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "5       56   0.420894  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "...    ...        ...                              ...          ...   \n",
       "8999  3562   6.083093                    #988-1001.wav        44100   \n",
       "9000  3556   6.719908                    #988-1001.wav        44100   \n",
       "9009  3553   6.128580                    #988-1001.wav        44100   \n",
       "9011  3561  11.614280                    #988-1001.wav        44100   \n",
       "9012  3552   2.920249                    #988-1001.wav        44100   \n",
       "\n",
       "     record_datetime sound_type       species  gender  fed plurality  age  \\\n",
       "1     08-09-16 08:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "2     08-09-16 08:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "3     08-09-16 08:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "4     08-09-16 08:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "5     08-09-16 08:00   mosquito    ae aegypti     NaN  NaN    Plural  NaN   \n",
       "...              ...        ...           ...     ...  ...       ...  ...   \n",
       "8999  01-07-18 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "9000  01-07-18 12:00   mosquito  an maculatus  Female    t    Single  NaN   \n",
       "9009  01-07-18 12:00   mosquito  an maculatus  Female    t    Single  NaN   \n",
       "9011  01-07-18 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "9012  01-07-18 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "\n",
       "     method mic_type    device_type   country          district  \\\n",
       "1       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "2       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "3       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "4       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "5       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "...     ...      ...            ...       ...               ...   \n",
       "8999    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9000    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9009    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9011    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9012    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "\n",
       "                   province                            place location_type  \\\n",
       "1                   Atlanta     CDC insect cultures, Atlanta       culture   \n",
       "2                   Atlanta     CDC insect cultures, Atlanta       culture   \n",
       "3                   Atlanta     CDC insect cultures, Atlanta       culture   \n",
       "4                   Atlanta     CDC insect cultures, Atlanta       culture   \n",
       "5                   Atlanta     CDC insect cultures, Atlanta       culture   \n",
       "...                     ...                              ...           ...   \n",
       "8999  Kanchanaburi Province  field site near Pu Teuy Village           cup   \n",
       "9000  Kanchanaburi Province  field site near Pu Teuy Village           cup   \n",
       "9009  Kanchanaburi Province  field site near Pu Teuy Village           cup   \n",
       "9011  Kanchanaburi Province  field site near Pu Teuy Village           cup   \n",
       "9012  Kanchanaburi Province  field site near Pu Teuy Village           cup   \n",
       "\n",
       "     specie_ind  \n",
       "1      NULL_VAL  \n",
       "2      NULL_VAL  \n",
       "3      NULL_VAL  \n",
       "4      NULL_VAL  \n",
       "5      NULL_VAL  \n",
       "...         ...  \n",
       "8999   NULL_VAL  \n",
       "9000   NULL_VAL  \n",
       "9009   NULL_VAL  \n",
       "9011   NULL_VAL  \n",
       "9012   NULL_VAL  \n",
       "\n",
       "[6008 rows x 20 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a771195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specie = an arabiensisand its index = 0\n",
      "specie = culex pipiens complexand its index = 1\n",
      "specie = ae aegyptiand its index = 2\n",
      "specie = an funestus ssand its index = 3\n",
      "specie = an squamosusand its index = 4\n",
      "specie = an coustaniand its index = 5\n",
      "specie = ma uniformisand its index = 6\n",
      "specie = ma africanusand its index = 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    }
   ],
   "source": [
    "# Adding a new column to encode specie_index in the same order as the list \"classes\"\n",
    "ind = 0\n",
    "for specie in classes_no_other:\n",
    "    print(\"specie = \" + str(specie) + \"and its index = \" + str(ind) )\n",
    "    row_indexes=df[df['species']==specie].index \n",
    "    df.loc[row_indexes,'specie_ind']= ind\n",
    "    ind+=1\n",
    "\n",
    "    \n",
    "other_df_ind = df[df['specie_ind'] == \"NULL_VAL\"].index\n",
    "df.loc[other_df_ind,'specie_ind']= other_ind                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1171a5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SHORT_AUDIO = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2697970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f30f441",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offset = get_offsets_df(df, short_audio=USE_SHORT_AUDIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "926ebcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.7682284218474814, -0.6895085928726417, -0.3807096079158877, 0.2212761043995835, -1.1418371318893186}\n",
      "{0.7682284218474814, -0.6895085928726417, -0.3807096079158877, 0.2212761043995835, -1.1418371318893186}\n"
     ]
    }
   ],
   "source": [
    "a = set(np.random.randn(5))\n",
    "print(a)\n",
    "b = np.array(a)\n",
    "print(b)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b36ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tr_test_split(df):\n",
    "    # find out all the distinct classes and take it in a list\n",
    "    all_uniq_ind = df.specie_ind.unique()\n",
    "    print(\"all_indices in the df = \" +str(all_uniq_ind))\n",
    "    all_train_categories = set(np.random.choice(a= all_uniq_ind, size=math.ceil(len(all_uniq_ind) / 2), replace=False))\n",
    "    print(\"all_train_categories = \" +str(all_train_categories))\n",
    "    # get a list of labels for all samples in the df\n",
    "#     labels_list = []\n",
    "#     for ind,lab in enumerate(all_uniq_ind):\n",
    "#         #print(\"ind = \" +str(ind))\n",
    "#         labels_list.append(lab)\n",
    "#     print(\"labels_list = \" + str(labels_list))\n",
    "    # get a mask for indices where label is included in train_categories\n",
    "    #labels_mask = np.isin(all_uniq_ind, all_train_categories)\n",
    "    #print(\"label mask = \" + str(labels_mask))\n",
    "    # get a list of indices to be used as train samples\n",
    "    train_indices = all_train_categories\n",
    "    #train_indices = np.argwhere(labels_mask).squeeze()\n",
    "    print(\"train_indices = \" + str(train_indices))\n",
    "    # others will be used as test samples\n",
    "    test_indices = set(all_uniq_ind) - all_train_categories\n",
    "    #test_indices = np.argwhere(np.logical_not(labels_mask)).squeeze()\n",
    "    print(\"test_indices = \" + str(test_indices))\n",
    "    return train_indices,test_indices\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00c079d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_indices in the df = [2 8 0 7 6 1 3 4 5]\n",
      "all_train_categories = {1, 2, 3, 7, 8}\n",
      "train_indices = {1, 2, 3, 7, 8}\n",
      "test_indices = {0, 4, 5, 6}\n",
      "all_indices in the df = [1 2 3 7 8]\n",
      "all_train_categories = {8, 1, 2}\n",
      "train_indices = {8, 1, 2}\n",
      "test_indices = {3, 7}\n"
     ]
    }
   ],
   "source": [
    "train_indices_no_val,test_indices = get_tr_test_split(df_offset)\n",
    "df_train_temp_offset = concat_df(df_offset ,train_indices_no_val )\n",
    "df_test_offset = concat_df(df_offset ,test_indices )\n",
    "train_indices,val_indices = get_tr_test_split(df_train_temp_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56ef11d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a df for training indices\n",
    "df_train_offset = concat_df(df_train_temp_offset ,train_indices )\n",
    "df_val_offset = concat_df(df_train_temp_offset ,val_indices )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4bcd4479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>offset</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>length</th>\n",
       "      <th>specie_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>0.815149</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>0.496731</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>0.598625</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>0.732360</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>0.630467</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54804</th>\n",
       "      <td>221070</td>\n",
       "      <td>33281</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>7.680000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54805</th>\n",
       "      <td>221070</td>\n",
       "      <td>35841</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>7.680000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54806</th>\n",
       "      <td>221070</td>\n",
       "      <td>38401</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>7.680000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54807</th>\n",
       "      <td>221070</td>\n",
       "      <td>40961</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>7.680000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54808</th>\n",
       "      <td>221070</td>\n",
       "      <td>43521</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>7.680000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68975 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  offset sound_type    length  specie_ind\n",
       "34        110       0   mosquito  0.815149           8\n",
       "35        111       0   mosquito  0.496731           8\n",
       "36        112       0   mosquito  0.598625           8\n",
       "37        113       0   mosquito  0.732360           8\n",
       "38        114       0   mosquito  0.630467           8\n",
       "...       ...     ...        ...       ...         ...\n",
       "54804  221070   33281   mosquito  7.680000           2\n",
       "54805  221070   35841   mosquito  7.680000           2\n",
       "54806  221070   38401   mosquito  7.680000           2\n",
       "54807  221070   40961   mosquito  7.680000           2\n",
       "54808  221070   43521   mosquito  7.680000           2\n",
       "\n",
       "[68975 rows x 5 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "722234b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_indices in the df = [1 2 3 7 8]\n",
      "all_train_categories = {8, 2, 3}\n",
      "train_indices = {8, 2, 3}\n",
      "test_indices = {1, 7}\n"
     ]
    }
   ],
   "source": [
    "# now re- splitting into train and val\n",
    "train_indices,val_indices = get_tr_test_split(df_train_temp_offset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb09a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49526bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.reset_index(inplace = True)\n",
    "df_train_offset.reset_index(inplace = True)\n",
    "df_val_offset.reset_index(inplace = True)\n",
    "df_test_offset.reset_index(inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "731adf98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>offset</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>length</th>\n",
       "      <th>specie_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20258</td>\n",
       "      <td>221164</td>\n",
       "      <td>0</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>5.12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20259</td>\n",
       "      <td>221164</td>\n",
       "      <td>2561</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>5.12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20260</td>\n",
       "      <td>221164</td>\n",
       "      <td>5121</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>5.12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20261</td>\n",
       "      <td>221164</td>\n",
       "      <td>7681</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>5.12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20262</td>\n",
       "      <td>221164</td>\n",
       "      <td>10241</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>5.12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22757</th>\n",
       "      <td>123367</td>\n",
       "      <td>222470</td>\n",
       "      <td>74241</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>12.80</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22758</th>\n",
       "      <td>123368</td>\n",
       "      <td>222470</td>\n",
       "      <td>76801</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>12.80</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22759</th>\n",
       "      <td>123369</td>\n",
       "      <td>222470</td>\n",
       "      <td>79361</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>12.80</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22760</th>\n",
       "      <td>123370</td>\n",
       "      <td>222470</td>\n",
       "      <td>81921</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>12.80</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22761</th>\n",
       "      <td>123371</td>\n",
       "      <td>222470</td>\n",
       "      <td>84481</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>12.80</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22762 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index      id  offset sound_type  length  specie_ind\n",
       "0       20258  221164       0   mosquito    5.12           3\n",
       "1       20259  221164    2561   mosquito    5.12           3\n",
       "2       20260  221164    5121   mosquito    5.12           3\n",
       "3       20261  221164    7681   mosquito    5.12           3\n",
       "4       20262  221164   10241   mosquito    5.12           3\n",
       "...       ...     ...     ...        ...     ...         ...\n",
       "22757  123367  222470   74241   mosquito   12.80           7\n",
       "22758  123368  222470   76801   mosquito   12.80           7\n",
       "22759  123369  222470   79361   mosquito   12.80           7\n",
       "22760  123370  222470   81921   mosquito   12.80           7\n",
       "22761  123371  222470   84481   mosquito   12.80           7\n",
       "\n",
       "[22762 rows x 6 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c080b982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d19110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb869c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e29ad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # come on feel the noise\n",
    "\n",
    "# # first get the length of all the MUSAN noises https://www.openslr.org/17/ --> data/noise\n",
    "# files_noise = glob('../data/noise/**/*.wav')+glob('../data/audioset/*.wav')\n",
    "# df_noise = pd.DataFrame([{'path': f, 'length': torchaudio.sox_effects.apply_effects_file(f, effects=[[\"rate\", f'{config.rate}']])[0].shape[1]/config.rate} for f in tqdm(files_noise)])\n",
    "# # make the noise in the training data match the format\n",
    "# df_noise_train = df_train[(df_train['sound_type']!='mosquito')][['id','length']]\n",
    "# df_noise_train = df_noise_train[df_noise_train['length']>min_length].sample(frac=0.3)\n",
    "# df_noise_train['path'] = df_noise_train['id'].apply(lambda x: os.path.join(config.data_dir,f'{x}.wav'))\n",
    "# df_noise = pd.concat([df_noise,df_noise_train[['path','length']]],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7fae81c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use audio clips shorter than min length ?\n",
    "USE_SHORT_AUDIO = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d46fadb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the frame offsets for each audio file into dataframes\n",
    "# audio_df_train = get_offsets_df(df_train, short_audio=USE_SHORT_AUDIO)\n",
    "# audio_df_test_A = get_offsets_df(df_test_A, short_audio=False)\n",
    "# audio_df_test_B = get_offsets_df(df_test_B, short_audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b94967e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e102b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function pads a file with 0s to make it a 1.92 sec file\n",
    "def pad_zero(x_temp,rate = config.rate, min_length = config.min_duration ):\n",
    "    #print(\"inside padding zero...\")\n",
    "    left_pad_amt = int((rate*min_length-x_temp.shape[1])//2)\n",
    "    #print(\"left_pad_amt = \" + str(left_pad_amt))\n",
    "    left_pad = torch.zeros(1,left_pad_amt) #+ (0.1**0.5)*torch.randn(1, left_pad_amt)\n",
    "    right_pad_amt = int(rate*min_length-x_temp.shape[1]-left_pad_amt)\n",
    "    right_pad = torch.zeros(1,right_pad_amt)# + (0.1**0.5)*torch.randn(1, right_pad_amt)\n",
    "    f = torch.cat([left_pad,x_temp,right_pad],dim=1)[0]\n",
    "    #print(\"returning a tensor of shape = \" + str(f.shape))\n",
    "    return(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d0066ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MozDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, audio_df, data_dir, min_length, cache=None, transform=None,rate = config.rate):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    \n",
    "    def _get_sample_(self, path, resample=None):\n",
    "        waveform, inp_samp_rate = torchaudio.load(path)\n",
    "        \n",
    "        if inp_samp_rate != config.rate:\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_samp_rate, config.rate, dtype=waveform.dtype)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        if waveform.shape[1] < config.rate*config.min_duration :\n",
    "            print(\"need to pad\")\n",
    "            waveform = pad_zero(waveform)\n",
    "            #waveform = waveform.unsqueeze(dim = 0)\n",
    "        \n",
    "        f = waveform[0]\n",
    "        mu = torch.std_mean(f)[1]\n",
    "        st = torch.std_mean(f)[0]\n",
    "            #return waveform, rate, waveform\n",
    "        f_out = torch.clamp(f, min=mu-st*3, max=mu+st*3).unsqueeze(0)\n",
    "        \n",
    "        return f_out, config.rate\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = idx % len(self.audio_df)\n",
    "        x, _ = self._get_sample_(os.path.join(self.data_dir,f\"{self.audio_df.loc[idx]['id']}.wav\"), resample=config.rate)\n",
    "                                   \n",
    "        offset = self.audio_df.loc[real_idx]['offset']\n",
    "        return x[:,offset:int(offset+config.rate*self.min_length)], self.audio_df.loc[idx]['specie_ind'] \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "85f5c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MozDataset(df_train_offset, config.data_dir, config.min_duration)\n",
    "test_dataset = MozDataset(df_test_offset,config.data_dir, config.min_duration)\n",
    "val_dataset = MozDataset(df_val_offset,config.data_dir, config.min_duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d9df28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a32300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_pytorch.batch_size = 128\n",
    "config_pytorch.epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9bab9576",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = GroupSimilarityDataLoader(train_dataset, batch_size=config_pytorch.batch_size, shuffle= True)\n",
    "val_dataloader = GroupSimilarityDataLoader(val_dataset, batch_size=config_pytorch.batch_size, shuffle= False)\n",
    "test_dataloader = GroupSimilarityDataLoader( test_dataset, batch_size=config_pytorch.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae18dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(TrainableModel):\n",
    "    def __init__(self, lr: float, mining: str):\n",
    "        self._lr = lr\n",
    "        self._mining = mining\n",
    "        super().__init__()\n",
    "\n",
    "    def configure_encoders(self):\n",
    "        pre_trained_encoder = timm.create_model('convnext_small',pretrained=True, in_chans=1,drop_path_rate=0.05, global_pool='max',    drop_rate=0.05)\n",
    "        pre_trained_encoder.head.fc = nn.Identity()\n",
    "        return MozzEncoder(pre_trained_encoder)\n",
    "    \n",
    "    def configure_head(self, input_embedding_size):\n",
    "        return SkipConnectionHead(input_embedding_size, dropout=0.1)\n",
    "    \n",
    "    def configure_loss(self):\n",
    "        return TripletLoss(mining=self._mining, margin=0.5)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), self._lr)\n",
    "        return optimizer\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed35673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3457e9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0000b4af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8575e9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0360588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_metrics(self):\n",
    "    return AttachedMetric(\n",
    "        \"rrp\",\n",
    "        metric=RetrievalRPrecision(),\n",
    "        prog_bar=True,\n",
    "        on_epoch=True,\n",
    "        on_step=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f814caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MozzEncoder(Encoder):\n",
    "    def __init__(self, encoder_model: nn.Module):\n",
    "        super().__init__()\n",
    "        self._encoder = encoder_model\n",
    "        self._embedding_size = 768  # last dimension from the COnvnext model\n",
    "        self.spec_layer = features.STFT(n_fft=config.NFFT, freq_bins=None, hop_length=config.n_hop,\n",
    "                              window='hann', freq_scale='linear', center=True, pad_mode='reflect',\n",
    "                          fmin=400, fmax=2000, sr=config.rate, output_format=\"Magnitude\", trainable=True)\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def trainable(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def embedding_size(self) -> int:\n",
    "        return self._embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c99e06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, image):\n",
    "    spec = self.spec_layer(image)\n",
    "    embeddings = self._encoder.forward(spec)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6acc97d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(self, output_path = config.model_dir):\n",
    "    #os.makedirs(output_path, exist_ok=True)\n",
    "    torch.save(self._encoder, os.path.join(output_path, \"encoder.pth\"))\n",
    "\n",
    "@classmethod\n",
    "def load(cls, input_path = config.model_dir):\n",
    "    encoder_model = torch.load(os.path.join(input_path, \"encoder.pth\"))\n",
    "    return MozzEncoder(encoder_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5b17ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_dataloader,  val_dataloader ,epochs = config_pytorch.epochs, save_dir = config.model_dir,mining = \"all\"):\n",
    "    \n",
    "    #model = Model(lr=lr,mining=mining, )\n",
    "    #train_dataloader, val_dataloader = get_dataloaders( batch_size=batch_size, input_size=input_size, shuffle=shuffle )\n",
    "    early_stopping = EarlyStopping(monitor=\"validation_loss\",patience=50,  )\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        gpus=1 if torch.cuda.is_available() else 0,\n",
    "        max_epochs=epochs,\n",
    "        callbacks=[early_stopping, ModelSummary(max_depth=3)],\n",
    "        enable_checkpointing=False,\n",
    "        log_every_n_steps=1,\n",
    "    )\n",
    "\n",
    "    Quaterion.fit(\n",
    "        trainable_model=model,\n",
    "        trainer=trainer,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "    )\n",
    "\n",
    "    model.save_servable(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c3140204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling rate = 8000. Please make sure the sampling rate is correct in order toget a valid freq range\n",
      "STFT kernels created, time used = 0.0780 seconds\n"
     ]
    }
   ],
   "source": [
    "model = Model(lr= config_pytorch.lr,mining=\"all\", )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "046e6282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/base.py:22: LightningDeprecationWarning: pytorch_lightning.callbacks.base.Callback has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.callbacks.callback.Callback class instead.\n",
      "  rank_zero_deprecation(\n",
      "Missing logger folder: /dli/task/Capstone-HumBug/notebooks/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                      | Type               | Params\n",
      "-----------------------------------------------------------------\n",
      "0 | _model                    | SimilarityModel    | 54.2 M\n",
      "1 | _model.default            | MozzEncoder        | 53.7 M\n",
      "2 | _model.default._encoder   | ConvNeXt           | 49.5 M\n",
      "3 | _model.default.spec_layer | STFT               | 4.2 M \n",
      "4 | _model.head               | SkipConnectionHead | 591 K \n",
      "5 | _model.head.dropout       | Dropout            | 0     \n",
      "6 | _model.head.fc            | Linear             | 590 K \n",
      "7 | _model.head.skip_dropout  | Identity           | 0     \n",
      "8 | _loss                     | TripletLoss        | 0     \n",
      "-----------------------------------------------------------------\n",
      "591 K     Trainable params\n",
      "53.7 M    Non-trainable params\n",
      "54.2 M    Total params\n",
      "216.966   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01719069480895996,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d82add77804a9d8aa4341ebe9d15dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:225: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-0bbf0dbc8c0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-2e82bddf9550>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, val_dataloader, epochs, save_dir, mining)\u001b[0m\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     Quaterion.fit(\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mtrainable_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/quaterion/main.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(cls, trainable_model, trainer, train_dataloader, val_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 )\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             trainer.fit(\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \"\"\"\n\u001b[1;32m    695\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m         self._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    697\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m         \u001b[0;31m# TODO(awaelchli): Unify both exceptions below, where `KeyboardError` doesn't re-raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_provided\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         )\n\u001b[0;32m--> 735\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__}: trainer tearing down\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0misolate_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1274\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \u001b[0;31m# enable train mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1341\u001b[0m             \u001b[0;31m# run eval step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m                 \u001b[0mval_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_sanity_check_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_dataloaders\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dataloader_idx\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mdl_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_max_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m# store batch level output per dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoaderIterDataFetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mbatch_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/fetching.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetching_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/fetching.py\u001b[0m in \u001b[0;36mfetching_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;31m# this will run only when no pre-fetching was done.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0;31m# consume the batch we just fetched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/fetching.py\u001b[0m in \u001b[0;36m_fetch_next_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mstart_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_fetch_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetched\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch_batches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/quaterion/dataset/train_collator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSimilarityPairSample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSimilarityGroupSample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     ):\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_collate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mencoder_collate_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/quaterion/dataset/similarity_data_loader.py\u001b[0m in \u001b[0;36mpre_collate_fn\u001b[0;34m(cls, batch)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0msample_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0msample_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         features, feature_ids = cls.flatten_objects(\n\u001b[1;32m    123\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msimilarity_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/quaterion/dataset/similarity_data_loader.py\u001b[0m in \u001b[0;36mcollate_labels\u001b[0;34m(cls, batch)\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m'groups'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \"\"\"\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"groups\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrecord\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/quaterion/dataset/similarity_data_loader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m'groups'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \"\"\"\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"groups\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrecord\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "train(model, train_dataloader,  val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d747a7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685614fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e97614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de9ee5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acc30e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization():\n",
    "    \"\"\"This class is for normalizing the spectrograms batch by batch. The normalization used is min-max, two modes 'framewise' and 'imagewise' can be selected. In this paper, we found that 'imagewise' normalization works better than 'framewise'\"\"\"\n",
    "    def __init__(self, mode='framewise'):\n",
    "        if mode == 'framewise':\n",
    "            def normalize(x):\n",
    "                size = x.shape\n",
    "                x_max = x.max(1, keepdim=True)[0] # Finding max values for each frame\n",
    "                x_min = x.min(1, keepdim=True)[0]  \n",
    "                output = (x-x_min)/(x_max-x_min) # If there is a column with all zero, nan will occur\n",
    "                output[torch.isnan(output)]=0 # Making nan to 0\n",
    "                return output\n",
    "        elif mode == 'imagewise':\n",
    "            def normalize(x):\n",
    "                size = x.shape\n",
    "                x_max = x.reshape(size[0], size[1]*size[2]).max(1, keepdim=True)[0]\n",
    "                x_min = x.reshape(size[0], size[1]*size[2]).min(1, keepdim=True)[0]\n",
    "                x_max = x_max.unsqueeze(1) # Make it broadcastable\n",
    "                x_min = x_min.unsqueeze(1) # Make it broadcastable \n",
    "                return (x-x_min)/(x_max-x_min)\n",
    "        else:\n",
    "            print(f'please choose the correct mode')\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.normalize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4436d61c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa07dfb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61675d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08271f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f14c343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1070bf09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3b7bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e48b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcen(x, eps=1e-6, s=0.025, alpha=0.98, delta=2, r=0.5, training=False):\n",
    "    frames = x.split(1, -2)\n",
    "    m_frames = []\n",
    "    last_state = None\n",
    "    for frame in frames:\n",
    "        if last_state is None:\n",
    "            last_state = s * frame\n",
    "            m_frames.append(last_state)\n",
    "            continue\n",
    "        if training:\n",
    "            m_frame = ((1 - s) * last_state).add_(s * frame)\n",
    "        else:\n",
    "            m_frame = (1 - s) * last_state + s * frame\n",
    "        last_state = m_frame\n",
    "        m_frames.append(m_frame)\n",
    "    M = torch.cat(m_frames, 1)\n",
    "    if training:\n",
    "        pcen_ = (x / (M + eps).pow(alpha) + delta).pow(r) - delta ** r\n",
    "    else:\n",
    "        pcen_ = x.div_(M.add_(eps).pow_(alpha)).add_(delta).pow_(r).sub_(delta ** r)\n",
    "    return pcen_\n",
    "\n",
    "\n",
    "class PCENTransform(nn.Module):\n",
    "\n",
    "    def __init__(self, eps=1e-6, s=0.025, alpha=0.98, delta=2, r=0.5, trainable=True):\n",
    "        super().__init__()\n",
    "        if trainable:\n",
    "            self.log_s = nn.Parameter(torch.log(torch.Tensor([s])))\n",
    "            self.log_alpha = nn.Parameter(torch.log(torch.Tensor([alpha])))\n",
    "            self.log_delta = nn.Parameter(torch.log(torch.Tensor([delta])))\n",
    "            self.log_r = nn.Parameter(torch.log(torch.Tensor([r])))\n",
    "        else:\n",
    "            self.s = s\n",
    "            self.alpha = alpha\n",
    "            self.delta = delta\n",
    "            self.r = r\n",
    "        self.eps = eps\n",
    "        self.trainable = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x.permute((0,2,1)).squeeze(dim=1)\n",
    "        if self.trainable:\n",
    "            x = pcen(x, self.eps, torch.exp(self.log_s), torch.exp(self.log_alpha), torch.exp(self.log_delta), torch.exp(self.log_r), self.training and self.trainable)\n",
    "        else:\n",
    "            x = pcen(x, self.eps, self.s, self.alpha, self.delta, self.r, self.training and self.trainable)\n",
    "#         x = x.unsqueeze(dim=1).permute((0,1,3,2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a1dd4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass the pretrained model and make it a binary classification\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, model_name, image_size):\n",
    "        super().__init__()\n",
    "        # num_classes=0 removes the pretrained head\n",
    "        self.backbone = timm.create_model(model_name,\n",
    "                        pretrained=True, num_classes=2, in_chans=1, \n",
    "                        drop_path_rate=0.05, global_pool='max',\n",
    "                        drop_rate=0.05)\n",
    "        #####  This section is model specific\n",
    "        #### It freezes some fo the layers by name\n",
    "        #### you'll have to inspect the model to see the names\n",
    "        for name, param in self.backbone.named_parameters():\n",
    "            if param.requires_grad and 'head' not in name \\\n",
    "                and not name.startswith('norm') \\\n",
    "                and 'stages.3' not in name \\\n",
    "                and 'blocks.26' not in name and 'blocks.26' not in name \\\n",
    "                and 'blocks.24' not in name and 'blocks.25' not in name \\\n",
    "                and 'blocks.22' not in name and 'blocks.23' not in name \\\n",
    "                and 'blocks.20' not in name and 'blocks.21' not in name \\\n",
    "                and 'blocks.22' not in name and 'blocks.23' not in name \\\n",
    "                and 'blocks.19' not in name and 'blocks.18' not in name \\\n",
    "                and 'blocks.17' not in name:\n",
    "                param.requires_grad = False\n",
    "        #### end layer freezing\n",
    "        self.spec_layer = features.STFT(n_fft=config.NFFT, freq_bins=None, hop_length=config.n_hop,\n",
    "                              window='hann', freq_scale='linear', center=True, pad_mode='reflect',\n",
    "                          fmin=400, fmax=2000, sr=config.rate, output_format=\"Magnitude\", trainable=True)\n",
    "        self.out = nn.Linear(self.backbone.num_features, 1)\n",
    "        self.sizer = VT.Resize((image_size,image_size))\n",
    "        self.timeMasking = T.TimeMasking(time_mask_param=int(config.win_size*0.4), iid_masks=True)\n",
    "        self.freqMasking = T.FrequencyMasking(freq_mask_param=int((config.NFFT//4)*0.15), iid_masks=True)\n",
    "        self.norm_layer = Normalization(mode='framewise')\n",
    "        self.pcen_layer = PCENTransform(eps=1e-6, s=0.025, alpha=0.6, delta=0.1, r=0.2, trainable=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # first compute spectrogram\n",
    "        spec = self.spec_layer(x)  # (B, F, T)\n",
    "        # normalize\n",
    "#         spec = spec.transpose(1,2) # (B, T, F)\n",
    "        spec = self.pcen_layer(spec)\n",
    "        spec = self.norm_layer(spec)\n",
    "        \n",
    "#         if self.training:\n",
    "#             spec = self.timeMasking(spec)\n",
    "#             spec = self.freqMasking(spec)\n",
    "\n",
    "        # then size for CNN model\n",
    "        # and create a channel\n",
    "        spec = self.sizer(spec)\n",
    "        x = spec.unsqueeze(1)\n",
    "        # then repeat channels\n",
    "        x = self.backbone(x)\n",
    "        \n",
    "        pred = self.out(x)\n",
    "        \n",
    "        output = {\"prediction\": pred,\n",
    "                  \"spectrogram\": spec}\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb3b9b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling rate = 8000. Please make sure the sampling rate is correct in order toget a valid freq range\n",
      "STFT kernels created, time used = 0.0772 seconds\n"
     ]
    }
   ],
   "source": [
    "model = Model('convnext_small',224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d332d03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcac04a767cd4524b54a788742f4fe55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1842 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (128x2 and 768x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_483/2187717205.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_483/3543513541.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, val_loader, model, n_channels)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;31m# outputs = model(inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_483/488045245.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         output = {\"prediction\": pred,\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (128x2 and 768x1)"
     ]
    }
   ],
   "source": [
    "model, lr_log = train_model(train_loader, val_loader, model = model, n_channels = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5e5a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate schedule\n",
    "plt.plot([i for i in range(len(lr_log))], lr_log);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac76d575",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_A = MozTestDataset(audio_df_test_A, config.data_dir, min_length, audio_cache)\n",
    "\n",
    "test_loader_A = torch.utils.data.DataLoader(\n",
    "        dataset_test_A, batch_size=config_pytorch.batch_size,\n",
    "        num_workers=num_workers, pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "\n",
    "dataset_test_B = MozTestDataset(audio_df_test_B, config.data_dir, min_length, audio_cache)\n",
    "\n",
    "test_loader_B = torch.utils.data.DataLoader(\n",
    "        dataset_test_B, batch_size=config_pytorch.batch_size,\n",
    "        num_workers=num_workers, pin_memory=pin_memory,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db94e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path and model name are used to save the PDF output\n",
    "# but loading the model isn't needed unless the notebook\n",
    "# has been restarted.  \n",
    "path = '../outputs/models/pytorch/'\n",
    "# model_name = 'model_e0_2022_03_19_17_06_24.pth'\n",
    "model_name = 'model_e0_2022_03_19_17_15_45.pth'\n",
    "\n",
    "model = load_model(path + model_name, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c17d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "## feat_type is a unique name for the combination of paramters and pre-processing\n",
    "feat_type = 'CDL_convnext_lrg_stfs_512_more_noise_rand_small_snd_framewise'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e885cd3f",
   "metadata": {},
   "source": [
    "### Test B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef29a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_preds_all, y_test_B = evaluate_model(model, test_loader_B, 1, n_channels=1)  # Predict directly over feature windows (1.92 s)\n",
    "\n",
    "PE, MI, log_prob = get_results(y_preds_all, y_test_B, filename = feat_type + '_' + model_name +'_Test_B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673a982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_preds_all, y_test_B = evaluate_model(model, test_loader_B, 1, n_channels=1)  # Predict directly over feature windows (1.92 s)\n",
    "\n",
    "PE, MI, log_prob = get_results(y_preds_all, y_test_B, filename = feat_type + '_' + model_name +'_Test_B')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387accd6",
   "metadata": {},
   "source": [
    "### Test A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b02a88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_all, y_test_A = evaluate_model(model, test_loader_A, 1, n_channels=3)  # Predict directly over feature windows (1.92 s)\n",
    "PE, MI, log_prob = get_results(y_preds_all, y_test_A, filename = feat_type + '_' + model_name +'_Test_A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0132e757",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3d5077",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for i, val in enumerate(y_test_B.squeeze()):\n",
    "    if val != np.argmax(y_preds_all[0],-1)[i]:\n",
    "        errors.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2035a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(errors), len(y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7704f268",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_df_test_B.iloc[1934]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c11b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_df_test_B[audio_df_test_B['sound_type']=='mosquito'].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10c679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader_B.dataset[1656][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0608efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87785bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f094a335",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram(model(test_loader_B.dataset[1910][0].to('cuda:0'))['spectrogram'][0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75d0b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0b2ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.dataset[2001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bc0905",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_df_train[(audio_df_train['sound_type']=='mosquito') & (audio_df_train['length']>4)].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a9926",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(_get_sample('../data/audio/220371.wav', resample=8000)[0].numpy()[0], rate=8000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1752f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(_get_noisy_sample('../data/audio/220371.wav', noise_df=df_noise).numpy()[0], rate=8000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e1fc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_get_noisy_sample('../data/audio/220371.wav', noise_df=df_noise).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8023ad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "_get_sample('../data/audio/199900.wav', resample=8000)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c9018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't run this stuff.  Saved for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99fd58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram(spec.cpu().detach().numpy()[0],title='512')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bbcbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sound, _ = _get_sample('../data/audio/199900.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac10f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(sound.numpy()[0], rate=8000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56703f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6883d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397acf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = spec_layer(sound)  # (B, F, T)\n",
    "spec = pcen_layer(spec)\n",
    "spec = norm_layer(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a3d048",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcen_layer = PCENTransform(eps=1e-6, s=0.025, alpha=0.6, delta=0.1, r=0.2, trainable=True)\n",
    "norm_layer = Normalization(mode='framewise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98eff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_sample(path, resample=None):\n",
    "    effects = [\n",
    "        [\"remix\", \"1\"]\n",
    "    ]\n",
    "    if resample:\n",
    "        effects.extend([\n",
    "          #[\"lowpass\", f\"{resample // 2}\"],\n",
    "#             [\"bandpass\", f\"600\",\"1000\"],\n",
    "          [\"rate\", f'{resample}'],\n",
    "          ['gain', '-n'],\n",
    "         [\"highpass\", \"600\"],\n",
    "        ])\n",
    "    #returns waveform [1,XXXXX], sample_rate\n",
    "    \n",
    "    return torchaudio.sox_effects.apply_effects_file(path, effects=effects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30550539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_noisy_sample(path, noise_df):\n",
    "    x, _ = _get_sample(path, resample=8000)\n",
    "    sample_length = x.shape[1]/8000\n",
    "    if len(noise_df[noise_df['length'] >= sample_length]) > 0:\n",
    "        noise_path = noise_df[noise_df['length'] >= sample_length].sample(1).iloc[0]['path']\n",
    "        noise, _ = _get_sample(noise_path, resample=8000)\n",
    "        \n",
    "        noise = noise[:, :x.shape[1]]\n",
    "\n",
    "        mos_power = x.norm(p=2)\n",
    "        noise_power = noise.norm(p=2)\n",
    "\n",
    "        snr = math.exp(1 / 10)\n",
    "        scale = snr * noise_power / mos_power\n",
    "        x = (scale * x + noise) / 2\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ff2b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = _get_sample('../data/audio/220371.wav', resample=8000)\n",
    "sample_length = x.shape[1]/8000\n",
    "noise_path = df_noise[df_noise['length'] >= sample_length].sample(1).iloc[0]['path']\n",
    "noise, _ = _get_sample(noise_path, resample=8000)\n",
    "\n",
    "noise = noise[:, :x.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2b5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mos_power = x.norm(p=2)\n",
    "noise_power = noise.norm(p=2)\n",
    "mos_power, noise_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69366a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "snr = math.exp(3 / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfe8eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = snr * noise_power / mos_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff10d311",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df30407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_n = (scale * x / 10 + noise) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e1dc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66ca3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(x_n, rate=8000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b546bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(_get_sample(noise_path, resample=8000)[0].numpy()[0], rate=8000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151dbe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(_get_noisy_sample('../data/audio/220371.wav', noise_df=df_noise).numpy()[0], rate=8000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece6d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.set_title(title or 'Spectrogram (db)')\n",
    "    axs.set_ylabel(ylabel)\n",
    "    axs.set_xlabel('frame')\n",
    "#     im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n",
    "    im = axs.imshow(spec, origin='lower', aspect=aspect)\n",
    "    if xmax:\n",
    "        axs.set_xlim((0, xmax))\n",
    "    fig.colorbar(im, ax=axs)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ad96f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram(F.normalize(spec_layer(x.to('cuda:0')),dim=1).cpu().detach().numpy()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df66d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram(spec_layer(x.to('cuda:0')).cpu().detach().numpy()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b67a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram(tm(s[aug > 1]).cpu().detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb409d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run this cell to list pretrained models\n",
    "avail_pretrained_models = timm.list_models(pretrained=True)\n",
    "avail_pretrained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e659ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## put a model name in here to downlad a pretrained model and convert it\n",
    "## to binary classification\n",
    "pt_model = Model('convnext_small',224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at layer names to figure out what to freeze\n",
    "# You'll have to go edit the Model class and reload it based \n",
    "# on the model names you're freezing here\n",
    "[name for name, _ in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17e1e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4407e575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
