{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49ba249",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style type='text/css'>\n",
    ".CodeMirror{\n",
    "font-size: 14px;\n",
    "</style>\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f32c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had to find the right version of pytorch with the widget here https://pytorch.org/\n",
    "# I *think* this will work with AWS\n",
    "#!pip3 install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ecc7403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other dependencies\n",
    "#!pip install timm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8debf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nnAudio\n",
    "#!pip install git+https://github.com/KinWaiCheuk/nnAudio.git#subdirectory=Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ef0a0d",
   "metadata": {},
   "source": [
    "### 1 Import the kitchen sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "756a258c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
     ]
    }
   ],
   "source": [
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1ec813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug main imports\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../lib'))\n",
    "import config\n",
    "from evaluate import get_results\n",
    "import numpy as np\n",
    "\n",
    "# Troubleshooting and visualisation\n",
    "import IPython.display as ipd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1893a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug lib imports\n",
    "from sklearn.metrics import accuracy_score\n",
    "from PyTorch import config_pytorch\n",
    "from datetime import datetime\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4635d19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional pytorch tools\n",
    "import random\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchvision.transforms as VT\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "import timm\n",
    "import timm.optim\n",
    "from timm.loss import BinaryCrossEntropy\n",
    "from timm.utils import NativeScaler\n",
    "from timm.models import model_parameters\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33f713d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nnAudio\n",
    "from nnAudio import features\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14f1ea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarity learning\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from typing import Callable\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "from quaterion.dataset import (GroupSimilarityDataLoader, SimilarityGroupSample)\n",
    "import torchvision\n",
    "from quaterion_models.encoders import Encoder\n",
    "from quaterion_models.heads import EncoderHead, SkipConnectionHead\n",
    "from torch import nn\n",
    "from typing import Dict, Union, Optional, List\n",
    "\n",
    "from quaterion import TrainableModel\n",
    "from quaterion.eval.attached_metric import AttachedMetric\n",
    "from quaterion.eval.group import RetrievalRPrecision\n",
    "from quaterion.loss import SimilarityLoss, TripletLoss\n",
    "from quaterion.train.cache import CacheConfig, CacheType\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "from quaterion_models.encoders import Encoder\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelSummary\n",
    "\n",
    "from quaterion import Quaterion\n",
    "#from .data import get_dataloaders\n",
    "#from .models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc61a01",
   "metadata": {},
   "source": [
    "### Run all these function definition cells\n",
    "These have been extracted from the lib folder and are here to make them more easily editable.  Most of the action happens in *get_feat_torch*, which does feature extraction and *train_model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "170f2147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filepath, model):\n",
    "    # Instantiate model to inspect\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "    print(f'Training on {device}')\n",
    "        \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "    model = model.to(device)\n",
    "    # Load trained parameters from checkpoint (may need to download from S3 first)\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        map_location=lambda storage, loc: storage.cuda()\n",
    "    else:\n",
    "        map_location='cpu'\n",
    "        \n",
    "    checkpoint = model.load_state_dict(torch.load(filepath))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "084c0334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion, class_threshold=0.5, device=None):\n",
    "    with torch.no_grad():\n",
    "        if device is None:\n",
    "            torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        sigmoid = nn.Sigmoid()\n",
    "        test_loss = 0.0\n",
    "        model.eval()\n",
    "        \n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        counter = 1\n",
    "        for x, y, idx in tqdm(test_loader, desc='validation', leave=True):\n",
    "            \n",
    "            x, y = x.to(device), y.unsqueeze(1).float().to(device)\n",
    "            \n",
    "            y_pred = model(x)['prediction']\n",
    "                        \n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            all_y.append(y.cpu().detach())\n",
    "            all_y_pred.append(y_pred.cpu().detach())\n",
    "            \n",
    "            del x\n",
    "            del y\n",
    "            del y_pred\n",
    "            \n",
    "            counter +=1\n",
    "\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        \n",
    "        test_loss = test_loss/len(test_loader)\n",
    "        test_acc = accuracy_score(all_y.numpy(), (sigmoid(all_y_pred).numpy() > class_threshold).astype(float))\n",
    "    \n",
    "    \n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51ca7136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_siamese(model, test_loader, criterion, class_threshold=0.5, device=None):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        mean_val_y_pred = []\n",
    "        all_y_pred = []\n",
    "        val_running_loss = 0.0\n",
    "        for img1, img2, y in val_loader:\n",
    "            x1 = img1.to(device)\n",
    "            x2 = img2.to(device)\n",
    "            y = y.to(device)\n",
    "            model = model.to(device)\n",
    "            y_pred = model(x1,x2)\n",
    "            loss = criterion(y_pred, y)\n",
    "            val_running_loss += loss.item()\n",
    "            #test_loss += loss.item()\n",
    "            #all_y.append(y.cpu().detach())\n",
    "            all_y_pred.append(y_pred.cpu().detach())\n",
    "            mean_val_y_pred.append(torch.mean(y_pred.cpu().detach()))\n",
    "            \n",
    "            del x1, \n",
    "            del x2\n",
    "            del y\n",
    "            del y_pred\n",
    "            \n",
    "        #all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        avg_val_loss = val_running_loss / len(val_loader)\n",
    "        #test_loss = test_loss/len(test_loader)\n",
    "        #test_acc = accuracy_score(all_y.numpy(), (sigmoid(all_y_pred).numpy() > class_threshold).astype(float))\n",
    "    \n",
    "    \n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e008b704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_siamese(model, train_loader, val_loader,test_loader , classes):\n",
    "    loss_scaler = NativeScaler()\n",
    "    global_step = 0\n",
    "    torch.manual_seed(0)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Training on {device}')    \n",
    "    model = model.to(device)\n",
    "    criterion = BinaryCrossEntropy(smoothing=0.1)\n",
    "    optimiser = timm.optim.RAdam(model.parameters(), lr=config_pytorch.lr/10)\n",
    "    num_epochs = config_pytorch.epochs\n",
    "    all_train_loss = []\n",
    "    all_val_acc = []\n",
    "    best_val_loss = np.inf\n",
    "    best_train_loss = np.inf\n",
    "    best_epoch = -1\n",
    "    checkpoint_name = None\n",
    "    overrun_counter = 0\n",
    "    lr_log = []\n",
    "        \n",
    "    for e in range(num_epochs):\n",
    "        train_running_loss = 0.0 \n",
    "        model.train()\n",
    "        all_y_pred = []\n",
    "        y_pred_mean = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for ind , (img1, img2, y) in enumerate(train_loader):\n",
    "            # Forward\n",
    "            x1 = img1.to(device)\n",
    "            x2 = img2.to(device)\n",
    "            y = y.to(device)\n",
    "            global_step += 1\n",
    "            optimiser.zero_grad()\n",
    "            # AMP\n",
    "            if ind % 300 == 0 :\n",
    "                elapsed_time = time.time()\n",
    "                time_since_epoch = (elapsed_time - start_time)/60\n",
    "                print(\"epoch = \"+ str(e) + \"processed batch \" + str(ind) + \" of \" + str(len(train_loader)))\n",
    "                print(\"duration = \" + str(time_since_epoch))\n",
    "                \n",
    "            \n",
    "            with autocast():\n",
    "                y_pred = model(x1,x2)\n",
    "                #print(\"y_pred = \" + str(y_pred))\n",
    "                loss = criterion(y_pred, y)\n",
    "                   \n",
    "            loss_scaler(loss, optimiser, parameters=model_parameters(model))\n",
    "                       \n",
    "            train_running_loss += loss.item()\n",
    "            avg_train_loss = train_running_loss / len(train_loader)\n",
    "            #y_pred_bat_mean = (torch.mean(y_pred.cpu().detach()))\n",
    "            lr_log.append(optimiser.param_groups[0]['lr'])\n",
    "            del x1\n",
    "            del x2\n",
    "            del y\n",
    "        \n",
    "#         optimiser.sync_lookahead()\n",
    "        all_train_loss.append(train_running_loss/len(train_loader))\n",
    "        val_loss = test_model_siamese(model, val_loader, criterion, 0.5, device=device)\n",
    "        \n",
    "        #check if the current val_loss is less than the best -val loss\n",
    "        if val_loss < best_val_loss:\n",
    "            print(\"updating the best val loss..\")\n",
    "            best_val_loss = val_loss\n",
    "            print(\"saving the model ...\")\n",
    "            checkpoint_name = f'model_e{e}_{datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")}.pth'\n",
    "            torch.save(model.state_dict(), os.path.join(config.model_dir, 'pytorch', checkpoint_name))\n",
    "            print('Saving model to:', os.path.join(config.model_dir, 'pytorch', checkpoint_name)) \n",
    "            overrun_counter = -1\n",
    "            #print(\"also get the test score over here\")\n",
    "            eval_siamese(model , test_loader, classes)\n",
    "        else:\n",
    "            overrun_counter += 1\n",
    "            \n",
    "        if overrun_counter > config_pytorch.max_overrun:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    return model, lr_log\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91f83fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_siamese(model, test_loader,classes):\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    ind_other = classes.index('others')\n",
    "    print(\"length of test loader = \" + str(len(test_loader)))\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        y_hat_list = []\n",
    "        y_test = []\n",
    "        y_hat_dict = {}\n",
    "        for ind,(mainImg, imgSets, label) in enumerate(test_loader):\n",
    "            #print(\"loader ind = \" +str(ind))\n",
    "            mainImg = mainImg.to(device)\n",
    "            predVal = 0\n",
    "            pred = -1\n",
    "            y_hat = label.cpu().detach().numpy()\n",
    "            y_test.append(y_hat.item())\n",
    "            # determine which category an image belongs to\n",
    "            output_list = []\n",
    "            for i , testImg in enumerate(imgSets):\n",
    "                #print(\"i = \" +str(i))\n",
    "                testImg = testImg.to(device)\n",
    "                output = model(mainImg, testImg)\n",
    "                output_cpu = output.cpu().detach()\n",
    "                #print(\"output = \\n\" + str(output_cpu) )\n",
    "                output_list.append(output_cpu)\n",
    "                del output\n",
    "            #print(\"output_list = \" +str(output_list))\n",
    "            #print(\"max value in the list = \" +str(max(output_list)))\n",
    "            y_hat_ind = output_list.index(max(output_list))\n",
    "            y_hat_list.append(y_hat_ind)\n",
    "            y_hat_dict.update(ind = str(y_hat_ind) )\n",
    "            \n",
    "        #print(\"prediction list = \" + str(y_hat_list))\n",
    "        #print(\"Label  list = \" + str(y_test))\n",
    "        \n",
    "        print(\"Now printing classification report...\")\n",
    "        from sklearn.metrics import classification_report\n",
    "        #print(\"len of y_hat = \" + str(len(y_hat_list)))\n",
    "        #print(\"len of y_test = \" + str(len(y_test)))\n",
    "        \n",
    "        print(classification_report(y_test, y_hat_list, target_names= classes))\n",
    "        \n",
    "     \n",
    "            \n",
    "#                 if output > predVal:\n",
    "#                     pred = i\n",
    "#                     predVal = output\n",
    "#             label = label.to(device)\n",
    "#             if pred == label:\n",
    "#                 correct += 1\n",
    "#             count += 1\n",
    "#             if count % 20 == 0:\n",
    "#                 print(\"Current Count is: {}\".format(count))\n",
    "#                 print('Accuracy on n way: {}'.format(correct/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f22d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offsets_df(df, short_audio=False):\n",
    "    audio_offsets = []\n",
    "    min_length = config.win_size*config.NFFT/(((1/config.n_hop)*config.NFFT)*config.rate)\n",
    "    step_frac = config.step_size/config.win_size\n",
    "    for _,row in df.iterrows():\n",
    "        if row['length'] > min_length:\n",
    "            step_size = step_frac*min_length\n",
    "            audio_offsets.append({'id':row['id'], 'offset':0,'sound_type': row['sound_type'], 'length': row['length'],'specie_ind': row['specie_ind']})\n",
    "            for i in range(1, int((row['length']-min_length)//step_size)):\n",
    "                audio_offsets.append({'id': row['id'], 'offset':int(min_length+(i*step_size)*config.rate),'sound_type':row['sound_type'], 'length': row['length'],'specie_ind': row['specie_ind']})\n",
    "        elif short_audio:\n",
    "            audio_offsets.append({'id':row['id'], 'offset':0,'sound_type': row['sound_type'], 'length': row['length'],'specie_ind': row['specie_ind']})\n",
    "    return pd.DataFrame(audio_offsets)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd7040ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_df(df_offset, indices):\n",
    "    list_df_ind = []\n",
    "    #print(\"len of indices = \" + str(len(indices)))\n",
    "    for ind in indices :\n",
    "        df_name = \"df_\"+ str(ind)\n",
    "        df_name = df_offset[df_offset['specie_ind'] == ind]\n",
    "        list_df_ind.append(df_name)\n",
    "    df_offset_trimmed = pd.concat(list_df_ind)\n",
    "    return(df_offset_trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de8d8e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.92"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the min length based on config params\n",
    "min_length = (config.win_size * config.n_hop) / config.rate\n",
    "min_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd96e0",
   "metadata": {},
   "source": [
    "### 3 The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3d6276",
   "metadata": {},
   "source": [
    "### Read CSV and get train/test groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30cbe9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['an arabiensis','culex pipiens complex', 'ae aegypti','an funestus ss','an squamosus',\n",
    "               'an coustani','ma uniformis','ma africanus' , 'others']\n",
    "classes_no_other = ['an arabiensis','culex pipiens complex', 'ae aegypti','an funestus ss','an squamosus',\n",
    "               'an coustani','ma uniformis','ma africanus' ]\n",
    "other_ind = classes.index('others')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01bc909b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>record_datetime</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>species</th>\n",
       "      <th>gender</th>\n",
       "      <th>fed</th>\n",
       "      <th>plurality</th>\n",
       "      <th>age</th>\n",
       "      <th>method</th>\n",
       "      <th>mic_type</th>\n",
       "      <th>device_type</th>\n",
       "      <th>country</th>\n",
       "      <th>district</th>\n",
       "      <th>province</th>\n",
       "      <th>place</th>\n",
       "      <th>location_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>0.463456</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>08-09-16 08:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>0.170249</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>08-09-16 08:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>0.104041</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>08-09-16 08:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69</td>\n",
       "      <td>0.274290</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>08-09-16 08:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56</td>\n",
       "      <td>0.420894</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>08-09-16 08:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Plural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>3562</td>\n",
       "      <td>6.083093</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>01-07-18 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>3556</td>\n",
       "      <td>6.719908</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>01-07-18 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an maculatus</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9009</th>\n",
       "      <td>3553</td>\n",
       "      <td>6.128580</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>01-07-18 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an maculatus</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9011</th>\n",
       "      <td>3561</td>\n",
       "      <td>11.614280</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>01-07-18 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9012</th>\n",
       "      <td>3552</td>\n",
       "      <td>2.920249</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>01-07-18 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6008 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     length                             name  sample_rate  \\\n",
       "1       53   0.463456  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "2       57   0.170249  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "3       61   0.104041  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "4       69   0.274290  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "5       56   0.420894  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "...    ...        ...                              ...          ...   \n",
       "8999  3562   6.083093                    #988-1001.wav        44100   \n",
       "9000  3556   6.719908                    #988-1001.wav        44100   \n",
       "9009  3553   6.128580                    #988-1001.wav        44100   \n",
       "9011  3561  11.614280                    #988-1001.wav        44100   \n",
       "9012  3552   2.920249                    #988-1001.wav        44100   \n",
       "\n",
       "     record_datetime sound_type       species  gender  fed plurality  age  \\\n",
       "1     08-09-16 08:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "2     08-09-16 08:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "3     08-09-16 08:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "4     08-09-16 08:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "5     08-09-16 08:00   mosquito    ae aegypti     NaN  NaN    Plural  NaN   \n",
       "...              ...        ...           ...     ...  ...       ...  ...   \n",
       "8999  01-07-18 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "9000  01-07-18 12:00   mosquito  an maculatus  Female    t    Single  NaN   \n",
       "9009  01-07-18 12:00   mosquito  an maculatus  Female    t    Single  NaN   \n",
       "9011  01-07-18 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "9012  01-07-18 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "\n",
       "     method mic_type    device_type   country          district  \\\n",
       "1       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "2       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "3       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "4       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "5       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "...     ...      ...            ...       ...               ...   \n",
       "8999    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9000    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9009    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9011    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9012    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "\n",
       "                   province                            place location_type  \n",
       "1                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "2                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "3                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "4                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "5                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "...                     ...                              ...           ...  \n",
       "8999  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9000  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9009  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9011  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9012  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "\n",
       "[6008 rows x 19 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(config.data_df)\n",
    "#df = df.loc[df['Grade'].notnull()]\n",
    "df = df.loc[df['species'].notnull()]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59b706dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a colum for specie encoding\n",
    "df['specie_ind'] = \"NULL_VAL\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b798805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specie = an arabiensisand its index = 0\n",
      "specie = culex pipiens complexand its index = 1\n",
      "specie = ae aegyptiand its index = 2\n",
      "specie = an funestus ssand its index = 3\n",
      "specie = an squamosusand its index = 4\n",
      "specie = an coustaniand its index = 5\n",
      "specie = ma uniformisand its index = 6\n",
      "specie = ma africanusand its index = 7\n"
     ]
    }
   ],
   "source": [
    "# Adding a new column to encode specie_index in the same order as the list \"classes\"\n",
    "ind = 0\n",
    "for specie in classes_no_other:\n",
    "    print(\"specie = \" + str(specie) + \"and its index = \" + str(ind) )\n",
    "    row_indexes=df[df['species']==specie].index \n",
    "    df.loc[row_indexes,'specie_ind']= ind\n",
    "    ind+=1\n",
    "\n",
    "    \n",
    "other_df_ind = df[df['specie_ind'] == \"NULL_VAL\"].index\n",
    "df.loc[other_df_ind,'specie_ind']= other_ind                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "801e6a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SHORT_AUDIO = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39f79e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offset = get_offsets_df(df, short_audio=USE_SHORT_AUDIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "193f7a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train_offset_temp,df_test_offset  = train_test_split(df_offset, test_size=0.2)\n",
    "df_train_offset,df_val_offset  = train_test_split(df_train_offset_temp, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d271cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n",
      "7601\n",
      "i = 1\n",
      "3547\n",
      "i = 2\n",
      "560\n",
      "i = 3\n",
      "3291\n",
      "i = 4\n",
      "875\n",
      "i = 5\n",
      "471\n",
      "i = 6\n",
      "668\n",
      "i = 7\n",
      "314\n",
      "i = 8\n",
      "6963\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(classes)):\n",
    "    df_temp = df_val_offset[df_val_offset['specie_ind'] == i]\n",
    "    print(\"i = \" +str(i))\n",
    "    print(len(df_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "652621b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temp.reset_index(inplace = True)\n",
    "df_train_offset.reset_index(inplace = True)\n",
    "df_test_offset.reset_index(inplace = True)\n",
    "df_val_offset.reset_index(inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fcc00127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the frame offsets for each audio file into dataframes\n",
    "# audio_df_train = get_offsets_df(df_train, short_audio=USE_SHORT_AUDIO)\n",
    "# audio_df_test_A = get_offsets_df(df_test_A, short_audio=False)\n",
    "# audio_df_test_B = get_offsets_df(df_test_B, short_audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce5a99a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function pads a file with 0s to make it a 1.92 sec file\n",
    "def pad_zero(x_temp,rate = config.rate, min_length = config.min_duration ):\n",
    "    #print(\"inside padding zero...\")\n",
    "    left_pad_amt = int((rate*min_length-x_temp.shape[1])//2)\n",
    "    #print(\"left_pad_amt = \" + str(left_pad_amt))\n",
    "    left_pad = torch.zeros(1,left_pad_amt) #+ (0.1**0.5)*torch.randn(1, left_pad_amt)\n",
    "    right_pad_amt = int(rate*min_length-x_temp.shape[1]-left_pad_amt)\n",
    "    right_pad = torch.zeros(1,right_pad_amt)# + (0.1**0.5)*torch.randn(1, right_pad_amt)\n",
    "    f = torch.cat([left_pad,x_temp,right_pad],dim=1)[0]\n",
    "    f = f.unsqueeze(dim = 0)\n",
    "    #print(\"returning a tensor of shape = \" + str(f.shape))\n",
    "    return(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b538853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcen(x, eps=1e-6, s=0.025, alpha=0.98, delta=2, r=0.5, training=False):\n",
    "    frames = x.split(1, -2)\n",
    "    m_frames = []\n",
    "    last_state = None\n",
    "    for frame in frames:\n",
    "        if last_state is None:\n",
    "            last_state = s * frame\n",
    "            m_frames.append(last_state)\n",
    "            continue\n",
    "        if training:\n",
    "            m_frame = ((1 - s) * last_state).add_(s * frame)\n",
    "        else:\n",
    "            m_frame = (1 - s) * last_state + s * frame\n",
    "        last_state = m_frame\n",
    "        m_frames.append(m_frame)\n",
    "    M = torch.cat(m_frames, 1)\n",
    "    if training:\n",
    "        pcen_ = (x / (M + eps).pow(alpha) + delta).pow(r) - delta ** r\n",
    "    else:\n",
    "        pcen_ = x.div_(M.add_(eps).pow_(alpha)).add_(delta).pow_(r).sub_(delta ** r)\n",
    "    return pcen_\n",
    "\n",
    "\n",
    "class PCENTransform(nn.Module):\n",
    "\n",
    "    def __init__(self, eps=1e-6, s=0.025, alpha=0.98, delta=2, r=0.5, trainable=True):\n",
    "        super().__init__()\n",
    "        if trainable:\n",
    "            self.log_s = nn.Parameter(torch.log(torch.Tensor([s])))\n",
    "            self.log_alpha = nn.Parameter(torch.log(torch.Tensor([alpha])))\n",
    "            self.log_delta = nn.Parameter(torch.log(torch.Tensor([delta])))\n",
    "            self.log_r = nn.Parameter(torch.log(torch.Tensor([r])))\n",
    "        else:\n",
    "            self.s = s\n",
    "            self.alpha = alpha\n",
    "            self.delta = delta\n",
    "            self.r = r\n",
    "        self.eps = eps\n",
    "        self.trainable = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x.permute((0,2,1)).squeeze(dim=1)\n",
    "        if self.trainable:\n",
    "            x = pcen(x, self.eps, torch.exp(self.log_s), torch.exp(self.log_alpha), torch.exp(self.log_delta), torch.exp(self.log_r), self.training and self.trainable)\n",
    "        else:\n",
    "            x = pcen(x, self.eps, self.s, self.alpha, self.delta, self.r, self.training and self.trainable)\n",
    "#         x = x.unsqueeze(dim=1).permute((0,1,3,2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "165372f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization():\n",
    "    \"\"\"This class is for normalizing the spectrograms batch by batch. The normalization used is min-max, two modes 'framewise' and 'imagewise' can be selected. In this paper, we found that 'imagewise' normalization works better than 'framewise'\"\"\"\n",
    "    def __init__(self, mode='framewise'):\n",
    "        if mode == 'framewise':\n",
    "            def normalize(x):\n",
    "                size = x.shape\n",
    "                x_max = x.max(1, keepdim=True)[0] # Finding max values for each frame\n",
    "                x_min = x.min(1, keepdim=True)[0]  \n",
    "                output = (x-x_min)/(x_max-x_min) # If there is a column with all zero, nan will occur\n",
    "                output[torch.isnan(output)]=0 # Making nan to 0\n",
    "                return output\n",
    "        elif mode == 'imagewise':\n",
    "            def normalize(x):\n",
    "                size = x.shape\n",
    "                x_max = x.reshape(size[0], size[1]*size[2]).max(1, keepdim=True)[0]\n",
    "                x_min = x.reshape(size[0], size[1]*size[2]).min(1, keepdim=True)[0]\n",
    "                x_max = x_max.unsqueeze(1) # Make it broadcastable\n",
    "                x_min = x_min.unsqueeze(1) # Make it broadcastable \n",
    "                return (x-x_min)/(x_max-x_min)\n",
    "        else:\n",
    "            print(f'please choose the correct mode')\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.normalize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55dd5734",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Moz_train_dataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, audio_df, setSize , data_dir, min_length, cache=None, transform=None,rate = config.rate):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "        self.setSize = setSize\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.setSize\n",
    "    \n",
    "    def _get_tensor_(self, path, resample=None):\n",
    "        waveform, inp_samp_rate = torchaudio.load(path)\n",
    "        \n",
    "        if inp_samp_rate != config.rate:\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_samp_rate, config.rate, dtype=waveform.dtype)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        if waveform.shape[1] < config.rate*config.min_duration :\n",
    "            #print(\"need to pad\")\n",
    "            waveform = pad_zero(waveform)\n",
    "            #waveform = waveform.unsqueeze(dim = 0)\n",
    "        \n",
    "        f = waveform[0]\n",
    "        mu = torch.std_mean(f)[1]\n",
    "        st = torch.std_mean(f)[0]\n",
    "            #return waveform, rate, waveform\n",
    "        f_out = torch.clamp(f, min=mu-st*3, max=mu+st*3).unsqueeze(0)\n",
    "        \n",
    "        return f_out, config.rate\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        all_uniq_ind = self.audio_df.specie_ind.unique()\n",
    "        if idx % 2 == 0:\n",
    "            # select the same character for both images\n",
    "            category = int(np.random.choice(a= all_uniq_ind, size= 1, replace=False))\n",
    "            #select two images belonging to the category chosen.\n",
    "            df_temp = self.audio_df[self.audio_df['specie_ind']== category].sample(2,replace = False )\n",
    "            df_temp.reset_index(inplace = True)\n",
    "            #print(\"df_temp = \" + str(df_temp))\n",
    "            row1 = df_temp.iloc[0]\n",
    "            row2 = df_temp.iloc[1]\n",
    "            #print(\"row1.id = \" + str(row1.id))\n",
    "            #print(\"row2.id = \" + str(row2.id))\n",
    "            label = 1.0\n",
    "            # x_full  and x2_full represnts the entire tensor representations of the wav file\n",
    "            x1_full, _ = self._get_tensor_(os.path.join(self.data_dir,f\"{row1.id}.wav\"), resample=config.rate)\n",
    "            x2_full, _ = self._get_tensor_(os.path.join(self.data_dir,f\"{row2.id}.wav\"), resample=config.rate)\n",
    "            #print(\"x1_full shape = \" + str(x1_full.shape))\n",
    "            #print(\"x2_full shape = \" + str(x2_full.shape))\n",
    "            \n",
    "            \n",
    "            r1_offset = row1.offset\n",
    "            r2_offset = row2.offset\n",
    "            #print(\"row1_offset = \" + str(r1_offset))\n",
    "            #print(\"row2_offset = \" + str(r2_offset))\n",
    "            \n",
    "            x1 = x1_full[:,r1_offset:int(r1_offset+config.rate*self.min_length)]\n",
    "            x2 = x2_full[:,r2_offset:int(r2_offset+config.rate*self.min_length)]\n",
    "            #print(\"x1 shape = \" + str(x1.shape))\n",
    "            #print(\"x2 shape = \" + str(x2.shape))\n",
    "            \n",
    "        else:\n",
    "            #print(\"^^^^^^^^ODD INDEX^^^^^^^^^^\")\n",
    "            category1, category2 = np.random.choice(a= all_uniq_ind, size= 2, replace=False)\n",
    "            df_temp_cat1 = self.audio_df[self.audio_df['specie_ind']== int(category1)].sample(1,replace = False )\n",
    "            df_temp_cat2 = self.audio_df[self.audio_df['specie_ind']== int(category2)].sample(1,replace = False )\n",
    "            df_temp_cat1.reset_index(inplace = True)\n",
    "            df_temp_cat2.reset_index(inplace = True)\n",
    "            label = 0.0\n",
    "            \n",
    "            row1 = df_temp_cat1.iloc[0]\n",
    "            row2 = df_temp_cat2.iloc[0]\n",
    "            #print(\"row1.id = \" + str(row1.id))\n",
    "            #print(\"row2.id = \" + str(row2.id))\n",
    "            \n",
    "            # x_full  and x2_full represnts the entire tensor representations of the wav file\n",
    "            x1_full, _ = self._get_tensor_(os.path.join(self.data_dir,f\"{row1.id}.wav\"), resample=config.rate)\n",
    "            x2_full, _ = self._get_tensor_(os.path.join(self.data_dir,f\"{row2.id}.wav\"), resample=config.rate)\n",
    "            #print(\"x1_full shape = \" + str(x1_full.shape))\n",
    "            #print(\"x2_full shape = \" + str(x2_full.shape))\n",
    "            \n",
    "            r1_offset = row1.offset\n",
    "            r2_offset = row2.offset\n",
    "            #print(\"row1_offset = \" + str(r1_offset))\n",
    "            #print(\"row2_offset = \" + str(r2_offset))\n",
    "            \n",
    "            x1 = x1_full[:,r1_offset:int(r1_offset+config.rate*self.min_length)]\n",
    "            x2 = x2_full[:,r2_offset:int(r2_offset+config.rate*self.min_length)]\n",
    "            #print(\"x1 shape = \" + str(x1.shape))\n",
    "            #print(\"x2 shape = \" + str(x2.shape))\n",
    "            \n",
    "            \n",
    "                                   \n",
    "        \n",
    "        return x1,x2,torch.from_numpy(np.array([label], dtype=np.float32))  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "333b9260",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Moz_test_dataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, audio_df, setSize , data_dir, min_length, cache=None, transform=None,rate = config.rate,numway = len(classes)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "        self.setSize = setSize\n",
    "        self.numway = numway\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.setSize\n",
    "    \n",
    "    def _get_tensor_(self, path, resample=None):\n",
    "        waveform, inp_samp_rate = torchaudio.load(path)\n",
    "        \n",
    "        if inp_samp_rate != config.rate:\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_samp_rate, config.rate, dtype=waveform.dtype)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        if waveform.shape[1] < config.rate*config.min_duration :\n",
    "            #print(\"need to pad\")\n",
    "            waveform = pad_zero(waveform)\n",
    "            #waveform = waveform.unsqueeze(dim = 0)\n",
    "        \n",
    "        f = waveform[0]\n",
    "        mu = torch.std_mean(f)[1]\n",
    "        st = torch.std_mean(f)[0]\n",
    "            #return waveform, rate, waveform\n",
    "        f_out = torch.clamp(f, min=mu-st*3, max=mu+st*3).unsqueeze(0)\n",
    "        \n",
    "        return f_out, config.rate\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        all_uniq_ind = self.audio_df.specie_ind.unique()\n",
    "        # find one main image\n",
    "        all_uniq_ind = df.specie_ind.unique()\n",
    "        category =  int(np.random.choice(a= all_uniq_ind, size= 1, replace=False))\n",
    "        #sample a rando value from the category chosen above\n",
    "        df_main = self.audio_df[self.audio_df['specie_ind']== category].sample(1,replace = False )\n",
    "        df_main.reset_index(inplace = True)\n",
    "        #print(\"df_main = \" + str(df_main))\n",
    "                 \n",
    "        # find n numbers of distinct images, 1 in the same set as the main\n",
    "        testSet = []\n",
    "        label = int(np.random.choice(a= all_uniq_ind, size= 1, replace=False))\n",
    "        #print(\"label ->\" +str(label))\n",
    "        #print(\"self.numway = \" +str(self.numway))\n",
    "        for i in range(self.numway):\n",
    "            #print(\"i = \" +str(i))\n",
    "            if i == label:\n",
    "                #estImgName = random.choice(os.listdir(imgDir))\n",
    "                df_name = \"df_temp_\" + str(label)\n",
    "                df_temp = self.audio_df[self.audio_df['specie_ind']== label].sample(1,replace = False )\n",
    "                #print(\"i == label\")\n",
    "                #print(\"df_temp = \" +str(df_temp))\n",
    "                            \n",
    "            else:\n",
    "                df_name = \"df_temp_\" + str(i)\n",
    "                testCategory = int(np.random.choice(a= all_uniq_ind, size= 1, replace=False))\n",
    "                df_temp = self.audio_df[self.audio_df['specie_ind']== testCategory].sample(1,replace = False )\n",
    "                \n",
    "                \n",
    "\n",
    "            testSet.append(df_temp)\n",
    "        df_test = pd.concat(testSet, ignore_index=True)\n",
    "        #print(\"df_test = \" +str(df_test))\n",
    "            #you loophere on the dataframe to get x's \n",
    "            \n",
    "            \n",
    "            \n",
    "        # x_full  and x2_full represnts the entire tensor representations of the wav file\n",
    "        #x1_full, _ = self._get_tensor_(os.path.join(self.data_dir,f\"{row1.id}.wav\"), resample=config.rate)\n",
    "        main_row = df_main.iloc[0]\n",
    "        #print(main_row)\n",
    "        x_main_full, _ = self._get_tensor_(os.path.join(self.data_dir,f\"{main_row.id}.wav\"), resample=config.rate)\n",
    "        #print(\"x_main_full shape = \" + str(x_main_full.shape))\n",
    "        \n",
    "        main_offset = main_row.offset\n",
    "        #print(\"main_offset  = \" + str(main_offset))\n",
    "        \n",
    "        x_main = x_main_full[:,main_offset:int(main_offset+config.rate*self.min_length)]\n",
    "        #print(\"x_main shape = \" + str(x_main.shape))\n",
    "        \n",
    "        x_test = []\n",
    "        for ind,row in df_test.iterrows():\n",
    "            row_id = row['id']\n",
    "            #print(\"inside loop.... row_id = \" + str(row_id))\n",
    "            x_full, _ = self._get_tensor_(os.path.join(self.data_dir,f\"{str(row_id)}.wav\"), resample=config.rate)\n",
    "            #print(\"inside loop.... x_full = \" + str(x_full.shape))\n",
    "            offset = row['offset']\n",
    "            #print(\"inside loop...offset  = \" + str(offset))\n",
    "            x_temp = x_full[:,offset:int(offset+config.rate*self.min_length)]\n",
    "            #print(\"inside loop...x_temp shape   = \" + str(x_temp.shape))\n",
    "            \n",
    "            x_test.append(x_temp)\n",
    "        \n",
    "        return x_main,x_test,torch.from_numpy(np.array([label], dtype=np.float32))  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4cd34644",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, model_name, image_size,threshold = .5):\n",
    "        super().__init__()\n",
    "        # num_classes=0 removes the pretrained head\n",
    "        self.backbone = timm.create_model(model_name,\n",
    "                        pretrained=True,  in_chans=1, \n",
    "                        drop_path_rate=0.05, global_pool='max',\n",
    "                        drop_rate=0.05)\n",
    "        \n",
    "        self.spec_layer = features.STFT(n_fft=config.NFFT, freq_bins=None, hop_length=config.n_hop,\n",
    "                              window='hann', freq_scale='linear', center=True, pad_mode='reflect',\n",
    "                          fmin=400, fmax=2000, sr=config.rate, output_format=\"Magnitude\", trainable= True)\n",
    "        self.out = nn.Linear(self.backbone.num_features, 1)\n",
    "        self.sizer = VT.Resize((image_size,image_size))\n",
    "        self.timeMasking = T.TimeMasking(time_mask_param=int(config.win_size*0.4), iid_masks=True)\n",
    "        self.freqMasking = T.FrequencyMasking(freq_mask_param=int((config.NFFT//4)*0.15), iid_masks=True)\n",
    "        self.norm_layer = Normalization(mode='framewise')\n",
    "        self.pcen_layer = PCENTransform(eps=1e-6, s=0.025, alpha=0.6, delta=0.1, r=0.2, trainable=True)\n",
    "        #1000 due to the size of the final layer in convnext\n",
    "        self.fcOut = nn.Linear(1000, 1)\n",
    "        self.out_new  = nn.Sigmoid()\n",
    "        self.threshold  = threshold\n",
    "        \n",
    "        \n",
    "    def forward(self, x1,x2):\n",
    "        # first compute spectrogram\n",
    "        spec1 = self.spec_layer(x1)  # (B, F, T)\n",
    "        # normalize\n",
    "#         spec = spec.transpose(1,2) # (B, T, F)\n",
    "        spec1 = self.pcen_layer(spec1)\n",
    "        spec1 = self.norm_layer(spec1)\n",
    "        \n",
    "#         if self.training:\n",
    "#             spec = self.timeMasking(spec)\n",
    "#             spec = self.freqMasking(spec)\n",
    "\n",
    "        # then size for CNN model\n",
    "        # and create a channel\n",
    "        spec1 = self.sizer(spec1)\n",
    "        #print(\"spec1 shape post STFT = \" +str(spec1.shape))\n",
    "        x1 = spec1.unsqueeze(1)\n",
    "        #print(\"post unsqueeze x1 shape = \" +str(x1.shape))\n",
    "        # then repeat channels\n",
    "        x1 = self.backbone(x1)\n",
    "        #print(\"post backbone . x1 shape = \" +str(x1.shape))\n",
    "        \n",
    "        \n",
    "        spec2 = self.spec_layer(x2)  # (B, F, T)\n",
    "        # normalize\n",
    "#         spec = spec.transpose(1,2) # (B, T, F)\n",
    "        spec2 = self.pcen_layer(spec2)\n",
    "        spec2 = self.norm_layer(spec2)\n",
    "        \n",
    "#         if self.training:\n",
    "#             spec = self.timeMasking(spec)\n",
    "#             spec = self.freqMasking(spec)\n",
    "\n",
    "        # then size for CNN model\n",
    "        # and create a channel\n",
    "        spec2 = self.sizer(spec2)\n",
    "        #print(\"spec2 shape post STFT = \" +str(spec2.shape))\n",
    "        x2 = spec2.unsqueeze(1)\n",
    "        #print(\"post unsqueeze x2 shape = \" +str(x2.shape))\n",
    "        # then repeat channels\n",
    "        x2 = self.backbone(x2)\n",
    "        #print(\"post backbone . x2 shape = \" +str(x2.shape))\n",
    "              \n",
    "        x = torch.abs(x1-x2)\n",
    "        #print(\" x shape = \" +str(x.shape))\n",
    "        out = self.fcOut(x)\n",
    "        #print(\"output of fcout = \" +str(x))\n",
    "        #print(\"output = \" +str(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798575b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "861c30d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers= 8\n",
    "pin_memory=True\n",
    "train_size = 100000\n",
    "batch_size = 64\n",
    "test_batch_size = 1 \n",
    "\n",
    "\n",
    "train_dataset = Moz_train_dataset(audio_df = df_train_offset,data_dir = config.data_dir, setSize = train_size, min_length = config.min_duration)\n",
    "val_dataset = Moz_train_dataset(audio_df = df_train_offset,data_dir = config.data_dir, setSize = int(train_size*.2), min_length = config.min_duration)\n",
    "test_dataset = Moz_test_dataset(audio_df = df_test_offset, setSize = int(train_size*.01),  data_dir = config.data_dir, min_length= config.min_duration, numway = len(classes))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True, shuffle = True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=config_pytorch.batch_size, num_workers=num_workers, pin_memory=False , shuffle = False )\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, num_workers=num_workers, pin_memory=False , shuffle = False )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86810779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d559dd4a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f3d32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling rate = 8000. Please make sure the sampling rate is correct in order toget a valid freq range\n",
      "STFT kernels created, time used = 0.0796 seconds\n",
      "Training on cuda:0\n",
      "epoch = 0processed batch 0 of 1563\n",
      "duration = 0.10362240870793661\n",
      "epoch = 0processed batch 300 of 1563\n",
      "duration = 4.664414318402608\n",
      "epoch = 0processed batch 600 of 1563\n",
      "duration = 9.198399603366852\n",
      "epoch = 0processed batch 900 of 1563\n",
      "duration = 13.726795593897501\n",
      "epoch = 0processed batch 1200 of 1563\n",
      "duration = 18.256996301809945\n",
      "epoch = 0processed batch 1500 of 1563\n",
      "duration = 22.786646298567454\n",
      "updating the best val loss..\n",
      "saving the model ...\n",
      "Saving model to: ../outputs/models/pytorch/model_e0_2022_09_07_05_48_04.pth\n",
      "length of test loader = 1000\n",
      "Now printing classification report...\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "        an arabiensis       0.10      0.12      0.11       102\n",
      "culex pipiens complex       0.19      0.19      0.19       133\n",
      "           ae aegypti       0.16      0.16      0.16       108\n",
      "       an funestus ss       0.08      0.06      0.07       127\n",
      "         an squamosus       0.12      0.12      0.12        94\n",
      "          an coustani       0.16      0.16      0.16       114\n",
      "         ma uniformis       0.07      0.07      0.07       115\n",
      "         ma africanus       0.18      0.16      0.17       124\n",
      "               others       0.11      0.14      0.12        83\n",
      "\n",
      "             accuracy                           0.13      1000\n",
      "            macro avg       0.13      0.13      0.13      1000\n",
      "         weighted avg       0.13      0.13      0.13      1000\n",
      "\n",
      "epoch = 1processed batch 0 of 1563\n",
      "duration = 0.10455541610717774\n",
      "epoch = 1processed batch 300 of 1563\n",
      "duration = 4.665176264444987\n",
      "epoch = 1processed batch 600 of 1563\n",
      "duration = 9.20359794696172\n",
      "epoch = 1processed batch 900 of 1563\n",
      "duration = 13.733861275513966\n",
      "epoch = 1processed batch 1200 of 1563\n",
      "duration = 18.253901632626853\n",
      "epoch = 1processed batch 1500 of 1563\n",
      "duration = 22.777303167184193\n",
      "updating the best val loss..\n",
      "saving the model ...\n",
      "Saving model to: ../outputs/models/pytorch/model_e1_2022_09_07_06_29_59.pth\n",
      "length of test loader = 1000\n",
      "Now printing classification report...\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "        an arabiensis       0.21      0.19      0.20       123\n",
      "culex pipiens complex       0.10      0.10      0.10       109\n",
      "           ae aegypti       0.12      0.10      0.11       119\n",
      "       an funestus ss       0.08      0.09      0.08       117\n",
      "         an squamosus       0.13      0.16      0.14        96\n",
      "          an coustani       0.16      0.12      0.14       120\n",
      "         ma uniformis       0.13      0.17      0.15        95\n",
      "         ma africanus       0.13      0.15      0.14       102\n",
      "               others       0.11      0.11      0.11       119\n",
      "\n",
      "             accuracy                           0.13      1000\n",
      "            macro avg       0.13      0.13      0.13      1000\n",
      "         weighted avg       0.13      0.13      0.13      1000\n",
      "\n",
      "epoch = 2processed batch 0 of 1563\n",
      "duration = 0.09787946939468384\n",
      "epoch = 2processed batch 300 of 1563\n",
      "duration = 4.635427049795786\n",
      "epoch = 2processed batch 600 of 1563\n",
      "duration = 9.158815026283264\n",
      "epoch = 2processed batch 900 of 1563\n",
      "duration = 13.678641784191132\n",
      "epoch = 2processed batch 1200 of 1563\n",
      "duration = 18.215722930431365\n",
      "epoch = 2processed batch 1500 of 1563\n",
      "duration = 22.73613384962082\n",
      "updating the best val loss..\n",
      "saving the model ...\n",
      "Saving model to: ../outputs/models/pytorch/model_e2_2022_09_07_07_11_50.pth\n",
      "length of test loader = 1000\n",
      "Now printing classification report...\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "        an arabiensis       0.11      0.09      0.10       112\n",
      "culex pipiens complex       0.14      0.15      0.14       102\n",
      "           ae aegypti       0.08      0.08      0.08       108\n",
      "       an funestus ss       0.12      0.12      0.12       113\n",
      "         an squamosus       0.10      0.10      0.10       105\n",
      "          an coustani       0.05      0.06      0.05       108\n",
      "         ma uniformis       0.14      0.13      0.14       129\n",
      "         ma africanus       0.12      0.15      0.13       116\n",
      "               others       0.06      0.07      0.06       107\n",
      "\n",
      "             accuracy                           0.10      1000\n",
      "            macro avg       0.10      0.10      0.10      1000\n",
      "         weighted avg       0.10      0.10      0.10      1000\n",
      "\n",
      "epoch = 3processed batch 0 of 1563\n",
      "duration = 0.0983948270479838\n",
      "epoch = 3processed batch 300 of 1563\n",
      "duration = 4.648858213424683\n",
      "epoch = 3processed batch 600 of 1563\n",
      "duration = 9.18343990246455\n",
      "epoch = 3processed batch 900 of 1563\n",
      "duration = 13.729731273651122\n",
      "epoch = 3processed batch 1200 of 1563\n",
      "duration = 18.26605015595754\n",
      "epoch = 3processed batch 1500 of 1563\n",
      "duration = 22.798667732874552\n",
      "updating the best val loss..\n",
      "saving the model ...\n",
      "Saving model to: ../outputs/models/pytorch/model_e3_2022_09_07_07_53_45.pth\n",
      "length of test loader = 1000\n",
      "Now printing classification report...\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "        an arabiensis       0.17      0.18      0.18       123\n",
      "culex pipiens complex       0.13      0.15      0.14       106\n",
      "           ae aegypti       0.07      0.07      0.07       116\n",
      "       an funestus ss       0.12      0.14      0.13        91\n",
      "         an squamosus       0.14      0.13      0.14       106\n",
      "          an coustani       0.12      0.12      0.12       110\n",
      "         ma uniformis       0.16      0.13      0.14       129\n",
      "         ma africanus       0.12      0.14      0.13       107\n",
      "               others       0.08      0.07      0.08       112\n",
      "\n",
      "             accuracy                           0.13      1000\n",
      "            macro avg       0.12      0.13      0.12      1000\n",
      "         weighted avg       0.13      0.13      0.13      1000\n",
      "\n",
      "epoch = 4processed batch 0 of 1563\n",
      "duration = 0.10330888032913207\n",
      "epoch = 4processed batch 300 of 1563\n",
      "duration = 4.64141134818395\n",
      "epoch = 4processed batch 600 of 1563\n",
      "duration = 9.165876456101735\n",
      "epoch = 4processed batch 900 of 1563\n",
      "duration = 13.692493069171906\n",
      "epoch = 4processed batch 1200 of 1563\n",
      "duration = 18.206320897738138\n",
      "epoch = 4processed batch 1500 of 1563\n",
      "duration = 22.72420528729757\n",
      "updating the best val loss..\n",
      "saving the model ...\n",
      "Saving model to: ../outputs/models/pytorch/model_e4_2022_09_07_08_35_29.pth\n",
      "length of test loader = 1000\n",
      "Now printing classification report...\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "        an arabiensis       0.15      0.15      0.15       119\n",
      "culex pipiens complex       0.11      0.10      0.10       114\n",
      "           ae aegypti       0.07      0.08      0.07       106\n",
      "       an funestus ss       0.14      0.17      0.15        99\n",
      "         an squamosus       0.08      0.08      0.08       107\n",
      "          an coustani       0.09      0.07      0.08       128\n",
      "         ma uniformis       0.08      0.07      0.08       108\n",
      "         ma africanus       0.08      0.08      0.08       103\n",
      "               others       0.10      0.10      0.10       116\n",
      "\n",
      "             accuracy                           0.10      1000\n",
      "            macro avg       0.10      0.10      0.10      1000\n",
      "         weighted avg       0.10      0.10      0.10      1000\n",
      "\n",
      "epoch = 5processed batch 0 of 1563\n",
      "duration = 0.09019970893859863\n",
      "epoch = 5processed batch 300 of 1563\n",
      "duration = 4.618306052684784\n",
      "epoch = 5processed batch 600 of 1563\n",
      "duration = 9.13486129840215\n",
      "epoch = 5processed batch 900 of 1563\n",
      "duration = 13.659171342849731\n",
      "epoch = 5processed batch 1200 of 1563\n",
      "duration = 18.18963995774587\n",
      "epoch = 5processed batch 1500 of 1563\n",
      "duration = 22.70838451385498\n",
      "updating the best val loss..\n",
      "saving the model ...\n",
      "Saving model to: ../outputs/models/pytorch/model_e5_2022_09_07_09_17_19.pth\n",
      "length of test loader = 1000\n",
      "Now printing classification report...\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "        an arabiensis       0.17      0.17      0.17       120\n",
      "culex pipiens complex       0.09      0.08      0.08       126\n",
      "           ae aegypti       0.07      0.08      0.07       101\n",
      "       an funestus ss       0.10      0.12      0.11       107\n",
      "         an squamosus       0.13      0.12      0.13        98\n",
      "          an coustani       0.12      0.12      0.12       113\n",
      "         ma uniformis       0.06      0.06      0.06       106\n",
      "         ma africanus       0.10      0.11      0.11       115\n",
      "               others       0.08      0.07      0.08       114\n",
      "\n",
      "             accuracy                           0.10      1000\n",
      "            macro avg       0.10      0.10      0.10      1000\n",
      "         weighted avg       0.10      0.10      0.10      1000\n",
      "\n",
      "epoch = 6processed batch 0 of 1563\n",
      "duration = 0.09613723754882812\n",
      "epoch = 6processed batch 300 of 1563\n",
      "duration = 4.661897774537405\n",
      "epoch = 6processed batch 600 of 1563\n",
      "duration = 9.205254896481831\n",
      "epoch = 6processed batch 900 of 1563\n",
      "duration = 13.743273858229319\n",
      "epoch = 6processed batch 1200 of 1563\n",
      "duration = 18.26269455750783\n",
      "epoch = 6processed batch 1500 of 1563\n",
      "duration = 22.78714141448339\n",
      "updating the best val loss..\n",
      "saving the model ...\n",
      "Saving model to: ../outputs/models/pytorch/model_e6_2022_09_07_09_59_14.pth\n",
      "length of test loader = 1000\n",
      "Now printing classification report...\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "        an arabiensis       0.18      0.18      0.18       104\n",
      "culex pipiens complex       0.15      0.17      0.16       114\n",
      "           ae aegypti       0.07      0.06      0.06       126\n",
      "       an funestus ss       0.11      0.13      0.12        95\n",
      "         an squamosus       0.10      0.12      0.11       109\n",
      "          an coustani       0.07      0.06      0.06       115\n",
      "         ma uniformis       0.10      0.10      0.10       110\n",
      "         ma africanus       0.06      0.05      0.05       114\n",
      "               others       0.15      0.14      0.15       113\n",
      "\n",
      "             accuracy                           0.11      1000\n",
      "            macro avg       0.11      0.11      0.11      1000\n",
      "         weighted avg       0.11      0.11      0.11      1000\n",
      "\n",
      "epoch = 7processed batch 0 of 1563\n",
      "duration = 0.09764637947082519\n",
      "epoch = 7processed batch 300 of 1563\n",
      "duration = 4.63481965859731\n",
      "epoch = 7processed batch 600 of 1563\n",
      "duration = 9.172941704591116\n",
      "epoch = 7processed batch 900 of 1563\n",
      "duration = 13.703379340966542\n",
      "epoch = 7processed batch 1200 of 1563\n",
      "duration = 18.22660963535309\n",
      "epoch = 7processed batch 1500 of 1563\n",
      "duration = 22.76179625590642\n",
      "updating the best val loss..\n",
      "saving the model ...\n",
      "Saving model to: ../outputs/models/pytorch/model_e7_2022_09_07_10_41_06.pth\n",
      "length of test loader = 1000\n",
      "Now printing classification report...\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "        an arabiensis       0.14      0.15      0.14       116\n",
      "culex pipiens complex       0.10      0.12      0.11       103\n",
      "           ae aegypti       0.10      0.12      0.11       104\n",
      "       an funestus ss       0.10      0.10      0.10       115\n",
      "         an squamosus       0.15      0.15      0.15       114\n",
      "          an coustani       0.11      0.11      0.11        99\n",
      "         ma uniformis       0.08      0.07      0.07       133\n",
      "         ma africanus       0.13      0.10      0.11       118\n",
      "               others       0.10      0.11      0.11        98\n",
      "\n",
      "             accuracy                           0.11      1000\n",
      "            macro avg       0.11      0.11      0.11      1000\n",
      "         weighted avg       0.11      0.11      0.11      1000\n",
      "\n",
      "epoch = 8processed batch 0 of 1563\n",
      "duration = 0.09925459623336792\n",
      "epoch = 8processed batch 300 of 1563\n",
      "duration = 4.642135151227316\n",
      "epoch = 8processed batch 600 of 1563\n",
      "duration = 9.169908674558004\n",
      "epoch = 8processed batch 900 of 1563\n",
      "duration = 13.706285671393077\n",
      "epoch = 8processed batch 1200 of 1563\n",
      "duration = 18.246524858474732\n",
      "epoch = 8processed batch 1500 of 1563\n",
      "duration = 22.788227951526643\n",
      "updating the best val loss..\n",
      "saving the model ...\n",
      "Saving model to: ../outputs/models/pytorch/model_e8_2022_09_07_11_23_04.pth\n",
      "length of test loader = 1000\n",
      "Now printing classification report...\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "        an arabiensis       0.15      0.18      0.16        90\n",
      "culex pipiens complex       0.11      0.13      0.12       109\n",
      "           ae aegypti       0.10      0.10      0.10       102\n",
      "       an funestus ss       0.07      0.08      0.08       101\n",
      "         an squamosus       0.15      0.13      0.14       127\n",
      "          an coustani       0.11      0.10      0.10       120\n",
      "         ma uniformis       0.11      0.14      0.12       109\n",
      "         ma africanus       0.04      0.03      0.03       116\n",
      "               others       0.09      0.06      0.07       126\n",
      "\n",
      "             accuracy                           0.10      1000\n",
      "            macro avg       0.10      0.11      0.10      1000\n",
      "         weighted avg       0.10      0.10      0.10      1000\n",
      "\n",
      "epoch = 9processed batch 0 of 1563\n",
      "duration = 0.10920048157374064\n",
      "epoch = 9processed batch 300 of 1563\n",
      "duration = 4.664940317471822\n",
      "epoch = 9processed batch 600 of 1563\n",
      "duration = 9.196770497163136\n",
      "epoch = 9processed batch 900 of 1563\n",
      "duration = 13.710206993420918\n",
      "epoch = 9processed batch 1200 of 1563\n",
      "duration = 18.236898529529572\n",
      "epoch = 9processed batch 1500 of 1563\n",
      "duration = 22.74418394168218\n",
      "epoch = 10processed batch 0 of 1563\n",
      "duration = 0.09672619104385376\n",
      "epoch = 10processed batch 300 of 1563\n",
      "duration = 4.642799234390258\n",
      "epoch = 10processed batch 600 of 1563\n",
      "duration = 9.175063304106395\n",
      "epoch = 10processed batch 900 of 1563\n",
      "duration = 13.701716355482738\n",
      "epoch = 10processed batch 1200 of 1563\n",
      "duration = 18.228034687042236\n",
      "epoch = 10processed batch 1500 of 1563\n",
      "duration = 22.744465696811677\n",
      "updating the best val loss..\n",
      "saving the model ...\n",
      "Saving model to: ../outputs/models/pytorch/model_e10_2022_09_07_12_32_24.pth\n",
      "length of test loader = 1000\n",
      "Now printing classification report...\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "        an arabiensis       0.14      0.14      0.14       121\n",
      "culex pipiens complex       0.09      0.12      0.10        94\n",
      "           ae aegypti       0.07      0.08      0.07       103\n",
      "       an funestus ss       0.13      0.11      0.12       110\n",
      "         an squamosus       0.09      0.08      0.09       127\n",
      "          an coustani       0.11      0.13      0.12       103\n",
      "         ma uniformis       0.07      0.07      0.07       121\n",
      "         ma africanus       0.07      0.06      0.07       111\n",
      "               others       0.12      0.12      0.12       110\n",
      "\n",
      "             accuracy                           0.10      1000\n",
      "            macro avg       0.10      0.10      0.10      1000\n",
      "         weighted avg       0.10      0.10      0.10      1000\n",
      "\n",
      "epoch = 11processed batch 0 of 1563\n",
      "duration = 0.09875346819559733\n",
      "epoch = 11processed batch 300 of 1563\n",
      "duration = 4.64838019212087\n",
      "epoch = 11processed batch 600 of 1563\n",
      "duration = 9.17006285985311\n",
      "epoch = 11processed batch 900 of 1563\n",
      "duration = 13.663281591733297\n",
      "epoch = 11processed batch 1200 of 1563\n",
      "duration = 18.17543983856837\n",
      "epoch = 11processed batch 1500 of 1563\n",
      "duration = 22.685950084527335\n",
      "updating the best val loss..\n",
      "saving the model ...\n",
      "Saving model to: ../outputs/models/pytorch/model_e11_2022_09_07_13_14_13.pth\n",
      "length of test loader = 1000\n",
      "Now printing classification report...\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "        an arabiensis       0.16      0.17      0.16       104\n",
      "culex pipiens complex       0.14      0.14      0.14       104\n",
      "           ae aegypti       0.14      0.12      0.13       114\n",
      "       an funestus ss       0.12      0.13      0.12       110\n",
      "         an squamosus       0.12      0.13      0.12       106\n",
      "          an coustani       0.07      0.07      0.07       113\n",
      "         ma uniformis       0.10      0.11      0.10       113\n",
      "         ma africanus       0.12      0.10      0.11       124\n",
      "               others       0.11      0.12      0.12       112\n",
      "\n",
      "             accuracy                           0.12      1000\n",
      "            macro avg       0.12      0.12      0.12      1000\n",
      "         weighted avg       0.12      0.12      0.12      1000\n",
      "\n",
      "epoch = 12processed batch 0 of 1563\n",
      "duration = 0.10304123163223267\n",
      "epoch = 12processed batch 300 of 1563\n",
      "duration = 4.635726606845855\n",
      "epoch = 12processed batch 600 of 1563\n",
      "duration = 9.152956859270732\n",
      "epoch = 12processed batch 900 of 1563\n",
      "duration = 13.659618790944418\n",
      "epoch = 12processed batch 1200 of 1563\n",
      "duration = 18.18098531961441\n",
      "epoch = 12processed batch 1500 of 1563\n",
      "duration = 22.709557044506074\n",
      "epoch = 13processed batch 0 of 1563\n",
      "duration = 0.09419862826665243\n",
      "epoch = 13processed batch 300 of 1563\n",
      "duration = 4.63459541797638\n",
      "epoch = 13processed batch 600 of 1563\n",
      "duration = 9.153074196974437\n",
      "epoch = 13processed batch 900 of 1563\n",
      "duration = 13.658631992340087\n",
      "epoch = 13processed batch 1200 of 1563\n",
      "duration = 18.159283089637757\n",
      "epoch = 13processed batch 1500 of 1563\n",
      "duration = 22.683822302023568\n",
      "updating the best val loss..\n",
      "saving the model ...\n",
      "Saving model to: ../outputs/models/pytorch/model_e13_2022_09_07_14_23_24.pth\n",
      "length of test loader = 1000\n",
      "Now printing classification report...\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "        an arabiensis       0.14      0.17      0.16        98\n",
      "culex pipiens complex       0.09      0.11      0.10       104\n",
      "           ae aegypti       0.14      0.13      0.13       120\n",
      "       an funestus ss       0.08      0.09      0.09       104\n",
      "         an squamosus       0.06      0.06      0.06       102\n",
      "          an coustani       0.11      0.10      0.11       130\n",
      "         ma uniformis       0.14      0.11      0.12       126\n",
      "         ma africanus       0.09      0.08      0.09       108\n",
      "               others       0.10      0.11      0.11       108\n",
      "\n",
      "             accuracy                           0.11      1000\n",
      "            macro avg       0.11      0.11      0.11      1000\n",
      "         weighted avg       0.11      0.11      0.11      1000\n",
      "\n",
      "epoch = 14processed batch 0 of 1563\n",
      "duration = 0.10199786027272542\n",
      "epoch = 14processed batch 300 of 1563\n",
      "duration = 4.641878128051758\n",
      "epoch = 14processed batch 600 of 1563\n",
      "duration = 9.172574079036712\n",
      "epoch = 14processed batch 900 of 1563\n",
      "duration = 13.697265323003133\n",
      "epoch = 14processed batch 1200 of 1563\n",
      "duration = 18.22765342394511\n",
      "epoch = 14processed batch 1500 of 1563\n",
      "duration = 22.742808202902477\n",
      "updating the best val loss..\n",
      "saving the model ...\n",
      "Saving model to: ../outputs/models/pytorch/model_e14_2022_09_07_15_05_15.pth\n",
      "length of test loader = 1000\n",
      "Now printing classification report...\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "        an arabiensis       0.23      0.22      0.22       128\n",
      "culex pipiens complex       0.19      0.17      0.18       116\n",
      "           ae aegypti       0.11      0.10      0.11       109\n",
      "       an funestus ss       0.04      0.04      0.04       102\n",
      "         an squamosus       0.11      0.11      0.11       118\n",
      "          an coustani       0.05      0.07      0.06        87\n",
      "         ma uniformis       0.12      0.11      0.11       113\n",
      "         ma africanus       0.15      0.14      0.14       114\n",
      "               others       0.18      0.18      0.18       113\n",
      "\n",
      "             accuracy                           0.13      1000\n",
      "            macro avg       0.13      0.13      0.13      1000\n",
      "         weighted avg       0.13      0.13      0.13      1000\n",
      "\n",
      "epoch = 15processed batch 0 of 1563\n",
      "duration = 0.1014952818552653\n",
      "epoch = 15processed batch 300 of 1563\n",
      "duration = 4.6281462947527565\n",
      "epoch = 15processed batch 600 of 1563\n",
      "duration = 9.13658457994461\n",
      "epoch = 15processed batch 900 of 1563\n",
      "duration = 13.646125849088033\n"
     ]
    }
   ],
   "source": [
    "model =Model('convnext_small',224)\n",
    "model, lr_log = train_siamese(model , train_loader, val_loader,test_loader,classes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fbd83a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c39e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.tensor([[-0.0742],\n",
    "        [-0.6300],\n",
    "        [-0.2383],\n",
    "        [-0.1598]], device='cuda:0')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e33e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.tensor(8, device = \"cuda\")\n",
    "print(label)\n",
    "label_cpu = label.cpu().detach()\n",
    "print(label_cpu)\n",
    "label_np = label_cpu.numpy()\n",
    "print(type(label_np))\n",
    "label_np_item = label_np.item()\n",
    "print(type(label_np_item))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344706f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
