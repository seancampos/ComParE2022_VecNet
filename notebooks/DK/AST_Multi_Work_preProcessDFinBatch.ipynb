{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f79e618a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torchaudio in /opt/conda/lib/python3.8/site-packages (0.10.2)\n",
      "Requirement already satisfied: torch==1.10.2 in /opt/conda/lib/python3.8/site-packages (from torchaudio) (1.10.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.10.2->torchaudio) (4.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68ea9165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pydub in /opt/conda/lib/python3.8/site-packages (0.25.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.8/site-packages (0.19.2)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.8/site-packages (from scikit-image) (2022.2.9)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from scikit-image) (1.2.0)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /opt/conda/lib/python3.8/site-packages (from scikit-image) (2.16.1)\n",
      "Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.8/site-packages (from scikit-image) (2.7)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from scikit-image) (1.6.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from scikit-image) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.8/site-packages (from scikit-image) (1.21.4)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from scikit-image) (9.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->scikit-image) (3.0.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pydub\n",
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d19014bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../lib'))\n",
    "#import config\n",
    "import config_DK_AST\n",
    "#import config\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "from torchvision import datasets\n",
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "import skimage.util\n",
    "import pickle\n",
    "import math\n",
    "import collections\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from scipy import signal\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter , filtfilt\n",
    "import scipy.io as sio\n",
    "import scipy.io.wavfile\n",
    "import copy\n",
    "from PyTorch import config_pytorch\n",
    "from PyTorch.runTorch_AST_DK import ASTModel\n",
    "from evaluate import get_results\n",
    "from pydub import AudioSegment\n",
    "import math\n",
    "import pathlib\n",
    "import random\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86614e61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4321a54",
   "metadata": {},
   "source": [
    "## Few Global params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bb99def",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.path.pardir, 'data', 'audio')\n",
    "plot_dir = os.path.join(os.path.pardir, 'outputs',  'plots')\n",
    "model_dir = os.path.join(os.path.pardir, 'outputs', 'models') # Model sub-directory created in config_keras or config_pytorch\n",
    "\n",
    "# set this to True if you want to call the bandpass filter\n",
    "call_filter = False\n",
    "# This is the duration of the max. len train file. Due to randomization the length should decrease.\n",
    "max_train_mel_len = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7a055e",
   "metadata": {},
   "source": [
    "## Class Definitions for Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "439edf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://jonathanbgn.com/2021/08/30/audio-augmentation.html\n",
    "class RandomBackgroundNoise:\n",
    "    def __init__(self,  noise_dir ,sample_rate=config_DK_AST.rate, min_snr_db=0, max_snr_db=10):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.min_snr_db = min_snr_db\n",
    "        self.max_snr_db = max_snr_db\n",
    "\n",
    "        if not os.path.exists(noise_dir):\n",
    "            raise IOError(f'Noise directory `{noise_dir}` does not exist')\n",
    "        # find all WAV files including in sub-folders:\n",
    "        self.noise_files_list = list(pathlib.Path(noise_dir).glob('**/*.wav'))\n",
    "        if len(self.noise_files_list) == 0:\n",
    "            raise IOError(f'No .wav file found in the noise directory `{noise_dir}`')\n",
    "\n",
    "    def __call__(self, audio_data):\n",
    "        random_noise_file = random.choice(self.noise_files_list)\n",
    "        effects = [\n",
    "            ['remix', '1'], # convert to mono\n",
    "            ['rate', str(self.sample_rate)], # resample\n",
    "        ]\n",
    "        noise, _ = torchaudio.sox_effects.apply_effects_file(random_noise_file, effects, normalize=True)\n",
    "        audio_length = audio_data.shape[-1]\n",
    "        noise_length = noise.shape[-1]\n",
    "        if noise_length > audio_length:\n",
    "            offset = random.randint(0, noise_length-audio_length)\n",
    "            noise = noise[..., offset:offset+audio_length]\n",
    "        elif noise_length < audio_length:\n",
    "            noise = torch.cat([noise, torch.zeros((noise.shape[0], audio_length-noise_length))], dim=-1)\n",
    "\n",
    "        snr_db = random.randint(self.min_snr_db, self.max_snr_db)\n",
    "        snr = math.exp(snr_db / 10)\n",
    "        audio_power = audio_data.norm(p=2)\n",
    "        noise_power = noise.norm(p=2)\n",
    "        scale = snr * noise_power / audio_power\n",
    "\n",
    "        return (scale * audio_data + noise ) / 2\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9a18bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSpeedChange:\n",
    "    def __init__(self, sample_rate = config_DK_AST.rate):\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "    def __call__(self, audio_data):\n",
    "        speed_factor = random.choice([0.9, 1.0, 1.1])\n",
    "        if speed_factor == 1.0: # no change\n",
    "            return audio_data\n",
    "\n",
    "        # change speed and resample to original rate:\n",
    "        sox_effects = [\n",
    "            [\"speed\", str(speed_factor)],\n",
    "            [\"rate\", str(self.sample_rate)],\n",
    "        ]\n",
    "        transformed_audio, _ = torchaudio.sox_effects.apply_effects_tensor(\n",
    "            audio_data, self.sample_rate, sox_effects)\n",
    "        return transformed_audio\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7931f679",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComposeTransform:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, audio_data):\n",
    "        for t in self.transforms:\n",
    "            audio_data = t(audio_data)\n",
    "        return audio_data\n",
    "\n",
    "# 2 kind of transformations on the incoming training data\n",
    "compose_transform_aug_noise = ComposeTransform([\n",
    "    #RandomClip(sample_rate=sample_rate, sequence_length=64000),\n",
    "    RandomSpeedChange(config_DK_AST.rate  ),\n",
    "    RandomBackgroundNoise(noise_dir = \"../data/audio/noise\" )])\n",
    "\n",
    "compose_transform_aug_signal = ComposeTransform([\n",
    "    #RandomClip(sample_rate=sample_rate, sequence_length=64000),\n",
    "    RandomSpeedChange(config_DK_AST.rate  ),\n",
    "    RandomBackgroundNoise(noise_dir = \"../data/audio/signal\" , min_snr_db=5, max_snr_db=20)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "194befda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioUtil(ComposeTransform,RandomSpeedChange,RandomBackgroundNoise):\n",
    "## Function to apply Spectograms    \n",
    "    @staticmethod\n",
    "    def spec_augment(feat, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "        print(\"......APplying spec augment.....\")\n",
    "        _, n_mels, n_steps = feat.shape\n",
    "        #print(\"n_mels = \" + str(n_mels))\n",
    "        #print(\"n_steps = \" + str(n_steps))\n",
    "        mask_value = feat.mean()\n",
    "        #print(\"mask_val = \" + str(mask_value))\n",
    "        feat_aug = feat\n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "        \n",
    "        for _ in range(n_freq_masks):\n",
    "            feat_aug = T.FrequencyMasking(freq_mask_param)(feat_aug, mask_value)\n",
    "            \n",
    "        \n",
    "\n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "        for _ in range(n_time_masks):\n",
    "            feat_aug = T.TimeMasking(time_mask_param)(feat_aug, mask_value)\n",
    "            \n",
    "        \n",
    "        return feat_aug\n",
    "    \n",
    "    # This function gets the expected number of spectograms for a file.\n",
    "    # THis is needed because randomization/length and padding at times add an extra spectogram and we need to compare this expected number with the \n",
    "    # one that is returned from reshape\n",
    "    @staticmethod\n",
    "    def expected_specgrams(filename, label,size = config_DK_AST.win_size, step = config_DK_AST.step_size):\n",
    "              \n",
    "        print(\"....calculating expected num of spectogram for the file ->  \" +str(filename) + \"....\")\n",
    "        signal_8k, rate = torchaudio.load(filename , normalize = True)\n",
    "        #print(\"shape of signal_8k = \" +str(signal_8k.shape))\n",
    "        # resampling to 16 K as needed by AST\n",
    "        transform = T.Resample(orig_freq = 8000,new_freq = 16000)\n",
    "        signal_file = transform(signal_8k)\n",
    "        #print(\"Post Resampling signal_16k = \" +str(signal_file.shape))\n",
    "        # if the incoming training file is padded or a mosquito - transform it with a mosq sound\n",
    "        mel_spectrogram = T.MelSpectrogram(sample_rate=16000, n_fft=config_DK_AST.NFFT,win_length=config_DK_AST.win_size, \n",
    "                                                   hop_length=config_DK_AST.n_hop, center=True,\n",
    "                                                   pad_mode=\"reflect\", power=2.0, norm='slaney', onesided=True, n_mels=config_DK_AST.n_feat,\n",
    "                                                   mel_scale=\"htk\")\n",
    "        melspec = mel_spectrogram(signal_file)\n",
    "        db_obj= torchaudio.transforms.AmplitudeToDB(stype= 'power')\n",
    "        feat = db_obj(melspec)\n",
    "        spec_list = reshape_feat_pt_no_pad(feat,label)\n",
    "        return(spec_list)\n",
    "        \n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def num_specgrams(df, size = config_DK_AST.win_size, step = config_DK_AST.step_size):\n",
    "        list_spec_gram = [] \n",
    "        idx = 0\n",
    "        for row_idx_series in df.iterrows():\n",
    "            #print(\"row_idx_series...\" + str(row_idx_series))\n",
    "            idx+=1\n",
    "            #print(\"idx = \" + str(idx))\n",
    "            row = row_idx_series[1]\n",
    "            #print(\"row->\" + str(row))\n",
    "            label_duration = row['length']\n",
    "            print(\"......INSIDE num_specgrams.....\")\n",
    "            file_id = df.loc[idx,'id']\n",
    "            label_df = df.loc[idx,'sound_type']\n",
    "            filetag = df.loc[idx,'name']\n",
    "            _, file_format = os.path.splitext(filetag)\n",
    "            filename = os.path.join(data_dir, str(file_id) + file_format)\n",
    "            print(\"Filename = \" + str(filename))\n",
    "            \n",
    "            if label_duration < config_DK_AST.min_duration:\n",
    "                zero_vec = list(torch.zeros(1,config_DK_AST.n_feat , config_DK_AST.win_size))\n",
    "                list_spec_gram= list_spec_gram + zero_vec\n",
    "                continue\n",
    "            else:\n",
    "                signal_8k, rate = torchaudio.load(filename , normalize = True)\n",
    "                #print(\"shape of signal_8k = \" +str(signal_8k.shape))\n",
    "                # resampling to 16 K as needed by AST\n",
    "                transform = T.Resample(orig_freq = 8000,new_freq = 16000)\n",
    "                signal_file = transform(signal_8k)\n",
    "                #print(\"Post Resampling signal_16k = \" +str(signal_file.shape))\n",
    "                # if the incoming training file is padded or a mosquito - transform it with a mosq sound\n",
    "                \n",
    "                mel_spectrogram = T.MelSpectrogram(sample_rate=16000, n_fft=config_DK_AST.NFFT,win_length=config_DK_AST.win_size, \n",
    "                                                       hop_length=config_DK_AST.n_hop, center=True,\n",
    "                                                       pad_mode=\"reflect\", power=2.0, norm='slaney', onesided=True, n_mels=config_DK_AST.n_feat,\n",
    "                                                       mel_scale=\"htk\")\n",
    "                melspec = mel_spectrogram(signal_file)\n",
    "                db_obj= torchaudio.transforms.AmplitudeToDB(stype= 'power')\n",
    "                feat = db_obj(melspec)\n",
    "                #print(\"shape after POWER to DB in num_spec = \" + str(feat.shape))\n",
    "                #print(\"len pre reshape_feat_pt_no_pad =  \" + str(len(list_spec_gram)))\n",
    "                if feat.shape[2] < config_DK_AST.win_size:\n",
    "                    zero_vec = list(torch.zeros(1,config_DK_AST.n_feat , config_DK_AST.win_size))\n",
    "                    list_spec_gram= list_spec_gram + zero_vec\n",
    "                \n",
    "                else:\n",
    "                    test = reshape_feat_pt_no_pad(feat,label)\n",
    "                    print(test)\n",
    "                    list_spec_gram = list_spec_gram + reshape_feat_pt_no_pad(feat,label)\n",
    "                    #print(\"len post reshape_feat_pt_no_pad =  \" + str(len(list_spec_gram)))\n",
    "\n",
    "\n",
    "            return(len(list_spec_gram))\n",
    "    \n",
    "    \n",
    "    ### Function to process the wav file #########\n",
    "    @staticmethod\n",
    "    def process_wav (filename,label_duration,label,pad = False,filter_signal = call_filter ):\n",
    "        print(\"****** ENTERING PROCESS_WAV for file = \" + str(filename) + \"*****\")\n",
    "        #length = librosa.get_duration(filename = filename)\n",
    "        bugs = 0    \n",
    "        #to resamp to 16Khz\n",
    "        # The below condition check any mismatch in labels\n",
    "        #print(\"length from librosa = \" + str(length) )\n",
    "        #  assert math.isclose(length,label_duration, rel_tol=0.01), \"File: %s label duration (%.4f) does not match audio length (%.4f)\" % (row['path'], label_duration, length)\n",
    "        signal_8k, rate = torchaudio.load(filename , normalize = True)\n",
    "        #print(\"shape of signal_8k = \" +str(signal_8k.shape))\n",
    "        # resampling to 16 K as needed by AST\n",
    "        transform = T.Resample(orig_freq = 8000,new_freq = 16000)\n",
    "        signal_file = transform(signal_8k)\n",
    "        #print(\"Post Resampling signal_16k = \" +str(signal_file.shape))\n",
    "        # if the incoming training file is padded or a mosquito - transform it with a mosq sound\n",
    "        \n",
    "        if (label == 1) :\n",
    "            print(\"About to apply signal transform...\")\n",
    "            transformed_audio = compose_transform_aug_signal(signal_file)\n",
    "            #print(\"Filename->\" +str(filename) + \"shape of transformed audio = \" + str(transformed_audio.shape))\n",
    "        else:\n",
    "            print(\"About to apply background transform...\")\n",
    "            transformed_audio = compose_transform_aug_noise(signal_file)\n",
    "        \n",
    "        if pad == True :\n",
    "            # this indicates that the function is being called from padding and the file has to be appended\n",
    "                print(\"****** EXITING PROCESS_WAV (pad == TRUE)*****\")\n",
    "                return transformed_audio , label , None\n",
    "        else:#when pad == false\n",
    "            try:\n",
    "                mel_spectrogram = T.MelSpectrogram(sample_rate=16000, n_fft=config_DK_AST.NFFT,win_length=config_DK_AST.win_size, \n",
    "                                                   hop_length=config_DK_AST.n_hop, center=True,\n",
    "                                                   pad_mode=\"reflect\", power=2.0, norm='slaney', onesided=True, n_mels=config_DK_AST.n_feat,\n",
    "                                                   mel_scale=\"htk\")\n",
    "                melspec = mel_spectrogram(transformed_audio)\n",
    "                db_obj= torchaudio.transforms.AmplitudeToDB(stype= 'power')\n",
    "                feat = db_obj(melspec)\n",
    "                #print(\"After melspectrogram . Shape of feat ->\" + str(melspec.shape))\n",
    "                if feat.shape[2] < config_DK_AST.win_size:\n",
    "                    #print(\"Post Power_db. Shape = \" + str(feat.shape))\n",
    "                    print(\"RAISING VALUE ERROR..Power_db issue\")\n",
    "                    raise ValueError\n",
    "                else:\n",
    "                    feat_aug = AudioUtil.spec_augment(feat, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1)\n",
    "                    #print(\"post spec augment . Shape = \" + str(feat_aug.shape))\n",
    "                    bugs = None\n",
    "                    print(\"****** EXITING PROCESS_WAV*****\")\n",
    "                    return feat_aug ,label,bugs\n",
    "                        \n",
    "                        \n",
    "            except ValueError:\n",
    "                print(\"FILE SIZE TOOO SMALLLL ..error while processing wav..skipping..\")\n",
    "                bugs = filename\n",
    "                feat_aug = -1\n",
    "                y_temp = \"FAIL\"\n",
    "                print(\"****** EXITING PROCESS_WAV*****\")\n",
    "                return feat_aug, y_temp,bugs\n",
    "            \n",
    "        \n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16da35ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom collate function to accommodate variable size data\n",
    "def collate_data(spec_list):\n",
    "    print(\"inside Collate Test\")\n",
    "    print(\"len spec_list = \" + str(len(spec_list)))\n",
    "    print(\"Type spec_list = \" + str(type(spec_list)))\n",
    "    \n",
    "    for i in range(len(spec_list)):\n",
    "        print(\"i = \" + str(i))\n",
    "        print(\"*****\")\n",
    "        print(\"Type spec_list = \" + str(type(spec_list[i])))\n",
    "        \n",
    "        print(spec_list[i])\n",
    "    return(spec_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb9081a",
   "metadata": {},
   "source": [
    "## Pre Processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9decdde",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e1eab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundDS(Dataset):\n",
    "    \n",
    "    def __init__(self,df,max_dim = None ,sr = config_DK_AST.rate ):\n",
    "        self.df = df\n",
    "        self.sr = sr\n",
    "        self.channel = 1\n",
    "        #self.feat_list = len(final_feat_list)\n",
    "        \n",
    "               \n",
    "  \n",
    "  # Number of items in dataset\n",
    "  # \n",
    "    def __len__(self):\n",
    "        #all_spec_gram = AudioUtil.num_specgrams(self.df)\n",
    "        #print(\"total number of specgram for the dataset is = \" +str(all_spec_gram))\n",
    "        return len(self.df)\n",
    "    \n",
    "  # ----------------------------\n",
    "  # Get i'th item in dataset\n",
    "  # ----------------------------\n",
    "    def __getitem__(self, idx , win_size = config_DK_AST.win_size , step_size = config_DK_AST.step_size , min_duration = config_DK_AST.min_duration,sr = config_DK_AST.sr):\n",
    "        # Absolute file path of the audio file - concatenate the audio directory with\n",
    "        # the relative path\n",
    "        #get the file details from the idex passed from the train_loop\n",
    "        print(\"inside get_item for train . Idx = \" + str(idx))\n",
    "        file_id = self.df.loc[idx,'id']\n",
    "        label_df = self.df.loc[idx,'sound_type']\n",
    "        \n",
    "        # converting to numeric\n",
    "        if label_df == 'mosquito':\n",
    "            label  = 1\n",
    "        else:\n",
    "            label  = 0\n",
    "        #filetag is the \"name\"   column in the df\n",
    "        filetag = self.df.loc[idx,'name']\n",
    "        _, file_format = os.path.splitext(filetag)\n",
    "        filename = os.path.join(data_dir, str(file_id) + file_format)\n",
    "        label_duration = self.df.loc[idx,'length']\n",
    "        print(\"^^^^^^^^ Start processing filename = \" + str(filename) + \"^^^^^^^^\")\n",
    "        #length = librosa.get_duration(filename = filename)\n",
    "        #print(\"length from librosa = \" + str(length) )\n",
    "        \n",
    "        if label_duration >= min_duration:\n",
    "            #print(\"filename = \" + str(filename))\n",
    "            #call the function to process the sound file\n",
    "            feat_aug, y_temp , bugs = AudioUtil.process_wav (filename,label_duration,label,filter_signal = call_filter )\n",
    "            if (y_temp != \"FAIL\" ):\n",
    "                #print(\"File processeing successful . SHape being returned is   \" + str(feat_aug.shape))\n",
    "                expec_spec_list = AudioUtil.expected_specgrams(filename,label)\n",
    "                print(\"without any augmentation, we expect \" + str(len(expec_spec_list)) + \"spectograms\")\n",
    "                final_feat_list = reshape_feat_pt_no_pad(feat_aug,label)\n",
    "                #print(\"final_feat_list length = \" + str(len(final_feat_list)))\n",
    "                if (len(expec_spec_list)== len(final_feat_list)):\n",
    "                    return final_feat_list\n",
    "                    \n",
    "                else:\n",
    "                    print(\" mismatch in number of spectograms ->expected and augmented \")\n",
    "                    print(\"len(expec_spec_list) =\" +  str(len(expec_spec_list)))\n",
    "                    print(\"len(final_feat_list) =\" +  str(len(final_feat_list)))\n",
    "                    #now we'll select the min number of elements from the list to avoid run time errors\n",
    "                    trunc_feat = min(len(final_feat_list) , len(expec_spec_list))\n",
    "                    print(\"trunc_feat = \" + str(trunc_feat))\n",
    "                    final_feat_list = final_feat_list[0:trunc_feat]\n",
    "                    return final_feat_list\n",
    "                \n",
    "            #when process_wav has processing issues    \n",
    "            else:\n",
    "                print(\"a failure has occured during processing, return a tensor of zeros\")\n",
    "                feat_aug = torch.zeros(1, config_DK_AST.win_size,config_DK_AST.n_feat)\n",
    "                #print(\"Shape being returned from process_wav = \" + str(feat_aug.shape))\n",
    "                #final_feat_list = reshape_feat_pt_no_pad(feat_aug,max_length = max_length)\n",
    "                #print(\"final_feat_list length , returning from reshape = \" + str(len(final_feat_list)))\n",
    "                tup = (list(feat_aug),999)\n",
    "                return tup\n",
    "                \n",
    "                \n",
    "        \n",
    "        #when there is padding needed and         \n",
    "        else:\n",
    "            print(\"inside shorter files\")\n",
    "            size_to_pad = min_duration - label_duration \n",
    "            #print(\"size_to_pad = \" + str(size_to_pad))\n",
    "            #calling process_wav first to ensure that the 'signal' is centered\n",
    "            feat, y_temp , bugs = AudioUtil.process_wav (filename,label_duration,label,pad = True,filter_signal = call_filter )\n",
    "            #print(\"shape of returned file from process_wav -> \" + str(feat.shape))\n",
    "            # adding tolerance because the randomization from augmentation sometimes reduces the file size < window size.\n",
    "            # This causes downstream problem in reshape\n",
    "            size = config_DK_AST.rate*(size_to_pad + .25*label_duration) + feat.shape[1]\n",
    "            #print(\"size with which the file will be appende -> \" + str(size))\n",
    "            #print(\"about to apply padding\")\n",
    "            feat_padded = librosa.util.pad_center(feat, size=size , axis = 1,mode = 'mean')\n",
    "            #print(\"shape of padded file = \" + str(feat_padded.shape) + \"and its type = \" + str(type(feat_padded)))\n",
    "            #convert to mel scale\n",
    "            mel_spectrogram = T.MelSpectrogram(sample_rate=16000, n_fft=config_DK_AST.NFFT,win_length=config_DK_AST.win_size, \n",
    "                                                       hop_length=config_DK_AST.n_hop, center=True,\n",
    "                                                       pad_mode=\"reflect\", power=2.0, norm='slaney', onesided=True, n_mels=config_DK_AST.n_feat,\n",
    "                                                       mel_scale=\"htk\")\n",
    "            \n",
    "            melspec_temp = mel_spectrogram(torch.tensor(feat_padded))\n",
    "            #print(\"After melspectrogram . Shape of feat ->\" + str(melspec_temp.shape))\n",
    "            try:\n",
    "                db_obj= torchaudio.transforms.AmplitudeToDB(stype= 'power')\n",
    "                feat = db_obj(melspec_temp)\n",
    "                #print(\"after converting to mel scale , shape of feat = \" + str(feat.shape))\n",
    "                #print(\" feat[2] =\" + str(feat.shape[2]))\n",
    "                    \n",
    "                if feat.shape[2] < config_DK_AST.win_size :\n",
    "                    #print(\"After power_to_db . Shape of feat ->\" + str(feat.shape))\n",
    "                    raise ValueError\n",
    "                        \n",
    "                else:\n",
    "                    feat_aug = AudioUtil.spec_augment(feat, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1)\n",
    "                    #print(\"POst spec_augment ...for filename = \" + str(filename) + \"shape of feat_aug = \" + str(feat_aug.shape))\n",
    "                    #librosa.display.specshow(feat)\n",
    "                    \n",
    "                    #when the file is shorter than min duration then we'd return only 1 spectogram\n",
    "                    expec_spec_list = [1]\n",
    "                    print(\"without any augmentation, we expect \" + str(len(expec_spec_list)) + \"spectograms\")\n",
    "                    final_feat_list = reshape_feat_pt_no_pad(feat_aug,label)\n",
    "                    #print(\"final_feat_list length = \" + str(len(final_feat_list)))\n",
    "                    if (len(expec_spec_list)== len(final_feat_list)):\n",
    "                        return final_feat_list\n",
    "                        \n",
    "                    else:\n",
    "                        print(\" mismatch in number of spectograms ->expected and augmented \")\n",
    "                        print(\"len(expec_spec_list) =\" +  str(len(expec_spec_list)))\n",
    "                        print(\"len(final_feat_list) =\" +  str(len(final_feat_list)))\n",
    "                        #now we'll select the min number of elements from the list to avoid run time errors\n",
    "                        trunc_feat = min(len(final_feat_list) , len(expec_spec_list))\n",
    "                        print(\"trunc_feat = \" + str(trunc_feat))\n",
    "                        final_feat_list = final_feat_list[0:trunc_feat]\n",
    "                        return final_feat_list\n",
    "            except ValueError:\n",
    "                print(\"!!!!ValueError!!!!!!for filename \" + str(filename) + \"FILE SIZE < Win size \")\n",
    "                final_feat_list = torch.zeros(1, config_DK_AST.win_size,config_DK_AST.n_feat)\n",
    "                #print(\"final_feat_list length , returning from reshape = \" + str(len(final_feat_list)))\n",
    "                final_feat_list = (list(final_feat_list),999)\n",
    "                return final_feat_list\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875a4df3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "178e9241",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundDS_test(Dataset):\n",
    "    \n",
    "    def __init__(self,df ,sr = config_DK_AST.rate ):\n",
    "        self.df = df\n",
    "        self.sr = sr\n",
    "        self.channel = 1\n",
    "              \n",
    "  # \n",
    "  # Number of items in dataset\n",
    "  # \n",
    "    def __len__(self):\n",
    "        #all_spec_gram = AudioUtil.num_specgrams(self.df)\n",
    "        #print(\"total number of specgram for the dataset is = \" +str(all_spec_gram))\n",
    "        return len(self.df)\n",
    "    \n",
    "  # ----------------------------\n",
    "  # Get i'th item in dataset\n",
    "  # ----------------------------\n",
    "    def __getitem__(self, idx , win_size = config_DK_AST.win_size , step_size = config_DK_AST.step_size , min_duration = config_DK_AST.min_duration,sr = config_DK_AST.sr):\n",
    "        # Absolute file path of the audio file - concatenate the audio directory with\n",
    "        # the relative path\n",
    "        print(\"inside get_item for val/test. Idx = \" +str(idx))\n",
    "        file_id = self.df.loc[idx,'id']\n",
    "        label_df = self.df.loc[idx,'sound_type']\n",
    "        # converting to numeric\n",
    "        if label_df == 'mosquito':\n",
    "            label  = 1\n",
    "        else:\n",
    "            label  = 0\n",
    "           \n",
    "        filetag = self.df.loc[idx,'name']\n",
    "        _, file_format = os.path.splitext(filetag)\n",
    "        filename = os.path.join(data_dir, str(file_id) + file_format)\n",
    "        label_duration = self.df.loc[idx,'length']\n",
    "        print(\"Test File :- filename = \" + str(filename))\n",
    "        length = librosa.get_duration(filename = filename)\n",
    "        if label_duration > min_duration:\n",
    "            #print(\"AUdio duration of File \" + str(filename) + \"is more than min duration\")\n",
    "            signal_8k, rate = torchaudio.load(filename , normalize = True)\n",
    "            #print(\"Signal 8K shape = \" + str(signal_8k.shape))\n",
    "            # resampling to 16 K as needed by AST\n",
    "            transform = T.Resample(orig_freq = 8000,new_freq = 16000)\n",
    "            signal_file = transform(signal_8k)\n",
    "            #print(\"Signal 16K shape = \" + str(signal_file.shape))\n",
    "            #converting to mel scale\n",
    "            mel_spectrogram = T.MelSpectrogram(sample_rate=16000, n_fft=config_DK_AST.NFFT,win_length=config_DK_AST.win_size, \n",
    "                                                   hop_length=config_DK_AST.n_hop, center=True,\n",
    "                                                   pad_mode=\"reflect\", power=2.0, norm='slaney', onesided=True, n_mels=config_DK_AST.n_feat,\n",
    "                                                   mel_scale=\"htk\")\n",
    "            melspec = mel_spectrogram(signal_file)\n",
    "            db_obj= torchaudio.transforms.AmplitudeToDB(stype= 'power')\n",
    "            feat = db_obj(melspec)\n",
    "            #print(\"After melspectrogram . Shape of feat ->\" + str(melspec.shape))\n",
    "            #feat = librosa.power_to_db(feat, ref=np.max)\n",
    "            #print(\"After power_to_db . Shape of feat ->\" + str(feat.shape))\n",
    "                            \n",
    "            # split into winsize x labels\n",
    "            if feat.shape[2] < config_DK_AST.win_size:\n",
    "                print(\"File->\" + str(filename) + \"is smaller than win size. Returning a 999 label\")\n",
    "                temp_feat = torch.zeros(1, config_DK_AST.win_size,config_DK_AST.n_feat)\n",
    "                temp_feat = (list(temp_feat),999)\n",
    "                return temp_feat\n",
    "                \n",
    "            else:\n",
    "                test_feat_list = reshape_feat_pt_no_pad(feat, label,size = config_DK_AST.win_size, step = config_DK_AST.step_size )\n",
    "                #print(\"length of list from reshape = \" + str(len(test_feat_list)))\n",
    "                print(\"file \" + str(filename) + \"will have \" + str(len(test_feat_list)) + \" specgrams\")\n",
    "                expec_spec_list_val = AudioUtil.expected_specgrams(filename)\n",
    "                #print(\"val returned from AudioUtil.expected_specgrams = \" +str(len(expec_spec_list_val)))\n",
    "                if (len(expec_spec_list)== len(test_feat_list)):\n",
    "                        return test_feat_list\n",
    "                        \n",
    "                else:\n",
    "                    print(\" mismatch in number of spectograms ->expected and augmented \")\n",
    "                    print(\"len(expec_spec_list) =\" +  str(len(expec_spec_list)))\n",
    "                    print(\"len(test_feat_list) =\" +  str(len(test_feat_list)))\n",
    "                    #now we'll select the min number of elements from the list to avoid run time errors\n",
    "                    trunc_feat = min(len(test_feat_list) , len(expec_spec_list))\n",
    "                    print(\"trunc_feat = \" + str(trunc_feat))\n",
    "                    test_feat_list = test_feat_list[0:trunc_feat]\n",
    "                    return test_feat_list\n",
    "            # if the incoming test file is shorter then we need to append/pad \n",
    "        else:            \n",
    "            #print(\"Audio duration of File \" + str(filename) + \"is less than min duration\")\n",
    "            signal_8k, rate = torchaudio.load(filename , normalize = True)\n",
    "            # resampling to 16 K as needed by AST\n",
    "            transform = T.Resample(orig_freq = 8000,new_freq = 16000)\n",
    "            signal_file = transform(signal_8k)\n",
    "            #print(\"shape of signal file = \" + str(signal_file.shape))\n",
    "            num_rows, sig_len = signal_file.shape\n",
    "            # the code below is to pad with 0\n",
    "            len_begin = int(round(min_duration*1000 - label_duration*1000 ))\n",
    "            pad_begin_len = random.randint(0,len_begin)\n",
    "            #print(\"pad_begin_len = \" + str(pad_begin_len))\n",
    "            pad_end_len = (min_duration*1000 - label_duration*1000 - pad_begin_len)*(config_DK_AST.sr/1000)\n",
    "            #print(\"pad_end_len = \" + str(pad_end_len))\n",
    "            # Pad with 0s\n",
    "            pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "            #print(\"pad_begin = \" +str(pad_begin))\n",
    "            pad_end = torch.zeros((num_rows, int(pad_end_len)))\n",
    "            #print(\"pad_end = \" +str(pad_end))\n",
    "            signal_appended = torch.cat((pad_begin, signal_file, pad_end), 1)\n",
    "            #print(\"signal_appended shape = \" + str(signal_appended.shape))\n",
    "            \n",
    "            # now covert to mel scale\n",
    "            mel_spectrogram = T.MelSpectrogram(sample_rate=16000, n_fft=config_DK_AST.NFFT,win_length=config_DK_AST.win_size, \n",
    "                                                   hop_length=config_DK_AST.n_hop, center=True,\n",
    "                                                   pad_mode=\"reflect\", power=2.0, norm='slaney', onesided=True, n_mels=config_DK_AST.n_feat,\n",
    "                                                   mel_scale=\"htk\")\n",
    "            melspec = mel_spectrogram(signal_appended)\n",
    "            db_obj= torchaudio.transforms.AmplitudeToDB(stype= 'power')\n",
    "            feat = db_obj(melspec)\n",
    "            \n",
    "            try:\n",
    "                if feat.shape[2] < config_DK_AST.win_size:\n",
    "                    #print(\"After melspectrogram . Shape of feat ->\" + str(melspec.shape))\n",
    "                    #print(\"File->\" + str(filename) + \"being skipped because its < config_size\")\n",
    "                    #print return with a 999 label indicating that this is an invalid input\n",
    "                    raise ValueError                 \n",
    "                else:\n",
    "                    #print(\"After power_to_db . Shape of feat ->\" + str(feat.shape))\n",
    "                    test_feat_list = reshape_feat_pt_no_pad(feat, label,size = config_DK_AST.win_size, step = config_DK_AST.step_size )\n",
    "                    #print(\"file \" + str(filename) + \"will have \" + str(len(test_feat_list)) + \" specgrams\")\n",
    "                    tup = test_feat_list[0:1]\n",
    "                    return tup\n",
    "                    \n",
    "                    \n",
    "            except ValueError:\n",
    "                print(\" ValueError .for filename \" + str(filename) + \"FILE SIZE < Win size. Returning 999 \")\n",
    "                test_feat_list = torch.zeros(1, config_DK_AST.win_size,config_DK_AST.n_feat)\n",
    "                #test_feat_list = reshape_feat_pt_no_pad(feat, size = config_DK_AST.win_size, step = config_DK_AST.step_size,max_length = None )\n",
    "                test_feat_list = (list(test_feat_list),999)\n",
    "                return test_feat_list\n",
    "                \n",
    "                \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "123c58ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------AST Model Summary---------------\n",
      "ImageNet pretraining: True, AudioSet pretraining: False\n",
      "frequncey stride=10, time stride=10\n",
      "number of patches=84\n"
     ]
    }
   ],
   "source": [
    "def train_model_ast(train_dl, val_dl = None, model = ASTModel()):\n",
    "    # X-train is a list of tensor and y_train is a list\n",
    "    # we create a tuple of X-train and labels . This will help us to create a validation dataset.\n",
    "              \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Training on {device}')\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimiser = optim.Adam(model.parameters(), lr=config_pytorch.lr)\n",
    "    #print(\"Optim Device= \" +str(optimiser.device))\n",
    "    all_train_loss = []\n",
    "    all_train_acc = []\n",
    "    all_val_loss = []\n",
    "    all_val_acc = []\n",
    "    best_val_loss = np.inf\n",
    "    best_val_acc = -np.inf\n",
    "    best_train_acc = -np.inf\n",
    "    best_epoch = -1\n",
    "    checkpoint_name = None\n",
    "    overrun_counter = 0\n",
    "    for e in range(config_pytorch.epochs):\n",
    "        start_epoch_time = time.time()\n",
    "        print(\"epoch = \" +str(e))\n",
    "        train_loss = 0.0\n",
    "        model.to(device).train()\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        total_sgrams = 0\n",
    "        #our data is a list of dim = max_dims\n",
    "        for idx,(spec_list,label) in enumerate(train_dl):\n",
    "            start_time = time.time()\n",
    "            print(\"TRain_dl length is \" + str(len(train_dl)))\n",
    "            print(\"idx in train_ast = \" +str(idx))\n",
    "            #print(\"class type of tupe = \" +str(tupe))\n",
    "            print(\"label = \" +str(label))\n",
    "            print(\"(inside train_ast) = len of of spec_list = \" + str(len(spec_list)))\n",
    "            total_sgrams+=int(len(spec_list))\n",
    "            for i in range(len(spec_list)):\n",
    "                print(\"index of spec_list = \" + str(i))\n",
    "                y = torch.tensor(label).reshape(-1,1).to(device).float()\n",
    "                if y == 999:\n",
    "                    print(\"inside y = 999\")\n",
    "                    print(\"Failure occured for file and skipping this rec\")\n",
    "                    all_y.append(y.cpu().detach())\n",
    "                    total_sgrams-=1\n",
    "                    break\n",
    "                else:           \n",
    "                    print(\"%%%%%% INSIDE TRAIN LOOP(when there is no 999) TO GET A LOSS%%%%%%%%\")\n",
    "                    print(\"index of spec_list = \" + str(i))\n",
    "                    x_temp = spec_list[i]\n",
    "                    print(\"x_temp SHAPE = \" + str(x_temp.shape))\n",
    "                    #print(\"y SHAPE = \" + str(y.shape))\n",
    "                    # a zero tensor indicates a padding\n",
    "                    #bat_size = x_temp.shape[1]\n",
    "                    #time_dim = x_temp.shape[2]\n",
    "                    #freq_dim = x_temp.shape[3]\n",
    "                    #x_reshaped = x_temp.reshape(bat_size,time_dim,freq_dim)\n",
    "                    #print(\"shape of x_reshaped in train = \" +str(x_reshaped.shape))\n",
    "                    #x_reshaped = x_temp.squeeze()\n",
    "                    # this is recommended in paper.\n",
    "                    #print(x_reshaped.shape)\n",
    "                    x_temp.cuda()\n",
    "                    #print(\"X_REshaped device = \" +str(x_reshaped.device))\n",
    "                    y.cuda()\n",
    "                    #print(\"Y device = \" +str(y.device))\n",
    "                    y_pred = model(x_temp.to(device).detach()).float()\n",
    "                    #print(\"Y Pred Device = \" +str(y_pred.device))\n",
    "                    #print(\"Y  shape = \" +str(y.shape))\n",
    "                    #print(\"i = \" + str(i))\n",
    "                    print(\"@@@@Y_pred = >\" + str(y_pred))\n",
    "                    #print(\"sigmoid of Y_pred = \" +str(m(y_pred)))\n",
    "                    loss = criterion(y_pred, y)\n",
    "                    #print(\"loss  = \" +str(loss.device))\n",
    "                    optimiser.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimiser.step()\n",
    "                    #print(\"Current train loss before adding the loss = \" + str(train_loss))\n",
    "                    train_loss += loss.detach().item()\n",
    "                    #print(\"train loss - >\" + str(loss.item()))\n",
    "                    #print(\"train loss/len(train_loader)-> \" + str(train_loss/len(train_loader)))\n",
    "                    all_y.append(y.cpu().detach())\n",
    "                    all_y_pred.append(y_pred.cpu().detach())\n",
    "                    # if bat_size%100 == 0 :\n",
    "                     #print(\"Inside Epoch \" + str(e) + \" & inside batch \" + str(idx) + \"specgram \" + str(i) + \" of \" + str(len(tup_list))  )\n",
    "                    del x_temp\n",
    "                    del y\n",
    "                    del y_pred\n",
    "                    \n",
    "                    #print(\"Finished training a  batch\")\n",
    "                    finish_time = time.time()\n",
    "                    duration = (finish_time - start_time )/60\n",
    "                    if duration//120 > 1 :\n",
    "                        print(\"Duration of training this file = \" + str(duration)) \n",
    "                #total_sgrams = len(train_dl) + item_in_bat\n",
    "                #print(\"total_sgrams = \" + str(total_sgrams))\n",
    "                    all_train_loss.append(train_loss/total_sgrams)    \n",
    "            #removing all instances of 999\n",
    "        \n",
    "        \n",
    "        all_y_999_removed = all_y\n",
    "        while 999 in all_y_999_removed: all_y_999_removed.remove(999)\n",
    "        all_y_999_removed = torch.cat(all_y_999_removed)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        train_acc = accuracy_score(all_y_999_removed.numpy(), (all_y_pred.numpy() > 0.5).astype(float))\n",
    "        all_train_acc.append(train_acc)\n",
    "        \n",
    "        # Can add more conditions to support loss instead of accuracy. Use *-1 for loss inequality instead of acc\n",
    "        if val_dl is not None:\n",
    "            val_loss, val_acc,total_val_size,invalid_file_flag = test_model(model, val_dl, criterion, 0.5, device=device)\n",
    "            #when the val_data returns np.inf as loss and accuracy\n",
    "            if invalid_file_flag == True:\n",
    "                acc_metric = train_acc\n",
    "                best_acc_metric = best_train_acc\n",
    "            else:\n",
    "                all_val_loss.append(val_loss)\n",
    "                all_val_acc.append(val_acc)\n",
    "                acc_metric = val_acc\n",
    "                best_acc_metric = best_val_acc\n",
    "        else:\n",
    "            acc_metric = train_acc\n",
    "            best_acc_metric = best_train_acc\n",
    "        end_epoch_time = time.time()\n",
    "        duration_epoch = (end_epoch_time -start_epoch_time )/60\n",
    "        print(\"duration to train epoch \" + str(e) + \"= \" + str(duration_epoch))\n",
    "        if acc_metric > best_acc_metric:\n",
    "            checkpoint_name = f'model_e{e}_{datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")}.pth'\n",
    "            torch.save(model.state_dict(), os.path.join(config_DK_AST.model_dir, 'pytorch', checkpoint_name))\n",
    "            print('Saving model to:', os.path.join(config_DK_AST.model_dir, 'pytorch', checkpoint_name)) \n",
    "            best_epoch = e\n",
    "            best_train_acc = train_acc\n",
    "            best_train_loss = train_loss\n",
    "            best_val_acc = val_acc\n",
    "            best_val_loss = val_loss\n",
    "            overrun_counter = -1\n",
    "\n",
    "        overrun_counter += 1\n",
    "        if (val_dl is not None) and (invalid_file_flag == False):\n",
    "            print('Epoch: %d, Train Loss: %.8f, Train Acc: %.8f, Val Loss: %.8f, Val Acc: %.8f, overrun_counter %i' % (e, train_loss/(total_sgrams), train_acc, val_loss/(total_val_size), val_acc,  overrun_counter))\n",
    "        else:\n",
    "            print('Epoch: %d, Train Loss: %.8f, Train Acc: %.8f, overrun_counter %i' % (e, train_loss/(total_sgrams), train_acc, overrun_counter))\n",
    "        if overrun_counter > config_pytorch.max_overrun:\n",
    "            break\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8195bf34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d82604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4e8374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a59f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f29aa79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1d04e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "830aa024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_df):\n",
    "    \n",
    "    test_obj = SoundDS_test(test_df)\n",
    "    test_dl = torch.utils.data.DataLoader(test_df, batch_size=16, shuffle=False)\n",
    "    n_samples = len(test_df)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Evaluating on {device}')\n",
    "    print(\"n_samples = \" +str(n_samples))\n",
    "\n",
    "    #x_test,label = torch.tensor(X_test).float()\n",
    "                \n",
    "    y_preds_all = np.zeros([n_samples, len(y_test), 2])\n",
    "    #print(\"Shape of y_preds_all = \" + str(y_preds_all.shape))\n",
    "    model.eval() # Important to not leak info from batch norm layers and cause other issues\n",
    "\n",
    "    for n in range(n_samples):\n",
    "        all_y_pred = []\n",
    "        all_y = []\n",
    "        \n",
    "        for idx,(x, labels ) in enumerate(test_dl):\n",
    "            # The model needs input in the form of(batchsize, time, freq . Reshaping the features below)           \n",
    "            print(\"bat_idx = \" + str(idx))\n",
    "            feat_temp , labels  =  x, labels\n",
    "            #print(\"shape of feat in the batch = \" + str(feat_temp.shape))\n",
    "            #print(\"Calling reshape ....\")\n",
    "            feat_list =  reshape_feat_pt_temp(feat_temp, labels, size = config_DK_AST.win_size, step = config_DK_AST.step_size)\n",
    "            #This is of the form [1,var,80,128]\n",
    "            #print(\"Batch#->\" + str(idx) + \"will have \" + str(len(feat_list)) + \" elements\")          \n",
    "            one_image_loss = 0\n",
    "            for i in range(0,len(feat_list)):\n",
    "                x_temp,y_elem = feat_list[i]\n",
    "                bat_size = x_temp.shape[0]\n",
    "                time_dim = x_temp.shape[1]\n",
    "                freq_dim = x_temp.shape[2]\n",
    "                                \n",
    "                #print(\"X-temp Shape = \" +str(x_temp.shape))\n",
    "                #x_reshaped = x_temp.reshape(bat_size,time_dim,freq_dim)\n",
    "                \n",
    "            # this is recommended in paper.\n",
    "            #x_reshaped = ((x_reshaped - train_data_mean)/(train_data_sd)).cuda()\n",
    "                y = y_elem.reshape(-1,1).to(device).float()\n",
    "                #print(\"Lables = \" +str(y))\n",
    "                #print(\"Type of Y = \" + str(type(y)))\n",
    "                #print(\"Unsqueeze Y = \" + str(y))\n",
    "                #print(x_reshaped.shape)\n",
    "                optimiser.zero_grad()\n",
    "                x_reshaped.cuda()\n",
    "                #print(\"X_REshaped device = \" +str(x_reshaped.device))\n",
    "                y.cuda()\n",
    "            #print(\"Y device = \" +str(y.device))\n",
    "                y_pred = model(x_reshaped.to(device).detach()).float()\n",
    "                all_y.append(y.cpu().detach())\n",
    "                all_y_pred.append(y_pred.cpu().detach())\n",
    "                del x\n",
    "                del y\n",
    "                del y_pred\n",
    "                all_y_pred = torch.cat(all_y_pred)\n",
    "                all_y = torch.cat(all_y)\n",
    "\n",
    "        y_preds_all[n,:,1] = np.array(all_y_pred)\n",
    "        y_preds_all[n,:,0] = 1-np.array(all_y_pred) # Check ordering of classes (yes/no)\n",
    "        test_acc = accuracy_score(all_y.numpy(), (all_y_pred.numpy() > 0.5).astype(float))\n",
    "        # print(test_acc)\n",
    "    return y_preds_all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bcd9095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_aggregated(model, X_test, y_test, n_samples):\n",
    "    ''' Generate predictions for VGGish features (Feat. A) rescaled to time window of 1.92 second features (Feat. B)'''\n",
    "    preds_aggregated_by_mean = []\n",
    "    y_aggregated_prediction_by_mean = []\n",
    "    y_target_aggregated = []\n",
    "    \n",
    "    for idx, recording in enumerate(X_test):\n",
    "        n_target_windows = len(recording)//2  # Calculate expected length: discard edge\n",
    "        y_target = np.repeat(y_test[idx],n_target_windows) # Create y array of correct length\n",
    "        preds = evaluate_model(model, recording, np.repeat(y_test[idx],len(recording)),n_samples) # Sample BNN\n",
    "#         preds = np.mean(preds, axis=0) # Average across BNN samples\n",
    "#         print(np.shape(preds))\n",
    "        preds = preds[:,:n_target_windows*2,:] # Discard edge case\n",
    "#         print(np.shape(preds))\n",
    "#         print('reshaping')\n",
    "        preds = np.mean(preds.reshape(len(preds),-1,2,2), axis=2) # Average every 2 elements, keep samples in first dim\n",
    "#         print(np.shape(preds))\n",
    "        preds_y = np.argmax(preds)  # Append argmax prediction (label output)\n",
    "        y_aggregated_prediction_by_mean.append(preds_y)\n",
    "        preds_aggregated_by_mean.append(preds)  # Append prob (or log-prob/other space)\n",
    "        y_target_aggregated.append(y_target)  # Append y_target\n",
    "#     return preds_aggregated_by_mean, y_aggregated_prediction_by_mean, y_target_aggregated\n",
    "    return np.hstack(preds_aggregated_by_mean), np.concatenate(y_target_aggregated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ab78126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_feat_pt_no_pad(feats,label,size = config_DK_AST.win_size, step = config_DK_AST.step_size):\n",
    "    #tup_list will hold the values in the form of an image(x,label)\n",
    "    print(\"&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\")\n",
    "    print(\"initial shape of feat = \" + str(feats.shape))\n",
    "    feat_list = []\n",
    "    feat_first_tpose = torch.transpose(feats,1,2)\n",
    "    print(\"shape after first transpose = \" + str(feat_first_tpose.shape))\n",
    "    feat_new = feat_first_tpose.unfold(1,size  = config_DK_AST.win_size, step = config_DK_AST.step_size)\n",
    "    print(\"shape after unfolding = \" + str(feat_new.shape))\n",
    "    feat_squee = feat_new.squeeze()\n",
    "    print(\"shape post squeeze = \" + str(feat_squee.shape))\n",
    "    len_squeeze = list(feat_squee.shape)\n",
    "    if len(len_squeeze) == 2:\n",
    "        if feat_squee.shape[1] < config_DK_AST.win_size :\n",
    "            # 999 indicated error condition\n",
    "            print(\"inside feat_squee.shape[1] < config_DK_AST.win_size\")\n",
    "            feature_zero = list(torch.zeros(1,config_DK_AST.win_size,config_DK_AST.n_feat))\n",
    "            print(\"returning zeros and 999\")\n",
    "            tup = (feature_zero,999)\n",
    "            feat_list.append(tup)\n",
    "            print(\"returning a feat_list with elements = \" + str(len(feat_list)))\n",
    "            return feat_list\n",
    "            \n",
    "        else:\n",
    "            print(\"already in desired shape\")\n",
    "            t = torch.transpose(feat_squee,1,0)\n",
    "            t = torch.unsqueeze(t,dim = 0)            \n",
    "            print(\"shape of feat = \" + str(t.shape))\n",
    "            feature = list(t)\n",
    "            tup = (feature,label)\n",
    "            print(\"returning a feat_list with elements = \" + str(len(feat_list)))\n",
    "            feat_list.append(tup)\n",
    "            return feat_list\n",
    "              \n",
    "    else:\n",
    "        num_chunk = feat_squee.shape[0]\n",
    "        print(\"Num chunks to be generated = \" + str(num_chunk))\n",
    "        chunk_list = torch.chunk(feat_squee , num_chunk , 0)\n",
    "        print(\"One sound file being split into  \" + str(len(chunk_list)))\n",
    "        #print(\"LEngth of list of tensors = \" + str(len(chunk_list)))\n",
    "        \n",
    "        for s_gram in (chunk_list):\n",
    "            print(\"sgram shape =\" + str(torch.transpose(s_gram,2,1).shape))\n",
    "            #print(\"ctr =\" +str(ctr))\n",
    "            feature = torch.transpose(s_gram,2,1)\n",
    "            #print(\"before appending feat_list = \" + str(len(feat_list)))\n",
    "            tup = (list(feature),label)\n",
    "            feat_list.append(tup)\n",
    "            #print(\"POST appending feat_list = \" + str(len(feat_list)))\n",
    "            \n",
    "        print(\"returning a feat_list with elements = \" + str(len(feat_list)))\n",
    "        return feat_list\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cb0b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f392ddf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eeeaed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5dcb77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, val_dl, criterion, class_threshold=0.5, device=None):\n",
    "    \n",
    "        test_loss = 0.0\n",
    "        model.eval()\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        with torch.no_grad():\n",
    "            total_sgram_val = 0\n",
    "            print(\"^^^^^ INSIDE VAL DATA^^^^^^^^^\")\n",
    "            for idx,(tupe_val,label_val) in enumerate(val_dl):\n",
    "                start_time = time.time()\n",
    "                print(\"Inside test_model->\" + \"idx = \" +str(idx))\n",
    "                #print(idx,val_dl[idx])\n",
    "                #print(\"len of of X_el_val = \" + str(len(x_el_val)))\n",
    "                spec_val,label_val = tupe_val[idx]\n",
    "                total_sgram_val+=int(len(spec_val))\n",
    "                #print(\"Num items in this list = \" + str(len(x_el_val)))\n",
    "                label_val_t = torch.tensor(label_val)\n",
    "                for i in range(len(spec_val)):\n",
    "                    #get the i-th dim \n",
    "                    y= label_val_t.reshape(-1,1).to(device).float()\n",
    "                    if y == 999 :\n",
    "                        #print(\"y value = \" + str(y))\n",
    "                        print(\"999 encountered .Error file..skipping\")\n",
    "                        all_y.append(y.cpu().detach())\n",
    "                        print(\"Length of all_y = \" +str(len(all_y)))\n",
    "                        total_sgram_val -=1\n",
    "                        continue\n",
    "                    else:\n",
    "                        x_temp_val = spec_val[i]\n",
    "                        #print(\"x_temp SHAPE = \" + str(x_temp_val.shape))\n",
    "                        #print(\"y SHAPE = \" + str(y.shape))\n",
    "                        # a zero tensor indicates a padding\n",
    "                        #bat_size = x_temp.shape[1]\n",
    "                        #time_dim = x_temp.shape[2]\n",
    "                        #freq_dim = x_temp.shape[3]\n",
    "                        #x_reshaped = x_temp.reshape(bat_size,time_dim,freq_dim)\n",
    "                        #print(\"X_REshaped shape = \" +str(x_reshaped.shape))\n",
    "                        x_temp_val.to(device)\n",
    "                        #print(\"X_REshaped device = \" +str(x_reshaped.device))\n",
    "                        y.to(device)\n",
    "                        #print(\"Y device = \" +str(y.device))\n",
    "                        y_pred = model(x_temp_val.to(device).detach()).float()\n",
    "                        #print(\"Y Pred Device = \" +str(y_pred.device))\n",
    "                        #print(\"Y Pred shape = \" +str(y_pred.shape))\n",
    "                        #print(\"Y  shape = \" +str(y.shape))\n",
    "                        #print(\"@@@@Y_pred in val= >\" + str(y_pred))\n",
    "                        loss = criterion(y_pred, y)\n",
    "                        test_loss += loss.detach().item()\n",
    "                        #print(\"loss  = \" +str(loss.device))\n",
    "                        #print(\"train loss/len(train_loader)-> \" + str(train_loss/len(train_loader)))\n",
    "                        all_y.append(y.cpu().detach())\n",
    "                        all_y_pred.append(y_pred.cpu().detach())\n",
    "                        #if bat_size%100 == 0 :\n",
    "                        #    print(\"Inside Epoch \" + str(e) + \" & inside batch \" + str(idx) + \"specgram \" + str(i) + \" of \" + str(len(item_in_bat))  )\n",
    "                        del x_temp_val\n",
    "                        del y\n",
    "                        del y_pred\n",
    "                        \n",
    "\n",
    "                #removing instances of999\n",
    "            \n",
    "            all_y_999_removed = all_y\n",
    "            while 999 in all_y_999_removed: all_y_999_removed.remove(999)\n",
    "            #print(\"all_y_999_removed = \" + str(all_y_999_removed))\n",
    "            #print(\"all_y = \" + str(all_y))             \n",
    "            #all_y = torch.cat(all_y)\n",
    "            #all_y_999_removed = torch.cat(all_y_999_removed)\n",
    "            #all_y_pred = torch.cat(all_y_pred)\n",
    "            \n",
    "            if(len(all_y_999_removed) == 0):\n",
    "                \n",
    "                test_loss = np.inf\n",
    "                test_acc = -np.inf\n",
    "                total_val_size = 1\n",
    "                invalid_file_flag = True\n",
    "                return test_loss, test_acc,total_val_size,invalid_file_flag\n",
    "                \n",
    "                \n",
    "                \n",
    "            else:\n",
    "                test_loss = test_loss/(total_sgram_val)\n",
    "                all_y_999_removed = torch.cat(all_y_999_removed)\n",
    "                all_y_pred = torch.cat(all_y_pred)\n",
    "                #test_acc = accuracy_score(all_y_999_removed.numpy(), (all_y_pred.numpy() > class_threshold).astype(float))\n",
    "                #print(\"all_y_999_removed = \" + str(np.array(all_y_999_removed)))\n",
    "                #print(\"all_y_pred = \" + str(np.array(all_y_pred)))\n",
    "                test_acc = accuracy_score(all_y_999_removed.numpy(), all_y_pred.numpy() > class_threshold).astype(float)\n",
    "                #               test_acc = accuracy_score(np.array(all_y_999_removed), np.array(all_y_pred)).astype(float)\n",
    "            \n",
    "                total_val_size = total_sgram_val\n",
    "                invalid_file_flag = False\n",
    "                #print(\"total_val_size= \" + str(total_val_size))\n",
    "\n",
    "\n",
    "                return test_loss, test_acc,total_val_size,invalid_file_flag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2107232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098b4f69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b398e5ee",
   "metadata": {},
   "source": [
    "## Data Load and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b6f4ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "library = 'PyTorch'\n",
    "\n",
    "if library == 'PyTorch':\n",
    "    from PyTorch.runTorch_AST_DK import (ASTModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e508bc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(config_DK_AST.data_df_dk_1)\n",
    "\n",
    "# To be kept: please do not edit the test set: these paths select test set A, test set B as described in the paper\n",
    "idx_test_A = np.logical_and(df['country'] == 'Tanzania', df['location_type'] == 'field')\n",
    "idx_test_B = np.logical_and(df['country'] == 'UK', df['location_type'] == 'culture')\n",
    "idx_train = np.logical_not(np.logical_or(idx_test_A, idx_test_B))\n",
    "df_test_A = df[idx_test_A]\n",
    "df_test_B = df[idx_test_B]\n",
    "\n",
    "\n",
    "df_train = df[idx_train]\n",
    "\n",
    "# Modify by addition or sub-sampling of df_train here\n",
    "\n",
    "\n",
    "# Assertion to check that train does NOT appear in test:\n",
    "assert len(np.where(pd.concat([df_train,df_test_A,\n",
    "                               df_test_B]).duplicated())[0]) == 0, 'Train dataframe contains overlap with Test A, Test B'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0be1156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train) = 16\n",
      "len(val) = 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, val = train_test_split(df_train, test_size=0.2,shuffle = True)\n",
    "print(\"len(train) = \"  +str(len(train)))\n",
    "print(\"len(val) = \"  +str(len(val)))\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "val =  val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49f899c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83502c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e524e37e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d66dc76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e2d6f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efcaf646",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_obj = SoundDS(train )\n",
    "val_obj = SoundDS_test(val)\n",
    "\n",
    "\n",
    "# Create training and validation data loaders\n",
    "#train_dl = torch.utils.data.DataLoader(train_obj, batch_size=16, shuffle=True,num_workers = 2 , collate_fn = collate_data,pin_memory = True)\n",
    "train_dl = torch.utils.data.DataLoader(train_obj, batch_size= 1, shuffle=True,pin_memory = True)\n",
    "val_dl = torch.utils.data.DataLoader(val_obj, batch_size=1, shuffle=False,pin_memory = True)\n",
    "\n",
    "# train_dl = torch.utils.data.DataLoader(train_obj, batch_size= 1, shuffle=True, collate_fn = collate_data,pin_memory = True)\n",
    "# val_dl = torch.utils.data.DataLoader(val_obj, batch_size=1, shuffle=False,collate_fn = collate_data,pin_memory = True)\n",
    "\n",
    "# print(\"Len train DL = \" + str(len(train_dl)))\n",
    "# print(\"Len VAL DL = \" + str(len(val_dl)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1838721a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside get_item for train . Idx = 14\n",
      "^^^^^^^^ Start processing filename = ../data/audio/219432.wav^^^^^^^^\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/219432.wav*****\n",
      "About to apply background transform...\n",
      "......APplying spec augment.....\n",
      "****** EXITING PROCESS_WAV*****\n",
      "....calculating expected num of spectogram for the file ->  ../data/audio/219432.wav....\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "initial shape of feat = torch.Size([1, 128, 1697])\n",
      "shape after first transpose = torch.Size([1, 1697, 128])\n",
      "shape after unfolding = torch.Size([1, 324, 128, 80])\n",
      "shape post squeeze = torch.Size([324, 128, 80])\n",
      "Num chunks to be generated = 324\n",
      "One sound file being split into  324\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "returning a feat_list with elements = 324\n",
      "without any augmentation, we expect 324spectograms\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "initial shape of feat = torch.Size([1, 128, 1697])\n",
      "shape after first transpose = torch.Size([1, 1697, 128])\n",
      "shape after unfolding = torch.Size([1, 324, 128, 80])\n",
      "shape post squeeze = torch.Size([324, 128, 80])\n",
      "Num chunks to be generated = 324\n",
      "One sound file being split into  324\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "sgram shape =torch.Size([1, 80, 128])\n",
      "returning a feat_list with elements = 324\n",
      "<class 'list'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_dl)\n",
    "data = dataiter.next()\n",
    "\n",
    "data_x,data_y = data[0]\n",
    "print(type(data_x))\n",
    "print(len(data_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b58e20",
   "metadata": {},
   "source": [
    "### Get the max dims for the val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef078571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bbf35c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>record_datetime</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>species</th>\n",
       "      <th>gender</th>\n",
       "      <th>fed</th>\n",
       "      <th>plurality</th>\n",
       "      <th>age</th>\n",
       "      <th>method</th>\n",
       "      <th>mic_type</th>\n",
       "      <th>device_type</th>\n",
       "      <th>country</th>\n",
       "      <th>district</th>\n",
       "      <th>province</th>\n",
       "      <th>place</th>\n",
       "      <th>location_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>219431</td>\n",
       "      <td>59.392000</td>\n",
       "      <td>r2016-05-20_09.45.38.381.wav_u2016-05-20_09.16...</td>\n",
       "      <td>8000</td>\n",
       "      <td>20-05-16 09:45</td>\n",
       "      <td>background</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4015X</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nairobi</td>\n",
       "      <td>USAMRU-K</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4221</td>\n",
       "      <td>6.243902</td>\n",
       "      <td>larvae_#12-20_rec1.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LC</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58</td>\n",
       "      <td>0.170249</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>86</td>\n",
       "      <td>0.264832</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Plural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>218850</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>r2016-05-24_11.33.13.946.wav_u2016-05-25_06.23...</td>\n",
       "      <td>8000</td>\n",
       "      <td>24-05-16 11:33</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an gambiae ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>Single</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4015X</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nairobi</td>\n",
       "      <td>USAMRU-K</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>74</td>\n",
       "      <td>0.127687</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>54</td>\n",
       "      <td>0.104041</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>71</td>\n",
       "      <td>0.690455</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>72</td>\n",
       "      <td>0.506019</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>73</td>\n",
       "      <td>0.444539</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>63</td>\n",
       "      <td>0.080395</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>219429</td>\n",
       "      <td>54.272000</td>\n",
       "      <td>r2016-05-20_09.42.09.700.wav_u2016-05-20_09.15...</td>\n",
       "      <td>8000</td>\n",
       "      <td>20-05-16 09:42</td>\n",
       "      <td>background</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4015X</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nairobi</td>\n",
       "      <td>USAMRU-K</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>219430</td>\n",
       "      <td>18.176000</td>\n",
       "      <td>r2016-05-20_09.43.09.746.wav_u2016-05-20_09.15...</td>\n",
       "      <td>8000</td>\n",
       "      <td>20-05-16 09:43</td>\n",
       "      <td>background</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4015X</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nairobi</td>\n",
       "      <td>USAMRU-K</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>62</td>\n",
       "      <td>0.104041</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>219428</td>\n",
       "      <td>59.392000</td>\n",
       "      <td>r2016-05-20_09.41.04.403.wav_u2016-05-20_09.15...</td>\n",
       "      <td>8000</td>\n",
       "      <td>20-05-16 09:41</td>\n",
       "      <td>background</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4015X</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nairobi</td>\n",
       "      <td>USAMRU-K</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>219427</td>\n",
       "      <td>46.592000</td>\n",
       "      <td>r2016-05-20_09.37.29.958.wav_u2016-05-20_09.14...</td>\n",
       "      <td>8000</td>\n",
       "      <td>20-05-16 09:37</td>\n",
       "      <td>background</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4015X</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nairobi</td>\n",
       "      <td>USAMRU-K</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     length                                               name  \\\n",
       "0   219431  59.392000  r2016-05-20_09.45.38.381.wav_u2016-05-20_09.16...   \n",
       "1     4221   6.243902                             larvae_#12-20_rec1.wav   \n",
       "2       58   0.170249                    CDC_Ae-aegypti_labelled_800.wav   \n",
       "3       86   0.264832                    CDC_Ae-aegypti_labelled_800.wav   \n",
       "4   218850   5.000000  r2016-05-24_11.33.13.946.wav_u2016-05-25_06.23...   \n",
       "5       74   0.127687                    CDC_Ae-aegypti_labelled_800.wav   \n",
       "6       54   0.104041                    CDC_Ae-aegypti_labelled_800.wav   \n",
       "7       71   0.690455                    CDC_Ae-aegypti_labelled_800.wav   \n",
       "8       72   0.506019                    CDC_Ae-aegypti_labelled_800.wav   \n",
       "9       73   0.444539                    CDC_Ae-aegypti_labelled_800.wav   \n",
       "10      63   0.080395                    CDC_Ae-aegypti_labelled_800.wav   \n",
       "11  219429  54.272000  r2016-05-20_09.42.09.700.wav_u2016-05-20_09.15...   \n",
       "12  219430  18.176000  r2016-05-20_09.43.09.746.wav_u2016-05-20_09.15...   \n",
       "13      62   0.104041                    CDC_Ae-aegypti_labelled_800.wav   \n",
       "14  219428  59.392000  r2016-05-20_09.41.04.403.wav_u2016-05-20_09.15...   \n",
       "15  219427  46.592000  r2016-05-20_09.37.29.958.wav_u2016-05-20_09.14...   \n",
       "\n",
       "    sample_rate record_datetime  sound_type        species  gender  fed  \\\n",
       "0          8000  20-05-16 09:45  background            NaN     NaN  NaN   \n",
       "1         44100  1/7/2018 12:00    mosquito            NaN     NaN  NaN   \n",
       "2          8000   8/9/2016 8:00    mosquito     ae aegypti     NaN  NaN   \n",
       "3          8000   8/9/2016 8:00    mosquito     ae aegypti     NaN  NaN   \n",
       "4          8000  24-05-16 11:33    mosquito  an gambiae ss  Female    f   \n",
       "5          8000   8/9/2016 8:00    mosquito     ae aegypti     NaN  NaN   \n",
       "6          8000   8/9/2016 8:00    mosquito     ae aegypti     NaN  NaN   \n",
       "7          8000   8/9/2016 8:00    mosquito     ae aegypti     NaN  NaN   \n",
       "8          8000   8/9/2016 8:00    mosquito     ae aegypti     NaN  NaN   \n",
       "9          8000   8/9/2016 8:00    mosquito     ae aegypti     NaN  NaN   \n",
       "10         8000   8/9/2016 8:00    mosquito     ae aegypti     NaN  NaN   \n",
       "11         8000  20-05-16 09:42  background            NaN     NaN  NaN   \n",
       "12         8000  20-05-16 09:43  background            NaN     NaN  NaN   \n",
       "13         8000   8/9/2016 8:00    mosquito     ae aegypti     NaN  NaN   \n",
       "14         8000  20-05-16 09:41  background            NaN     NaN  NaN   \n",
       "15         8000  20-05-16 09:37  background            NaN     NaN  NaN   \n",
       "\n",
       "   plurality  age method mic_type    device_type   country          district  \\\n",
       "0        NaN  NaN    NaN    phone  Alcatel 4015X     Kenya               NaN   \n",
       "1        NaN  NaN     LC  telinga        olympus  Thailand  Sai Yok District   \n",
       "2     Single  NaN    NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "3     Plural  NaN    NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "4     Single  3.0    NaN    phone  Alcatel 4015X     Kenya               NaN   \n",
       "5     Single  NaN    NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "6     Single  NaN    NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "7     Single  NaN    NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "8     Single  NaN    NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "9     Single  NaN    NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "10    Single  NaN    NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "11       NaN  NaN    NaN    phone  Alcatel 4015X     Kenya               NaN   \n",
       "12       NaN  NaN    NaN    phone  Alcatel 4015X     Kenya               NaN   \n",
       "13    Single  NaN    NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "14       NaN  NaN    NaN    phone  Alcatel 4015X     Kenya               NaN   \n",
       "15       NaN  NaN    NaN    phone  Alcatel 4015X     Kenya               NaN   \n",
       "\n",
       "                 province                            place location_type  \n",
       "0                 Nairobi                         USAMRU-K       culture  \n",
       "1   Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "2                 Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "3                 Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "4                 Nairobi                         USAMRU-K       culture  \n",
       "5                 Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "6                 Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "7                 Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "8                 Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "9                 Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "10                Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "11                Nairobi                         USAMRU-K       culture  \n",
       "12                Nairobi                         USAMRU-K       culture  \n",
       "13                Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "14                Nairobi                         USAMRU-K       culture  \n",
       "15                Nairobi                         USAMRU-K       culture  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2fc9ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>record_datetime</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>species</th>\n",
       "      <th>gender</th>\n",
       "      <th>fed</th>\n",
       "      <th>plurality</th>\n",
       "      <th>age</th>\n",
       "      <th>method</th>\n",
       "      <th>mic_type</th>\n",
       "      <th>device_type</th>\n",
       "      <th>country</th>\n",
       "      <th>district</th>\n",
       "      <th>province</th>\n",
       "      <th>place</th>\n",
       "      <th>location_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>219432</td>\n",
       "      <td>54.272000</td>\n",
       "      <td>r2016-05-20_09.46.43.670.wav_u2016-05-20_09.16...</td>\n",
       "      <td>8000</td>\n",
       "      <td>20-05-16 09:46</td>\n",
       "      <td>background</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4015X</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nairobi</td>\n",
       "      <td>USAMRU-K</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>219976</td>\n",
       "      <td>59.820408</td>\n",
       "      <td>IFA_33_36_1316_background.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>29-02-20 00:00</td>\n",
       "      <td>background</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>218964</td>\n",
       "      <td>5.376000</td>\n",
       "      <td>r2016-05-26_09.34.39.032.wav_u2016-05-31_07.01...</td>\n",
       "      <td>8000</td>\n",
       "      <td>26-05-16 09:34</td>\n",
       "      <td>background</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4015X</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nairobi</td>\n",
       "      <td>USAMRU-K</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>218851</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>r2016-05-24_11.33.13.946.wav_u2016-05-25_06.23...</td>\n",
       "      <td>8000</td>\n",
       "      <td>24-05-16 11:33</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an gambiae ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>Single</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4015X</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nairobi</td>\n",
       "      <td>USAMRU-K</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>219424</td>\n",
       "      <td>5.376000</td>\n",
       "      <td>r2015-12-17_18.30.08.828.wav_u2016-04-08_14.22...</td>\n",
       "      <td>8000</td>\n",
       "      <td>17-12-15 18:30</td>\n",
       "      <td>background</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4015X</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nairobi</td>\n",
       "      <td>USAMRU-K</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id     length                                               name  \\\n",
       "0  219432  54.272000  r2016-05-20_09.46.43.670.wav_u2016-05-20_09.16...   \n",
       "1  219976  59.820408                      IFA_33_36_1316_background.wav   \n",
       "2  218964   5.376000  r2016-05-26_09.34.39.032.wav_u2016-05-31_07.01...   \n",
       "3  218851   2.900000  r2016-05-24_11.33.13.946.wav_u2016-05-25_06.23...   \n",
       "4  219424   5.376000  r2015-12-17_18.30.08.828.wav_u2016-04-08_14.22...   \n",
       "\n",
       "   sample_rate record_datetime  sound_type        species  gender  fed  \\\n",
       "0         8000  20-05-16 09:46  background            NaN     NaN  NaN   \n",
       "1        44100  29-02-20 00:00  background            NaN     NaN  NaN   \n",
       "2         8000  26-05-16 09:34  background            NaN     NaN  NaN   \n",
       "3         8000  24-05-16 11:33    mosquito  an gambiae ss  Female    f   \n",
       "4         8000  17-12-15 18:30  background            NaN     NaN  NaN   \n",
       "\n",
       "  plurality  age method mic_type    device_type   country            district  \\\n",
       "0       NaN  NaN    NaN    phone  Alcatel 4015X     Kenya                 NaN   \n",
       "1       NaN  NaN    HBN  telinga         tascam  Tanzania  Kilombero District   \n",
       "2       NaN  NaN    NaN    phone  Alcatel 4015X     Kenya                 NaN   \n",
       "3    Single  3.0    NaN    phone  Alcatel 4015X     Kenya                 NaN   \n",
       "4       NaN  NaN    NaN    phone  Alcatel 4015X     Kenya                 NaN   \n",
       "\n",
       "   province     place location_type  \n",
       "0   Nairobi  USAMRU-K       culture  \n",
       "1  Morogoro   Ifakara           cup  \n",
       "2   Nairobi  USAMRU-K       culture  \n",
       "3   Nairobi  USAMRU-K       culture  \n",
       "4   Nairobi  USAMRU-K       culture  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7119df83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60016c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar  5 20:32:59 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   24C    P0    22W / 300W |      2MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28916624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:0\n",
      "epoch = 0\n",
      "inside get_item for train . Idx = 13\n",
      "^^^^^^^^ Start processing filename = ../data/audio/62.wav^^^^^^^^\n",
      "inside shorter files\n",
      "****** ENTERING PROCESS_WAV for file = ../data/audio/62.wav*****\n",
      "About to apply signal transform...\n",
      "****** EXITING PROCESS_WAV (pad == TRUE)*****\n",
      "......APplying spec augment.....\n",
      "without any augmentation, we expect 1spectograms\n",
      "&&&&&&&&&&&&&&&& INSIDE RESHAPE &&&&&&&&&&&&&&&&&&&&&&&\n",
      "shape of tup = torch.Size([1, 80, 128])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_653/3829185638.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model_ast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_653/2878158842.py\u001b[0m in \u001b[0;36mtrain_model_ast\u001b[0;34m(train_dl, val_dl, model)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtotal_sgrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#our data is a list of dim = max_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TRain_dl length is \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_653/2110784097.py\u001b[0m in \u001b[0;36mcollate_data\u001b[0;34m(spec_list)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mx_collate\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_collate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m#print(\"x_collate len = \" + str(len(x_collate)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#print(\"y_collate  = \" + str(y_collate))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "model = train_model_ast(train_dl , val_dl )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c50c7e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45fa71f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3490/973257664.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f1997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = reshape_feat_pt(torch.random(1,128,408))\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7489f524",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model = train_model_ast(X_train, y_train, X_val, y_val, model=ASTModel())\n",
    "#model = train_model_ast(train_data, val_data,model=ASTModel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb184f9c",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1dddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dim_testA = get_max_file_dims(df_test_A )\n",
    "max_dim_testA = max_dim_testA + int(max_dim_testA*config_DK_AST.len_tol)\n",
    "\n",
    "\n",
    "max_dim_testB = get_max_file_dims(df_test_B )\n",
    "max_dim_testB = max_dim_testB + int(max_dim_testB*config_DK_AST.len_tol)\n",
    "\n",
    "\n",
    "testA_obj = SoundDS_test(df_test_A,max_dim = max_dim_testA)\n",
    "testB_obj = SoundDS_test(df_test_B , max_dim = max_dim_testA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d3f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filepath, model=ASTModel()):\n",
    "    # Instantiate model to inspect\n",
    "    print(\"Filepath = \" + str(filepath))\n",
    "    print(\"model = \" +str(model))\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "    print(f'Training on {device}')\n",
    "        \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "    model = model.to(device)\n",
    "    # Load trained parameters from checkpoint (may need to download from S3 first)\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        map_location=lambda storage, loc: storage.cuda()\n",
    "    else:\n",
    "        map_location='cpu'\n",
    "        \n",
    "    checkpoint = model.load_state_dict(torch.load(filepath))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab7427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlp = MLP()\n",
    "#mlp.load_state_dict(torch.load(save_path))\n",
    "\n",
    "\n",
    "model_init=ASTModel()\n",
    "model_init.eval()\n",
    "path = '../outputs/models/pytorch/'\n",
    "model_name = 'model_e2_2022_02_14_21_57_53.pth'\n",
    "\n",
    "\n",
    "#checkpoint = model.load_state_dict(torch.load(filepath))\n",
    "trained_model = load_model(path + model_name , model = model_init)\n",
    "\n",
    "\n",
    "# Dummy data for evaluation\n",
    "\n",
    "#creating random data to test \n",
    "#f1 = np.random.randn(128,1075)\n",
    "#f2 = np.random.randn(128,242)\n",
    "#f3 = np.random.randn(128,234)\n",
    "#f4 = np.ranadom.randn(128,263)\n",
    "\n",
    "# replicates get_feat\n",
    "#feats = [f1,f2,f3,f4]\n",
    "#labels = [0,1,0,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07171e92",
   "metadata": {},
   "source": [
    "## Test A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc34f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_type = 'FeatB'\n",
    "if feat_type == 'FeatA':\n",
    "    p, yt = evaluate_model_aggregated(trained_model, X_test_A, y_test_A, 1)  # Aggregate windows from feature list (0.96->1.92 s)\n",
    "    PE, MI, log_prob = get_results(p, yt, filename = feat_type + '_' + model_name +'_Test_A')\n",
    "elif feat_type == 'FeatB':\n",
    "    y_preds_all = evaluate_model(trained_model, X_test_A, y_test_A, 1)  # Predict directly over feature windows (1.92 s)\n",
    "    PE, MI, log_prob = get_results(y_preds_all, y_test_A, filename = feat_type + '_' + model_name +'_Test_A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b50822",
   "metadata": {},
   "source": [
    "## TEST B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ad54bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_type = 'FeatB'\n",
    "if feat_type == 'FeatA':\n",
    "    \n",
    "    p, yt = evaluate_model_aggregated(model, X_test_B, y_test_B, 1)  # Aggregate windows from feature list (0.96->1.92 s)\n",
    "    PE, MI, log_prob = get_results(p, yt, filename = feat_type + '_' + model_name +'_Test_B')\n",
    "elif feat_type == 'FeatB':\n",
    "    y_preds_all = evaluate_model(trained_model, X_test_B, y_test_B, 1)  # Predict directly over feature windows (1.92 s)\n",
    "    PE, MI, log_prob = get_results(y_preds_all, y_test_B, filename = feat_type + '_' + model_name +'_Test_B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f01a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ffe727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75860179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
