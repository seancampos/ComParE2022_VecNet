{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bddf7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".CodeMirror{\n",
       "font-size: 14px;\n",
       "</style>\n",
       "CUDA_LAUNCH_BLOCKING=1\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style type='text/css'>\n",
    ".CodeMirror{\n",
    "font-size: 14px;\n",
    "</style>\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffbc31ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch_audiomentations in /opt/conda/lib/python3.8/site-packages (0.10.1)\n",
      "Requirement already satisfied: torchaudio>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (0.11.0+cu113)\n",
      "Requirement already satisfied: librosa>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (0.8.1)\n",
      "Requirement already satisfied: torch>=1.7.0 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (1.11.0+cu113)\n",
      "Requirement already satisfied: torch-pitch-shift>=1.2.0 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (1.2.2)\n",
      "Requirement already satisfied: julius<0.3,>=0.2.3 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (0.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (21.3)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (0.24.0)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (2.1.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (1.21.4)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (5.1.0)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (0.2.2)\n",
      "Requirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (1.5.2)\n",
      "Requirement already satisfied: numba>=0.43.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (0.53.1)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (0.10.3.post1)\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (1.6.3)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /opt/conda/lib/python3.8/site-packages (from numba>=0.43.0->librosa>=0.6.0->torch_audiomentations) (0.36.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from numba>=0.43.0->librosa>=0.6.0->torch_audiomentations) (59.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->librosa>=0.6.0->torch_audiomentations) (3.0.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (2.26.0)\n",
      "Requirement already satisfied: appdirs in /opt/conda/lib/python3.8/site-packages (from pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (1.4.4)\n",
      "Requirement already satisfied: six>=1.3 in /opt/conda/lib/python3.8/site-packages (from resampy>=0.2.2->librosa>=0.6.0->torch_audiomentations) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa>=0.6.0->torch_audiomentations) (3.0.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.8/site-packages (from soundfile>=0.10.2->librosa>=0.6.0->torch_audiomentations) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa>=0.6.0->torch_audiomentations) (2.21)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.7.0->torch_audiomentations) (4.0.1)\n",
      "Requirement already satisfied: primePy>=1.3 in /opt/conda/lib/python3.8/site-packages (from torch-pitch-shift>=1.2.0->torch_audiomentations) (1.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (2.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_audiomentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d693b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_audiomentations import Compose,AddBackgroundNoise , AddColoredNoise , ApplyImpulseResponse,PeakNormalization,TimeInversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64e5b378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had to find the right version of pytorch with the widget here https://pytorch.org/\n",
    "# I *think* this will work with AWS\n",
    "#!pip3 install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f05880e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other dependencies\n",
    "#!pip install timm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "434750e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nnAudio\n",
    "#!pip install git+https://github.com/KinWaiCheuk/nnAudio.git#subdirectory=Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a162ceb",
   "metadata": {},
   "source": [
    "### 1 Import the kitchen sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dbe3864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
     ]
    }
   ],
   "source": [
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59e1a849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug main imports\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../lib'))\n",
    "import config\n",
    "from evaluate import get_results\n",
    "import numpy as np\n",
    "\n",
    "# Troubleshooting and visualisation\n",
    "import IPython.display as ipd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "098cf55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug lib imports\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from PyTorch import config_pytorch\n",
    "from datetime import datetime\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9478fbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional pytorch tools\n",
    "import random\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchvision.transforms as VT\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "import timm\n",
    "import timm.optim\n",
    "from timm.loss import BinaryCrossEntropy\n",
    "from timm.utils import NativeScaler\n",
    "from timm.models import model_parameters\n",
    "from glob import glob\n",
    "from torch_audiomentations import Compose, Gain, PolarityInversion,AddColoredNoise,ApplyImpulseResponse,PeakNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bb4c5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nnAudio\n",
    "from nnAudio import features\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e71a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Training variables \n",
    "USE_SHORT_AUDIO = True\n",
    "num_workers=4\n",
    "pin_memory=True\n",
    "#train_size = 100\n",
    "batch_size = 32\n",
    "test_batch_size = 32\n",
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    batch_size = 4\n",
    "    test_batch_size = 4\n",
    "    num_workers=1\n",
    "    \n",
    "     \n",
    "\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bcd731",
   "metadata": {},
   "source": [
    "### Run all these function definition cells\n",
    "These have been extracted from the lib folder and are here to make them more easily editable.  Most of the action happens in *get_feat_torch*, which does feature extraction and *train_model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ba597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdfef97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates 1.92 secs rows of audio in a data frame format\n",
    "def get_offsets_df(df, short_audio=False):\n",
    "    audio_offsets = []\n",
    "    min_length = config.win_size*config.NFFT/(((1/config.n_hop)*config.NFFT)*config.rate)\n",
    "    step_frac = config.step_size/config.win_size\n",
    "    for _,row in df.iterrows():\n",
    "        if row['length'] > min_length:\n",
    "            step_size = step_frac*min_length\n",
    "            audio_offsets.append({'id':row['id'], 'offset':0, 'length': row['length'],'specie_ind': row['specie_ind']})\n",
    "            for i in range(1, int((row['length']-min_length)//step_size)):\n",
    "                audio_offsets.append({'id': row['id'], 'offset':int(min_length+(i*step_size)*config.rate), 'length': row['length'],'specie_ind': row['specie_ind']})\n",
    "        elif short_audio:\n",
    "            audio_offsets.append({'id':row['id'], 'offset':0,'length': row['length'],'specie_ind': row['specie_ind']})\n",
    "    return pd.DataFrame(audio_offsets)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c85f23d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['an arabiensis','culex pipiens complex', 'ae aegypti','an funestus ss','an squamosus',\n",
    "               'an coustani','ma uniformis','ma africanus' ]\n",
    "# classes_no_other = ['an arabiensis','culex pipiens complex', 'ae aegypti','an funestus ss','an squamosus',\n",
    "#                'an coustani','ma uniformis','ma africanus' ]\n",
    "#other_ind = classes.index('others')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aeb184",
   "metadata": {},
   "source": [
    "### Read CSV and get train/test groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "553dc59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>record_datetime</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>species</th>\n",
       "      <th>gender</th>\n",
       "      <th>fed</th>\n",
       "      <th>plurality</th>\n",
       "      <th>age</th>\n",
       "      <th>method</th>\n",
       "      <th>mic_type</th>\n",
       "      <th>device_type</th>\n",
       "      <th>country</th>\n",
       "      <th>district</th>\n",
       "      <th>province</th>\n",
       "      <th>place</th>\n",
       "      <th>location_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>0.463456</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>08-09-16 08:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>0.170249</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>08-09-16 08:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>0.104041</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>08-09-16 08:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69</td>\n",
       "      <td>0.274290</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>08-09-16 08:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56</td>\n",
       "      <td>0.420894</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>08-09-16 08:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Plural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>3562</td>\n",
       "      <td>6.083093</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>01-07-18 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>3556</td>\n",
       "      <td>6.719908</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>01-07-18 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an maculatus</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9009</th>\n",
       "      <td>3553</td>\n",
       "      <td>6.128580</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>01-07-18 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an maculatus</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9011</th>\n",
       "      <td>3561</td>\n",
       "      <td>11.614280</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>01-07-18 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9012</th>\n",
       "      <td>3552</td>\n",
       "      <td>2.920249</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>01-07-18 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6008 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     length                             name  sample_rate  \\\n",
       "1       53   0.463456  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "2       57   0.170249  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "3       61   0.104041  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "4       69   0.274290  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "5       56   0.420894  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "...    ...        ...                              ...          ...   \n",
       "8999  3562   6.083093                    #988-1001.wav        44100   \n",
       "9000  3556   6.719908                    #988-1001.wav        44100   \n",
       "9009  3553   6.128580                    #988-1001.wav        44100   \n",
       "9011  3561  11.614280                    #988-1001.wav        44100   \n",
       "9012  3552   2.920249                    #988-1001.wav        44100   \n",
       "\n",
       "     record_datetime sound_type       species  gender  fed plurality  age  \\\n",
       "1     08-09-16 08:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "2     08-09-16 08:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "3     08-09-16 08:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "4     08-09-16 08:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "5     08-09-16 08:00   mosquito    ae aegypti     NaN  NaN    Plural  NaN   \n",
       "...              ...        ...           ...     ...  ...       ...  ...   \n",
       "8999  01-07-18 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "9000  01-07-18 12:00   mosquito  an maculatus  Female    t    Single  NaN   \n",
       "9009  01-07-18 12:00   mosquito  an maculatus  Female    t    Single  NaN   \n",
       "9011  01-07-18 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "9012  01-07-18 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "\n",
       "     method mic_type    device_type   country          district  \\\n",
       "1       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "2       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "3       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "4       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "5       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "...     ...      ...            ...       ...               ...   \n",
       "8999    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9000    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9009    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9011    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9012    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "\n",
       "                   province                            place location_type  \n",
       "1                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "2                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "3                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "4                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "5                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "...                     ...                              ...           ...  \n",
       "8999  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9000  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9009  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9011  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9012  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "\n",
       "[6008 rows x 19 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    df = pd.read_csv(config.data_df_msc_test)\n",
    "else:\n",
    "    df = pd.read_csv(config.data_df)\n",
    "\n",
    "#df = df.loc[df['Grade'].notnull()]\n",
    "df = df.loc[df['species'].notnull()]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "020d4304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a colum for specie encoding\n",
    "df['specie_ind'] = \"NULL_VAL\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a39df15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specie = an arabiensisand its index = 0\n",
      "specie = culex pipiens complexand its index = 1\n",
      "specie = ae aegyptiand its index = 2\n",
      "specie = an funestus ssand its index = 3\n",
      "specie = an squamosusand its index = 4\n",
      "specie = an coustaniand its index = 5\n",
      "specie = ma uniformisand its index = 6\n",
      "specie = ma africanusand its index = 7\n"
     ]
    }
   ],
   "source": [
    "# Adding a new column to encode specie_index in the same order as the list \"classes\"\n",
    "ind = 0\n",
    "for specie in classes:\n",
    "    print(\"specie = \" + str(specie) + \"and its index = \" + str(ind) )\n",
    "    row_indexes=df[df['species']==specie].index \n",
    "    df.loc[row_indexes,'specie_ind']= ind\n",
    "    ind+=1\n",
    "\n",
    "    \n",
    "# other_df_ind = df[df['specie_ind'] == \"NULL_VAL\"].index\n",
    "# df.loc[other_df_ind,'specie_ind']= other_ind                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09333d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['specie_ind'] == \"NULL_VAL\"].index, inplace=True)\n",
    "#other_df_ind = df[df['specie_ind'] == \"NULL_VAL\"].index\n",
    "#df.loc[other_df_ind,'specie_ind']= other_ind        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24e4bb65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3476"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f6a3882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the TZ and Cup data on which test should happen\n",
    "#Test should happen on Cup and Tanzania\n",
    "\n",
    "idx_multiclass = np.logical_and(df['country'] == 'Tanzania', df['location_type'] == 'cup')\n",
    "df_all = df[idx_multiclass]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71c00327",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b80bcb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>record_datetime</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>species</th>\n",
       "      <th>gender</th>\n",
       "      <th>fed</th>\n",
       "      <th>...</th>\n",
       "      <th>age</th>\n",
       "      <th>method</th>\n",
       "      <th>mic_type</th>\n",
       "      <th>device_type</th>\n",
       "      <th>country</th>\n",
       "      <th>district</th>\n",
       "      <th>province</th>\n",
       "      <th>place</th>\n",
       "      <th>location_type</th>\n",
       "      <th>specie_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1879</td>\n",
       "      <td>221103</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_24_664.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1880</td>\n",
       "      <td>221111</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_25_665.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1881</td>\n",
       "      <td>221110</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_25_665.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1882</td>\n",
       "      <td>221149</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_26_666.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an arabiensis</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1883</td>\n",
       "      <td>221150</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_26_666.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an arabiensis</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>4546</td>\n",
       "      <td>222615</td>\n",
       "      <td>30.72</td>\n",
       "      <td>IFA_86_39_3439.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2284</th>\n",
       "      <td>4547</td>\n",
       "      <td>222585</td>\n",
       "      <td>25.60</td>\n",
       "      <td>IFA_86_40_3440.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2285</th>\n",
       "      <td>4548</td>\n",
       "      <td>222586</td>\n",
       "      <td>40.90</td>\n",
       "      <td>IFA_87_10_3450.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2286</th>\n",
       "      <td>4549</td>\n",
       "      <td>222596</td>\n",
       "      <td>40.90</td>\n",
       "      <td>IFA_87_11_3451.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287</th>\n",
       "      <td>4550</td>\n",
       "      <td>222614</td>\n",
       "      <td>38.40</td>\n",
       "      <td>IFA_87_12_3452.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2288 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index      id  length                name  sample_rate record_datetime  \\\n",
       "0      1879  221103    2.56   IFA_17_24_664.wav        44100  30-01-20 00:00   \n",
       "1      1880  221111    2.56   IFA_17_25_665.wav        44100  30-01-20 00:00   \n",
       "2      1881  221110    2.56   IFA_17_25_665.wav        44100  30-01-20 00:00   \n",
       "3      1882  221149    2.56   IFA_17_26_666.wav        44100  30-01-20 00:00   \n",
       "4      1883  221150    2.56   IFA_17_26_666.wav        44100  30-01-20 00:00   \n",
       "...     ...     ...     ...                 ...          ...             ...   \n",
       "2283   4546  222615   30.72  IFA_86_39_3439.wav        44100  23-08-20 00:00   \n",
       "2284   4547  222585   25.60  IFA_86_40_3440.wav        44100  23-08-20 00:00   \n",
       "2285   4548  222586   40.90  IFA_87_10_3450.wav        44100  23-08-20 00:00   \n",
       "2286   4549  222596   40.90  IFA_87_11_3451.wav        44100  23-08-20 00:00   \n",
       "2287   4550  222614   38.40  IFA_87_12_3452.wav        44100  23-08-20 00:00   \n",
       "\n",
       "     sound_type         species  gender fed  ... age  method mic_type  \\\n",
       "0      mosquito    ma africanus  Female   f  ... NaN     HBN  telinga   \n",
       "1      mosquito    ma africanus  Female   f  ... NaN     HBN  telinga   \n",
       "2      mosquito    ma africanus  Female   f  ... NaN     HBN  telinga   \n",
       "3      mosquito   an arabiensis  Female   f  ... NaN     HBN  telinga   \n",
       "4      mosquito   an arabiensis  Female   f  ... NaN     HBN  telinga   \n",
       "...         ...             ...     ...  ..  ...  ..     ...      ...   \n",
       "2283   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2284   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2285   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2286   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2287   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "\n",
       "     device_type   country            district  province    place  \\\n",
       "0         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "1         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "3         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "4         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "...          ...       ...                 ...       ...      ...   \n",
       "2283      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2284      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2285      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2286      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2287      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "\n",
       "     location_type specie_ind  \n",
       "0              cup          7  \n",
       "1              cup          7  \n",
       "2              cup          7  \n",
       "3              cup          0  \n",
       "4              cup          0  \n",
       "...            ...        ...  \n",
       "2283           cup          3  \n",
       "2284           cup          3  \n",
       "2285           cup          3  \n",
       "2286           cup          3  \n",
       "2287           cup          3  \n",
       "\n",
       "[2288 rows x 21 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4ce581",
   "metadata": {},
   "source": [
    "As a result of extraction ,\"df_tz_cup\" and \"DF\" doesn't have indices in sequence, let's reset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bf5053",
   "metadata": {},
   "source": [
    "### Now peforming a \"set difference\" on temp_train and df-val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e609f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# avoiding train test split and using the same logic as in original Humbug paper\n",
    "# train_recordings[i] =  shuffle(pd.unique(df_class.name), random_state=random_seed)[:n_train]  \n",
    "#test_recordings[i] = shuffle(pd.unique(df_class.name),random_state=random_seed)[n_train:]\n",
    "# #df_train_offset_temp,df_test_offset  = train_test_split(df_offset, test_size=0.2,random_state = 152)\n",
    "msk_test = np.random.rand(len(df_all)) < 0.2\n",
    "df_test = df_all[msk_test]\n",
    "df_train_temp  = df_all[~msk_test]\n",
    "#df_train = pd.concat([df_temp_train, df_val, df_val]).drop_duplicates(keep=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "260d43be",
   "metadata": {},
   "outputs": [],
   "source": [
    "msk_train = np.random.rand(len(df_train_temp)) < 0.2\n",
    "df_val = df_train_temp[msk_train]\n",
    "df_train  = df_train_temp[~msk_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c303ef",
   "metadata": {},
   "source": [
    "Let's verify if any of the test recording has leaked into train or val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb1cb9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, index_y, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_test,df_train, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6365198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, index_y, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_test,df_val, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef12cb83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, index_y, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_train,df_val, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc00156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_offset = get_offsets_df(df_train, short_audio=USE_SHORT_AUDIO)\n",
    "df_test_offset = get_offsets_df(df_test, short_audio=USE_SHORT_AUDIO)\n",
    "df_val_offset = get_offsets_df(df_val, short_audio=USE_SHORT_AUDIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef15c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train offset = 65825\n",
      "length of test offset = 18862\n",
      "length of val offset = 17893\n"
     ]
    }
   ],
   "source": [
    "print(\"length of train offset = \" +str(len(df_train_offset)))\n",
    "print(\"length of test offset = \" +str(len(df_test_offset)))\n",
    "print(\"length of val offset = \" +str(len(df_val_offset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b7dc0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28099171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# df_train_offset_temp,df_test_offset  = train_test_split(df_offset, test_size=0.2,random_state = 152)\n",
    "# df_train_offset,df_val_offset  = train_test_split(df_train_offset_temp, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54fbf832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,len(classes)):\n",
    "#     df_temp = df_val_offset[df_val_offset['specie_ind'] == i]\n",
    "#     print(\"i = \" +str(i))\n",
    "#     print(len(df_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bfb2cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temp.reset_index(inplace = True)\n",
    "df_train_offset.reset_index(inplace = True)\n",
    "df_test_offset.reset_index(inplace = True)\n",
    "df_val_offset.reset_index(inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71517aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specie_distri(df , classes , type_df = None):\n",
    "    for i in range(len(classes)):\n",
    "        print(\"DF type = \" + str(type_df))\n",
    "        df_temp = df[df['specie_ind'] == i]\n",
    "        print(\"i = \" +str(i))\n",
    "        print(len(df_temp))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9cf5837c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31081196 0.59684644 3.69471262 0.6111204  2.19885756 4.38599414\n",
      " 2.66627511 7.02657985]\n"
     ]
    }
   ],
   "source": [
    "#class_weights = class_weight.compute_class_weight('balanced',classes=np.unique(np.array(y_train_CNN)),y=np.array(y_train_CNN))\n",
    "np.array(df_train_offset.specie_ind)\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',classes=np.unique(np.array(df_train_offset.specie_ind)),y=np.array(np.array(df_train_offset.specie_ind)))\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76862d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ecd55d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF type = train\n",
      "i = 0\n",
      "26473\n",
      "DF type = train\n",
      "i = 1\n",
      "13786\n",
      "DF type = train\n",
      "i = 2\n",
      "2227\n",
      "DF type = train\n",
      "i = 3\n",
      "13464\n",
      "DF type = train\n",
      "i = 4\n",
      "3742\n",
      "DF type = train\n",
      "i = 5\n",
      "1876\n",
      "DF type = train\n",
      "i = 6\n",
      "3086\n",
      "DF type = train\n",
      "i = 7\n",
      "1171\n"
     ]
    }
   ],
   "source": [
    "get_specie_distri(df_train_offset , classes , type_df = \"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b9683b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF type = Val\n",
      "i = 0\n",
      "7366\n",
      "DF type = Val\n",
      "i = 1\n",
      "3637\n",
      "DF type = Val\n",
      "i = 2\n",
      "568\n",
      "DF type = Val\n",
      "i = 3\n",
      "3690\n",
      "DF type = Val\n",
      "i = 4\n",
      "1048\n",
      "DF type = Val\n",
      "i = 5\n",
      "630\n",
      "DF type = Val\n",
      "i = 6\n",
      "544\n",
      "DF type = Val\n",
      "i = 7\n",
      "410\n"
     ]
    }
   ],
   "source": [
    "get_specie_distri(df_val_offset , classes , type_df = \"Val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2543e49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF type = test\n",
      "i = 0\n",
      "7271\n",
      "DF type = test\n",
      "i = 1\n",
      "4683\n",
      "DF type = test\n",
      "i = 2\n",
      "789\n",
      "DF type = test\n",
      "i = 3\n",
      "3632\n",
      "DF type = test\n",
      "i = 4\n",
      "870\n",
      "DF type = test\n",
      "i = 5\n",
      "491\n",
      "DF type = test\n",
      "i = 6\n",
      "731\n",
      "DF type = test\n",
      "i = 7\n",
      "395\n"
     ]
    }
   ],
   "source": [
    "get_specie_distri(df_test_offset , classes , type_df = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ab74038",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get the frame offsets for each audio file into dataframes\n",
    "# audio_df_train = get_offsets_df(df_train, short_audio=USE_SHORT_AUDIO)\n",
    "# audio_df_test_A = get_offsets_df(df_test_A, short_audio=False)\n",
    "# audio_df_test_B = get_offsets_df(df_test_B, short_audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5608f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function pads a file with 0s to make it a 1.92 sec file\n",
    "def pad_mean(x_temp,rate = config.rate, min_length = config.min_duration ):\n",
    "    if DEBUG:\n",
    "        print(\"inside padding mean...\")\n",
    "    x_mean = torch.mean(x_temp)\n",
    "    #x_mean.cuda()\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"X_mean = \" + str(x_mean))\n",
    "    left_pad_amt = int((rate*min_length-x_temp.shape[1])//2)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_amt = \" + str(left_pad_amt))\n",
    "    left_pad = torch.zeros(1,left_pad_amt) #+ (0.1**0.5)*torch.randn(1, left_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad shape = \" + str(left_pad.shape))\n",
    "    left_pad_mean_add = left_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_mean shape = \" + str(left_pad_mean_add))\n",
    "        print(\"sum of left pad mean add = \" + str(torch.sum(left_pad_mean_add)))\n",
    "    \n",
    "    right_pad_amt = int(rate*min_length-x_temp.shape[1]-left_pad_amt)\n",
    "    right_pad = torch.zeros(1,right_pad_amt)# + (0.1**0.5)*torch.randn(1, right_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"right_pad shape = \" + str(right_pad.shape))\n",
    "    right_pad_mean_add = right_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"right_pad_mean shape = \" + str(right_pad_mean_add))\n",
    "        print(\"sum of right pad mean add = \"  + str(torch.sum(right_pad_mean_add)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    f = torch.cat([left_pad,x_temp,right_pad],dim=1)[0]\n",
    "    f = f.unsqueeze(dim = 0)\n",
    "    #print(\"returning a tensor of shape = \" + str(f.shape))\n",
    "    return(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31f18fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7a03ecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_hat,y_true,classes):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_hat, y_true ,labels= range(len(classes)))\n",
    "    import seaborn as sns\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cellsplt.xticks(rotation=90)\n",
    "    ax.xaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    ax.xaxis.tick_bottom()\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.set_ylabel('True', fontsize=20)\n",
    "    ax.yaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6581314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d753135f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.92"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the min length based on config params\n",
    "min_length = (config.win_size * config.n_hop) / config.rate\n",
    "min_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a95fb3",
   "metadata": {},
   "source": [
    "### Class Defintions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6385d2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization():\n",
    "    \"\"\"This class is for normalizing the spectrograms batch by batch. The normalization used is min-max, two modes 'framewise' and 'imagewise' can be selected. In this paper, we found that 'imagewise' normalization works better than 'framewise'\"\"\"\n",
    "    def __init__(self, mode='framewise'):\n",
    "        if mode == 'framewise':\n",
    "            def normalize(x):\n",
    "                size = x.shape\n",
    "                x_max = x.max(1, keepdim=True)[0] # Finding max values for each frame\n",
    "                x_min = x.min(1, keepdim=True)[0]  \n",
    "                output = (x-x_min)/(x_max-x_min) # If there is a column with all zero, nan will occur\n",
    "                output[torch.isnan(output)]=0 # Making nan to 0\n",
    "                return output\n",
    "        elif mode == 'imagewise':\n",
    "            def normalize(x):\n",
    "                size = x.shape\n",
    "                x_max = x.reshape(size[0], size[1]*size[2]).max(1, keepdim=True)[0]\n",
    "                x_min = x.reshape(size[0], size[1]*size[2]).min(1, keepdim=True)[0]\n",
    "                x_max = x_max.unsqueeze(1) # Make it broadcastable\n",
    "                x_min = x_min.unsqueeze(1) # Make it broadcastable \n",
    "                return (x-x_min)/(x_max-x_min)\n",
    "        else:\n",
    "            print(f'please choose the correct mode')\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.normalize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9bdb5082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcen(x, eps=1e-6, s=0.025, alpha=0.98, delta=2, r=0.5, training=False):\n",
    "    frames = x.split(1, -2)\n",
    "    m_frames = []\n",
    "    last_state = None\n",
    "    for frame in frames:\n",
    "        if last_state is None:\n",
    "            last_state = s * frame\n",
    "            m_frames.append(last_state)\n",
    "            continue\n",
    "        if training:\n",
    "            m_frame = ((1 - s) * last_state).add_(s * frame)\n",
    "        else:\n",
    "            m_frame = (1 - s) * last_state + s * frame\n",
    "        last_state = m_frame\n",
    "        m_frames.append(m_frame)\n",
    "    M = torch.cat(m_frames, 1)\n",
    "    if training:\n",
    "        pcen_ = (x / (M + eps).pow(alpha) + delta).pow(r) - delta ** r\n",
    "    else:\n",
    "        pcen_ = x.div_(M.add_(eps).pow_(alpha)).add_(delta).pow_(r).sub_(delta ** r)\n",
    "    return pcen_\n",
    "\n",
    "\n",
    "class PCENTransform(nn.Module):\n",
    "\n",
    "    def __init__(self, eps=1e-6, s=0.025, alpha=0.98, delta=2, r=0.5, trainable=True):\n",
    "        super().__init__()\n",
    "        if trainable:\n",
    "            self.log_s = nn.Parameter(torch.log(torch.Tensor([s])))\n",
    "            self.log_alpha = nn.Parameter(torch.log(torch.Tensor([alpha])))\n",
    "            self.log_delta = nn.Parameter(torch.log(torch.Tensor([delta])))\n",
    "            self.log_r = nn.Parameter(torch.log(torch.Tensor([r])))\n",
    "        else:\n",
    "            self.s = s\n",
    "            self.alpha = alpha\n",
    "            self.delta = delta\n",
    "            self.r = r\n",
    "        self.eps = eps\n",
    "        self.trainable = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x.permute((0,2,1)).squeeze(dim=1)\n",
    "        if self.trainable:\n",
    "            x = pcen(x, self.eps, torch.exp(self.log_s), torch.exp(self.log_alpha), torch.exp(self.log_delta), torch.exp(self.log_r), self.training and self.trainable)\n",
    "        else:\n",
    "            x = pcen(x, self.eps, self.s, self.alpha, self.delta, self.r, self.training and self.trainable)\n",
    "#         x = x.unsqueeze(dim=1).permute((0,1,3,2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9a1348f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>offset</th>\n",
       "      <th>length</th>\n",
       "      <th>specie_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>221103</td>\n",
       "      <td>0</td>\n",
       "      <td>2.56</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>221103</td>\n",
       "      <td>2561</td>\n",
       "      <td>2.56</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>221149</td>\n",
       "      <td>0</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>221149</td>\n",
       "      <td>2561</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>221150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index      id  offset  length  specie_ind\n",
       "0      0  221103       0    2.56           7\n",
       "1      1  221103    2561    2.56           7\n",
       "2      2  221149       0    2.56           0\n",
       "3      3  221149    2561    2.56           0\n",
       "4      4  221150       0    2.56           0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_offset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d162fc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, loader, criterion,  classes = classes,device=None , call = \"val\"):\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"calling for ...\" +str(call))\n",
    "    with torch.no_grad():\n",
    "        if device is None:\n",
    "            torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        sigmoid = nn.Sigmoid()\n",
    "        test_loss = 0.0\n",
    "        model.eval()\n",
    "        if DEBUG:\n",
    "            print(\"inside test....\")\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        counter = 1\n",
    "        if DEBUG:\n",
    "            print(\"length of loader = \" + str(len(loader)))\n",
    "        for idx,(x,y) in enumerate(loader):\n",
    "            if DEBUG:\n",
    "                print(\"loader index = \" + str(idx))\n",
    "                            \n",
    "            x = x.to(device).float() \n",
    "            y = y.type(torch.LongTensor).to(device)\n",
    "            if DEBUG:\n",
    "                print(\"y = \" + str(y))\n",
    "            y_pred = model(x)['prediction']\n",
    "            preds = torch.argmax(y_pred, axis = 1)\n",
    "            y_pred_cpu = y_pred.cpu().detach()\n",
    "            if DEBUG:\n",
    "                print(\"y_pred_cpu = \" + str(y_pred_cpu))\n",
    "            preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "            if DEBUG:\n",
    "                print(\"preds = \" +str(preds))\n",
    "            all_y_pred.append(preds.cpu().detach())\n",
    "                                   \n",
    "            loss = criterion(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "            all_y.append(y.cpu().detach())\n",
    "            #all_y_pred.append(np.argmax(y_pred.cpu().detach().numpy()))\n",
    "            \n",
    "            del x\n",
    "            del y\n",
    "            del y_pred\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        if DEBUG:\n",
    "            print(\"inside test....\")\n",
    "            print(\"y = \" + str(all_y))\n",
    "            print(\"y_pred  = \" + str(all_y_pred))\n",
    "        \n",
    "        test_loss = test_loss/len(test_loader)\n",
    "        test_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')\n",
    "    \n",
    "    \n",
    "    return test_loss, test_f1 , all_y,all_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f2987df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(train_loader, val_loader, test_loader,model, classes ,class_weights ,num_epochs = num_epochs )\n",
    "def train_model(train_loader, val_loader,test_loader, model = None,  classes = classes,class_weights = class_weights,num_epochs = num_epochs ,n_channels = 1):\n",
    "    # Creates a GradScaler once at the beginning of training.\n",
    "    loss_scaler = NativeScaler()\n",
    "    global_step = 0\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Training on {device}')    \n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "\n",
    "    model = model.to(device)\n",
    "    weights_adj = torch.tensor(class_weights).type(torch.float).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights_adj)\n",
    "    optimiser = timm.optim.RAdam(model.parameters(), lr=config_pytorch.lr/10)\n",
    "    num_epochs = num_epochs\n",
    "    all_train_loss = []\n",
    "    all_train_f1 = []\n",
    "    all_val_loss = []\n",
    "    all_val_f1 = []\n",
    "    best_val_loss = np.inf\n",
    "    best_val_f1 = -np.inf\n",
    "    best_train_f1 = -np.inf\n",
    "    best_epoch = -1\n",
    "    checkpoint_name = None\n",
    "    overrun_counter = 0\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    lr_log = []\n",
    "    for e in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        tk0 = tqdm(train_loader, total=int(len(train_loader)))\n",
    "        for batch_i, inputs in enumerate(tk0):\n",
    "            if DEBUG:\n",
    "                print(\"inside train loop.. batch_ind = \" +str(batch_i))\n",
    "            if batch_i % 500 == 0:\n",
    "                bat_time = time.time()\n",
    "                durn = (bat_time - start_time)/60\n",
    "                print(\"epoch = \" +str(e) + \"batch = \" +str(batch_i) + \" of \" + str(len(train_loader)) + \"duraation = \" + str(durn))\n",
    "            x = inputs[0].to(device).float()\n",
    "            y = inputs[1].type(torch.LongTensor).to(device)\n",
    "            global_step += 1\n",
    "            optimiser.zero_grad()\n",
    "            # AMP\n",
    "            with autocast():\n",
    "                y_pred = model(x)['prediction']\n",
    "                preds = torch.argmax(y_pred, axis = 1)\n",
    "                if DEBUG:\n",
    "                    print(\"y_pred  = \" +str(y_pred))\n",
    "                    print(\"preds = \" +str(preds))\n",
    "                loss = criterion(y_pred, y)\n",
    "            loss_scaler(loss, optimiser,parameters=model_parameters(model))\n",
    "            train_loss += loss.item()\n",
    "            all_y.append(y.cpu().detach())\n",
    "            y_pred_cpu = y_pred.cpu().detach()\n",
    "            preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "            if DEBUG:\n",
    "                print(\"batch_ind = \" +str(batch_i))\n",
    "                print(\"y_pred_cpu = \" + str(y_pred_cpu))\n",
    "                \n",
    "            all_y_pred.append(preds.cpu().detach())\n",
    "            lr_log.append(optimiser.param_groups[0]['lr'])\n",
    "            tk0.set_postfix(training_loss=(train_loss / (batch_i+1)), lr=optimiser.param_groups[0]['lr'])\n",
    "            del x\n",
    "            del y\n",
    "            del y_pred,preds\n",
    "        \n",
    "        all_train_loss.append(train_loss/len(train_loader))\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        if DEBUG:\n",
    "            print(\"y = \" + str(all_y))\n",
    "            print(\"y_pred  = \" + str(all_y_pred))\n",
    "        \n",
    "        train_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')\n",
    "        if DEBUG:\n",
    "            print(\"train acc = \" +str(train_acc))\n",
    "        all_train_f1.append(train_f1)\n",
    "        val_loss, val_f1 , _,_ = test_model(model, val_loader, criterion = nn.CrossEntropyLoss(), classes = classes ,device=device, call = \"val\")\n",
    "        if DEBUG:\n",
    "            print(\"val F1 = \" + str(val_f1))\n",
    "        all_val_loss.append(val_loss)\n",
    "        all_val_f1.append(val_f1)\n",
    "        \n",
    "        acc_metric = val_f1\n",
    "        best_acc_metric = best_val_f1\n",
    "        if acc_metric > best_acc_metric:  \n",
    "            overrun_counter = -1\n",
    "            checkpoint_name = f'model_e{e}_{datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")}.pth'\n",
    "            torch.save(model.state_dict(), os.path.join(config.model_dir, 'pytorch', checkpoint_name))\n",
    "            print('Epoch: %d, Train Loss: %.8f, Train f1: %.8f, Val Loss: %.8f, Val f1: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader), train_f1, val_loss/len(val_loader), val_f1,  overrun_counter))\n",
    "            print('Saving model to:', os.path.join(config.model_dir, 'pytorch', checkpoint_name)) \n",
    "            print(\"Now printing classification rport... \")\n",
    "            print(\"********************************\")\n",
    "            from sklearn.metrics import classification_report\n",
    "            _, _ , all_y_test,all_y_pred_test = test_model(model, test_loader, criterion = nn.CrossEntropyLoss(), classes = classes ,device=device, call = \"test\")\n",
    "            print(classification_report(all_y_test.numpy(), all_y_pred_test.numpy(), target_names= classes))\n",
    "            print(\"********************************\")\n",
    "            plot_confusion_matrix(all_y_pred_test.numpy(), all_y_test.numpy() , classes)\n",
    "            best_epoch = e\n",
    "            best_val_f1 = val_f1\n",
    "            best_val_loss = val_loss\n",
    "            \n",
    "        else:\n",
    "            print(\"..Overrun....no improvement\")\n",
    "            overrun_counter += 1\n",
    "            print('Epoch: %d, Train Loss: %.8f, Train f1: %.8f, Val Loss: %.8f, Val f1: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader), train_f1, val_loss/len(val_loader), val_f1,  overrun_counter))\n",
    "        if overrun_counter > config_pytorch.max_overrun:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    return model, lr_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b690c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_audiomentations import Compose, Gain, PolarityInversion,AddColoredNoise,ApplyImpulseResponse,PeakNormalization\n",
    "#apply_augmentation = Compose(transforms=[PolarityInversion(p=0.5 ,output_type = 'tensor'),AddColoredNoise(), PeakNormalization(apply_to=\"only_too_loud_sounds\"),TimeInversion(output_type = 'tensor')  ])\n",
    "\n",
    "\n",
    "apply_augmentation = Compose(transforms=[AddColoredNoise(p = 1) ,TimeInversion( p = 1) ,PolarityInversion(p = 1)])\n",
    "\n",
    "#apply_augmentation = Compose(transforms=[PolarityInversion(p=0.5 ,output_type = 'tensor'),AddColoredNoise(), PeakNormalization(apply_to=\"only_too_loud_sounds\"),TimeInversion(output_type = 'tensor')  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c56ffe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MozTrainDataset(Dataset):\n",
    "\n",
    "    def __init__(self, audio_df, data_dir, min_length, cache=None, transform=apply_augmentation):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            noise_df (DataFrame): the df of noise files and lengths\n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        #self.noise_df = noise_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    \n",
    "    def _get_sample_(self, path, resample=None):\n",
    "        \n",
    "        waveform, inp_rate = torchaudio.load(path)\n",
    "        \n",
    "        if inp_rate != config.rate:\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_rate, config.rate, dtype=waveform.dtype)\n",
    "            waveform = resampler(waveform)\n",
    "    \n",
    "        \n",
    "        #waveform, rate = torchaudio.load(path)\n",
    "                \n",
    "        if waveform.shape[1] < config.rate*self.min_length:\n",
    "            #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "            f_out = pad_mean(waveform)\n",
    "        else:\n",
    "            f = waveform[0]\n",
    "            f_out = f.unsqueeze(0)\n",
    "            \n",
    "        return f_out\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #real_idx = idx % len(self.audio_df)\n",
    "        if DEBUG:\n",
    "            print(\"idx = \" + str(idx))\n",
    "        x = self._get_sample_(os.path.join(self.data_dir,f\"{int(self.audio_df.loc[idx]['id'])}.wav\"), resample=config.rate)\n",
    "        if idx%2 == 0:\n",
    "            if DEBUG:\n",
    "                print(\"shape of x = \" + str(x.shape))\n",
    "                print(\"applying augmentations...\")\n",
    "            #augmentation expect input in the form of [bat,chann,sample]\n",
    "            x_unsq = x.unsqueeze(dim = 1)\n",
    "            if DEBUG:\n",
    "                print(\"shape of x_unsq = \" + str(x_unsq.shape))\n",
    "            x = apply_augmentation(x_unsq, sample_rate=config.rate)\n",
    "            x = x.squeeze(dim = 1)\n",
    "            if DEBUG:\n",
    "                print(\"shape of x post augmentation = \" + str(x.shape))\n",
    "            \n",
    "                       \n",
    "        \n",
    "        # random noise on even number indexes\n",
    "        offset = int(self.audio_df.loc[idx]['offset'])\n",
    "        \n",
    "        return (x[:,offset:int(offset+config.rate*self.min_length)],self.audio_df.loc[idx]['specie_ind'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a325a36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MozTestDataset(Dataset):\n",
    "\n",
    "    def __init__(self, audio_df, data_dir, min_length, cache=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            noise_df (DataFrame): the df of noise files and lengths\n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        #self.noise_df = noise_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    \n",
    "    def _get_sample_(self, path, resample=None):\n",
    "        \n",
    "        waveform, inp_rate = torchaudio.load(path)\n",
    "        \n",
    "        if inp_rate != config.rate:\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_rate, config.rate, dtype=waveform.dtype)\n",
    "            waveform = resampler(waveform)\n",
    "    \n",
    "        \n",
    "        #waveform, rate = torchaudio.load(path)\n",
    "                \n",
    "        if waveform.shape[1] < config.rate*self.min_length:\n",
    "            #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "            f_out = pad_mean(waveform)\n",
    "        else:\n",
    "            f = waveform[0]\n",
    "            #mu = torch.std_mean(f)[1]\n",
    "            #st = torch.std_mean(f)[0]\n",
    "            # clip amplitudes\n",
    "            f_out = f.unsqueeze(0)\n",
    "            if self.cache is not None:\n",
    "                self.cache[path] = f_out\n",
    "        return f_out\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #real_idx = idx % len(self.audio_df)\n",
    "        if DEBUG:\n",
    "            print(\"idx = \" + str(idx))\n",
    "        x = self._get_sample_(os.path.join(self.data_dir,f\"{int(self.audio_df.loc[idx]['id'])}.wav\"), resample=config.rate)\n",
    "        \n",
    "        # random noise on even number indexes\n",
    "        offset = int(self.audio_df.loc[idx]['offset'])\n",
    "        \n",
    "        return (x[:,offset:int(offset+config.rate*self.min_length)],self.audio_df.loc[idx]['specie_ind'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802928ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "abbd2fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass the pretrained model and make it a binary classification\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, model_name, image_size):\n",
    "        super().__init__()\n",
    "        # num_classes=0 removes the pretrained head\n",
    "        self.backbone = timm.create_model(model_name,\n",
    "                        pretrained=True, num_classes=8, in_chans=1, \n",
    "                        drop_path_rate=0.05, global_pool='max',\n",
    "                        drop_rate=0.05)\n",
    "        #####  This section is model specific\n",
    "        #### It freezes some fo the layers by name\n",
    "        #### you'll have to inspect the model to see the names\n",
    "                #### end layer freezing\n",
    "        self.spec_layer = features.STFT(n_fft=config.NFFT, freq_bins=None, hop_length=config.n_hop,\n",
    "                              window='hann', freq_scale='linear', center=True, pad_mode='reflect',\n",
    "                           sr=config.rate, output_format=\"Magnitude\", trainable=True,)\n",
    "        self.out = nn.Linear(self.backbone.num_features, 1)\n",
    "        self.sizer = VT.Resize((image_size,image_size))\n",
    "        self.timeMasking = T.TimeMasking(time_mask_param=int(config.win_size*0.4), iid_masks=True)\n",
    "        self.freqMasking = T.FrequencyMasking(freq_mask_param=int((config.NFFT//4)*0.15), iid_masks=True)\n",
    "        self.norm_layer = Normalization(mode='framewise')\n",
    "        self.pcen_layer = PCENTransform(eps=1e-6, s=0.025, alpha=0.6, delta=0.1, r=0.2, trainable=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # first compute spectrogram\n",
    "        spec = self.spec_layer(x)  # (B, F, T)\n",
    "        # normalize\n",
    "#         spec = spec.transpose(1,2) # (B, T, F)\n",
    "        spec = self.pcen_layer(spec)\n",
    "        spec = self.norm_layer(spec)\n",
    "        \n",
    "#         if self.training:\n",
    "        spec = self.timeMasking(spec)\n",
    "        spec = self.freqMasking(spec)\n",
    "\n",
    "        # then size for CNN model\n",
    "        # and create a channel\n",
    "        spec = self.sizer(spec)\n",
    "        x = spec.unsqueeze(1)\n",
    "        # then repeat channels\n",
    "        x = self.backbone(x)\n",
    "        #print(\"x shape = \" + str(x.shape))\n",
    "        #print(\"x = \" +str(x))\n",
    "        #pred = nn.Softmax(x)\n",
    "        pred = x\n",
    "        #print(np.argmax(pred.detach().cpu().numpy()))\n",
    "        #print(pred)\n",
    "        output = {\"prediction\": pred,\n",
    "                  \"spectrogram\": spec}\n",
    "        #print(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "44adedea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = torch.rand(1,15360)\n",
    "\n",
    "# model =Model('convnext_small',224)\n",
    "# op = model(test)\n",
    "# print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "59293fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://discuss.pytorch.org/t/how-to-handle-imbalanced-classes/11264\n",
    "# from torch.utils.data.sampler import WeightedRandomSampler\n",
    "# class_sample_count = []\n",
    "# for i in range(0,len(classes)):\n",
    "#     df_temp = df_train_offset[df_train_offset['specie_ind'] == i]\n",
    "#     print(\"i = \" +str(i))\n",
    "#     print(len(df_temp))\n",
    "#     class_sample_count.append(len(df_temp))\n",
    "# print(\"class_sample_count = \" + str(class_sample_count))\n",
    "# class_sample_count_arr = np.array(class_sample_count)\n",
    "# weight = 1. / class_sample_count_arr\n",
    "# print(\"weight = \" +str(weight))\n",
    "# samples_weight = []\n",
    "# for t in range(len(classes)):\n",
    "#     samples_weight.append(weight[t])\n",
    "    \n",
    "# samples_weight = np.array(samples_weight)\n",
    "# print(\"samples_weight = \" +str(samples_weight))\n",
    "\n",
    "# samples_weight = torch.from_numpy(samples_weight)\n",
    "# samples_weigth = samples_weight.double()\n",
    "# sampler = WeightedRandomSampler(samples_weight, len(classes)*10000)\n",
    "  \n",
    "# class_sample_count = np.array( [len(np.where(target == t)[0]) for t in np.unique(target)])\n",
    "# weight = 1. / class_sample_count\n",
    "# samples_weight = np.array([weight[t] for t in target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4fa0e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MozTrainDataset(df_train_offset,  config.data_dir, min_length , transform = apply_augmentation)\n",
    "val_dataset = MozTestDataset(df_val_offset,  config.data_dir, min_length)\n",
    "test_dataset = MozTestDataset(df_test_offset,  config.data_dir, min_length)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, num_workers=num_workers,batch_size = batch_size,shuffle = True\n",
    "    , pin_memory=True )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size,\n",
    "        num_workers=num_workers, pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size,\n",
    "        num_workers= num_workers, pin_memory=pin_memory,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ed82e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d29bec78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset = 65825\n",
      "Length of train loader = 2058\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of train dataset = \" +str(len(train_dataset)))\n",
    "print(\"Length of train loader = \" +str(len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4bd12248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_iter = iter(val_loader)\n",
    "# a,b = test_iter.next()\n",
    "# print(a.shape)\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd259dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e66c61aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test block\n",
    "\n",
    "#temp_ten = torch.rand(64, 9, 1, 15360)\n",
    "# temp_ten.shape\n",
    "# bat_len = temp_ten.shape[0]\n",
    "# print(\"bat_len = \" +str(bat_len))\n",
    "# for i in range (bat_len):\n",
    "#     print(\"i = \" + str(i))\n",
    "#     elem = temp_ten[i,:,:,:]\n",
    "#     print(\"elem shape = \" +str(elem.shape))\n",
    "#     for j in range(elem.shape[0]):\n",
    "#         img = elem[j,:,:]\n",
    "#         print(\"img shape = \" +str(img.shape))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a76c3",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f49aed17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling rate = 8000. Please make sure the sampling rate is correct in order toget a valid freq range\n",
      "STFT kernels created, time used = 0.0786 seconds\n"
     ]
    }
   ],
   "source": [
    "def load_model(filepath, model=Model('convnext_small',224)):\n",
    "    # Instantiate model to inspect\n",
    "    print(\"Filepath = \" + str(filepath))\n",
    "    print(\"model = \" +str(model))\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "    print(f'Training on {device}')\n",
    "        \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "    model = model.to(device)\n",
    "    # Load trained parameters from checkpoint (may need to download from S3 first)\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        map_location=lambda storage, loc: storage.cuda()\n",
    "    else:\n",
    "        map_location='cpu'\n",
    "        \n",
    "    checkpoint = model.load_state_dict(torch.load(filepath))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "971c80c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling rate = 8000. Please make sure the sampling rate is correct in order toget a valid freq range\n",
      "STFT kernels created, time used = 0.0783 seconds\n",
      "Filepath = ../outputs/models/pytorch/model_e15_2022_09_20_02_13_07.pth\n",
      "model = Model(\n",
      "  (backbone): ConvNeXt(\n",
      "    (stem): Sequential(\n",
      "      (0): Conv2d(1, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "    (stages): Sequential(\n",
      "      (0): ConvNeXtStage(\n",
      "        (downsample): Identity()\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ConvNeXtStage(\n",
      "        (downsample): Sequential(\n",
      "          (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ConvNeXtStage(\n",
      "        (downsample): Sequential(\n",
      "          (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (3): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (4): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (5): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (6): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (7): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (8): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (9): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (10): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (11): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (12): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (13): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (14): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (15): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (16): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (17): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (18): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (19): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (20): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (21): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (22): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (23): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (24): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (25): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (26): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): ConvNeXtStage(\n",
      "        (downsample): Sequential(\n",
      "          (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm_pre): Identity()\n",
      "    (head): Sequential(\n",
      "      (global_pool): SelectAdaptivePool2d (pool_type=max, flatten=Identity())\n",
      "      (norm): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (drop): Dropout(p=0.05, inplace=False)\n",
      "      (fc): Linear(in_features=768, out_features=8, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (spec_layer): STFT(n_fft=2048, Fourier Kernel size=(1025, 1, 2048), iSTFT=False, trainable=True)\n",
      "  (out): Linear(in_features=768, out_features=1, bias=True)\n",
      "  (sizer): Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
      "  (timeMasking): TimeMasking()\n",
      "  (freqMasking): FrequencyMasking()\n",
      "  (pcen_layer): PCENTransform()\n",
      ")\n",
      "Training on cuda:0\n",
      "Training on cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f5562af3d54cea93a2e1ff991e62e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0batch = 0 of 2058duraation = 0.020770283540089925\n",
      "epoch = 0batch = 500 of 2058duraation = 3.1625597993532817\n",
      "epoch = 0batch = 1000 of 2058duraation = 6.293770440419515\n",
      "epoch = 0batch = 1500 of 2058duraation = 9.42163518667221\n",
      "epoch = 0batch = 2000 of 2058duraation = 12.548950592676798\n",
      "Epoch: 0, Train Loss: 0.68152843, Train f1: 0.74967911, Val Loss: 0.00134914, Val f1: 0.75739951, overrun_counter -1\n",
      "Saving model to: ../outputs/models/pytorch/model_e0_2022_09_20_05_00_30.pth\n",
      "Now printing classification rport... \n",
      "********************************\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "        an arabiensis       0.89      0.70      0.78      7271\n",
      "culex pipiens complex       0.74      0.71      0.72      4683\n",
      "           ae aegypti       0.68      0.81      0.74       789\n",
      "       an funestus ss       0.70      0.76      0.73      3632\n",
      "         an squamosus       0.50      0.76      0.60       870\n",
      "          an coustani       0.48      0.87      0.62       491\n",
      "         ma uniformis       0.49      0.62      0.55       731\n",
      "         ma africanus       0.46      0.73      0.56       395\n",
      "\n",
      "             accuracy                           0.72     18862\n",
      "            macro avg       0.62      0.75      0.66     18862\n",
      "         weighted avg       0.75      0.72      0.73     18862\n",
      "\n",
      "********************************\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a5fd5845f34922b2e5ddf2c47a633c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1batch = 0 of 2058duraation = 0.020965949694315592\n",
      "epoch = 1batch = 500 of 2058duraation = 3.170069722334544\n",
      "epoch = 1batch = 1000 of 2058duraation = 6.302395407358805\n",
      "epoch = 1batch = 1500 of 2058duraation = 9.445996181170146\n",
      "epoch = 1batch = 2000 of 2058duraation = 12.58817347685496\n",
      "..Overrun....no improvement\n",
      "Epoch: 1, Train Loss: 0.49946448, Train f1: 0.79432047, Val Loss: 0.00148545, Val f1: 0.73867489, overrun_counter 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b108c0be9a6140e198b2bcc4d8633ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 2batch = 0 of 2058duraation = 0.01963813304901123\n",
      "epoch = 2batch = 500 of 2058duraation = 3.161225219567617\n",
      "epoch = 2batch = 1000 of 2058duraation = 6.295922354857127\n",
      "epoch = 2batch = 1500 of 2058duraation = 9.423763187726339\n",
      "epoch = 2batch = 2000 of 2058duraation = 12.546259347597758\n",
      "..Overrun....no improvement\n",
      "Epoch: 2, Train Loss: 0.46405005, Train f1: 0.80652115, Val Loss: 0.00154278, Val f1: 0.73706395, overrun_counter 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42dc5371af643f8b298d1cda9dc7331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 3batch = 0 of 2058duraation = 0.021097429593404136\n",
      "epoch = 3batch = 500 of 2058duraation = 3.160792088508606\n",
      "epoch = 3batch = 1000 of 2058duraation = 6.2963059107462565\n",
      "epoch = 3batch = 1500 of 2058duraation = 9.432771865526835\n",
      "epoch = 3batch = 2000 of 2058duraation = 12.570151209831238\n",
      "..Overrun....no improvement\n",
      "Epoch: 3, Train Loss: 0.42378569, Train f1: 0.81915798, Val Loss: 0.00165180, Val f1: 0.72276921, overrun_counter 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a0102854cf4897916bf80a947e6b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 4batch = 0 of 2058duraation = 0.01973208983739217\n",
      "epoch = 4batch = 500 of 2058duraation = 3.16156226793925\n",
      "epoch = 4batch = 1000 of 2058duraation = 6.298768786589305\n",
      "epoch = 4batch = 1500 of 2058duraation = 9.431560587882995\n",
      "epoch = 4batch = 2000 of 2058duraation = 12.569248926639556\n",
      "..Overrun....no improvement\n",
      "Epoch: 4, Train Loss: 0.40544422, Train f1: 0.82778247, Val Loss: 0.00165073, Val f1: 0.71731357, overrun_counter 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b62960993341a6a12434c495965898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 5batch = 0 of 2058duraation = 0.019285110632578532\n",
      "epoch = 5batch = 500 of 2058duraation = 3.1599572896957397\n",
      "epoch = 5batch = 1000 of 2058duraation = 6.303181898593903\n",
      "epoch = 5batch = 1500 of 2058duraation = 9.434452438354493\n",
      "epoch = 5batch = 2000 of 2058duraation = 12.569148894151052\n",
      "..Overrun....no improvement\n",
      "Epoch: 5, Train Loss: 0.38271387, Train f1: 0.84122505, Val Loss: 0.00197586, Val f1: 0.68934452, overrun_counter 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83d25d8d8cf4f91b0f4e2c6c0082b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 6batch = 0 of 2058duraation = 0.019158411026000976\n",
      "epoch = 6batch = 500 of 2058duraation = 3.161738403638204\n",
      "epoch = 6batch = 1000 of 2058duraation = 6.2977621038754785\n",
      "epoch = 6batch = 1500 of 2058duraation = 9.426946914196014\n",
      "epoch = 6batch = 2000 of 2058duraation = 12.56478107770284\n",
      "..Overrun....no improvement\n",
      "Epoch: 6, Train Loss: 0.36620788, Train f1: 0.84544921, Val Loss: 0.00197019, Val f1: 0.68181937, overrun_counter 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e333568b604b3f8fb73fe2359894a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 7batch = 0 of 2058duraation = 0.01966073513031006\n",
      "epoch = 7batch = 500 of 2058duraation = 3.1604166905085247\n",
      "epoch = 7batch = 1000 of 2058duraation = 6.29698349237442\n",
      "epoch = 7batch = 1500 of 2058duraation = 9.425111214319864\n",
      "epoch = 7batch = 2000 of 2058duraation = 12.576581505934397\n",
      "..Overrun....no improvement\n",
      "Epoch: 7, Train Loss: 0.34795791, Train f1: 0.85214974, Val Loss: 0.00177023, Val f1: 0.68892703, overrun_counter 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55fade99e4d04a509d24a7d795ecc908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 8batch = 0 of 2058duraation = 0.02110231320063273\n",
      "epoch = 8batch = 500 of 2058duraation = 3.1603566964467364\n",
      "epoch = 8batch = 1000 of 2058duraation = 6.299421278635661\n",
      "epoch = 8batch = 1500 of 2058duraation = 9.43875640630722\n",
      "epoch = 8batch = 2000 of 2058duraation = 12.58191539446513\n",
      "..Overrun....no improvement\n",
      "Epoch: 8, Train Loss: 0.34261702, Train f1: 0.85390913, Val Loss: 0.00193977, Val f1: 0.69712638, overrun_counter 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e5bcd7e03a4088abb3115b0e37de16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 9batch = 0 of 2058duraation = 0.020280317465464274\n",
      "epoch = 9batch = 500 of 2058duraation = 3.1674617807070415\n",
      "epoch = 9batch = 1000 of 2058duraation = 6.301807530721029\n",
      "epoch = 9batch = 1500 of 2058duraation = 9.44540457725525\n",
      "epoch = 9batch = 2000 of 2058duraation = 12.589314035574596\n",
      "..Overrun....no improvement\n",
      "Epoch: 9, Train Loss: 0.33743148, Train f1: 0.85699347, Val Loss: 0.00190457, Val f1: 0.69317342, overrun_counter 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2aa30a243c433a8251e904387fdc98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 10batch = 0 of 2058duraation = 0.02129573424657186\n",
      "epoch = 10batch = 500 of 2058duraation = 3.1659567952156067\n",
      "epoch = 10batch = 1000 of 2058duraation = 6.303652711709341\n",
      "epoch = 10batch = 1500 of 2058duraation = 9.446571739514669\n",
      "epoch = 10batch = 2000 of 2058duraation = 12.589121798674265\n",
      "..Overrun....no improvement\n",
      "Epoch: 10, Train Loss: 0.32350343, Train f1: 0.86383927, Val Loss: 0.00218297, Val f1: 0.67232866, overrun_counter 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8100eec63e774a968d95c5ef300fdbb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 11batch = 0 of 2058duraation = 0.0206240177154541\n",
      "epoch = 11batch = 500 of 2058duraation = 3.158752918243408\n",
      "epoch = 11batch = 1000 of 2058duraation = 6.291513840357463\n",
      "epoch = 11batch = 1500 of 2058duraation = 9.43134438196818\n",
      "epoch = 11batch = 2000 of 2058duraation = 12.57442957162857\n",
      "..Overrun....no improvement\n",
      "Epoch: 11, Train Loss: 0.30949851, Train f1: 0.86547438, Val Loss: 0.00201457, Val f1: 0.68707380, overrun_counter 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8928f530d6485092e3dc9e3140ad6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 12batch = 0 of 2058duraation = 0.0205267071723938\n",
      "epoch = 12batch = 500 of 2058duraation = 3.1677361647288005\n",
      "epoch = 12batch = 1000 of 2058duraation = 6.306648445129395\n",
      "epoch = 12batch = 1500 of 2058duraation = 9.419455703099569\n",
      "epoch = 12batch = 2000 of 2058duraation = 12.542597119013468\n",
      "..Overrun....no improvement\n",
      "Epoch: 12, Train Loss: 0.29253587, Train f1: 0.87240569, Val Loss: 0.00223181, Val f1: 0.65847350, overrun_counter 11\n"
     ]
    }
   ],
   "source": [
    "model =Model('convnext_small',224)\n",
    "filepath = \"../outputs/models/pytorch/model_e15_2022_09_20_02_13_07.pth\"\n",
    "model_epcoh_15 = load_model(filepath,model)\n",
    "model, lr_log = train_model(train_loader, val_loader, test_loader,model_epcoh_15, classes ,class_weights ,num_epochs = num_epochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b734e746",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 9, does not match size of target_names, 8. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2418095/1820390444.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprediction\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mLabel\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   1985\u001b[0m             )\n\u001b[1;32m   1986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1987\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1988\u001b[0m                 \u001b[0;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1989\u001b[0m                 \u001b[0;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of classes, 9, does not match size of target_names, 8. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "prediction  = [2.0, 3.0, 8.0, 2.0, 8.0, 8.0, 4.0, 0.0, 7.0, 8.0, 8.0, 2.0, 8.0, 0.0, 1.0, 3.0, 4.0, 8.0, 5.0, 8.0, 2.0, 2.0, 2.0, 0.0, 5.0, 5.0, 3.0, 8.0, 4.0, 1.0, 2.0, 5.0, 6.0, 1.0, 8.0, 0.0, 2.0, 4.0, 6.0, 8.0, 7.0, 0.0, 7.0, 0.0, 8.0, 5.0, 8.0, 2.0, 0.0, 0.0, 4.0, 7.0, 5.0, 4.0, 1.0, 2.0, 4.0, 1.0, 6.0, 4.0, 3.0, 7.0, 8.0, 8.0, 0.0, 6.0, 7.0, 1.0, 6.0, 5.0, 7.0, 0.0, 5.0, 0.0, 0.0, 2.0, 1.0, 5.0, 8.0, 1.0, 5.0, 7.0, 5.0, 7.0, 3.0, 6.0, 6.0, 6.0, 2.0, 6.0, 2.0, 6.0, 6.0, 3.0, 6.0, 6.0, 0.0, 4.0, 6.0, 6.0, 0.0, 8.0, 7.0, 1.0, 4.0, 1.0, 3.0, 0.0, 8.0, 6.0, 5.0, 7.0, 7.0, 3.0, 2.0, 0.0, 4.0, 3.0, 4.0, 2.0, 4.0, 2.0, 7.0, 3.0, 1.0, 3.0, 6.0, 5.0, 5.0, 2.0, 0.0, 2.0, 0.0, 6.0, 3.0, 0.0, 3.0, 4.0, 8.0, 6.0, 4.0, 6.0, 0.0, 4.0, 5.0, 2.0, 6.0, 1.0, 1.0, 5.0, 4.0, 6.0, 5.0, 8.0, 0.0, 3.0, 4.0, 4.0, 4.0, 8.0, 8.0, 5.0, 5.0, 0.0, 1.0, 3.0, 3.0, 7.0, 7.0, 1.0, 5.0, 7.0, 6.0, 5.0, 8.0, 3.0, 8.0, 5.0, 2.0, 3.0, 7.0, 3.0, 7.0, 8.0, 4.0, 2.0, 0.0, 6.0, 8.0, 1.0, 3.0, 6.0, 2.0, 2.0, 7.0, 7.0, 2.0, 0.0, 4.0, 8.0]\n",
    "Label   = [3.0, 5.0, 0.0, 0.0, 2.0, 0.0, 1.0, 3.0, 4.0, 7.0, 1.0, 3.0, 2.0, 1.0, 4.0, 1.0, 3.0, 8.0, 8.0, 8.0, 3.0, 4.0, 7.0, 6.0, 4.0, 1.0, 3.0, 0.0, 7.0, 7.0, 3.0, 1.0, 4.0, 3.0, 5.0, 3.0, 4.0, 2.0, 8.0, 8.0, 7.0, 4.0, 7.0, 2.0, 6.0, 1.0, 8.0, 4.0, 8.0, 0.0, 1.0, 4.0, 2.0, 2.0, 0.0, 8.0, 1.0, 7.0, 4.0, 1.0, 2.0, 1.0, 2.0, 3.0, 6.0, 3.0, 6.0, 5.0, 4.0, 0.0, 5.0, 8.0, 5.0, 7.0, 1.0, 1.0, 6.0, 8.0, 6.0, 6.0, 4.0, 6.0, 6.0, 6.0, 8.0, 4.0, 5.0, 8.0, 0.0, 7.0, 4.0, 1.0, 6.0, 1.0, 5.0, 5.0, 5.0, 3.0, 8.0, 7.0, 8.0, 1.0, 2.0, 7.0, 2.0, 0.0, 4.0, 3.0, 2.0, 5.0, 1.0, 1.0, 2.0, 3.0, 0.0, 4.0, 7.0, 5.0, 2.0, 2.0, 6.0, 7.0, 2.0, 2.0, 6.0, 4.0, 2.0, 5.0, 8.0, 3.0, 6.0, 1.0, 1.0, 6.0, 5.0, 0.0, 2.0, 5.0, 3.0, 3.0, 0.0, 3.0, 3.0, 3.0, 1.0, 4.0, 3.0, 0.0, 6.0, 3.0, 7.0, 1.0, 2.0, 7.0, 3.0, 6.0, 1.0, 3.0, 8.0, 3.0, 6.0, 1.0, 4.0, 4.0, 6.0, 2.0, 5.0, 3.0, 0.0, 5.0, 7.0, 3.0, 5.0, 6.0, 4.0, 7.0, 1.0, 1.0, 6.0, 1.0, 4.0, 5.0, 8.0, 4.0, 6.0, 4.0, 5.0, 1.0, 6.0, 0.0, 3.0, 3.0, 3.0, 4.0, 7.0, 5.0, 2.0, 6.0, 2.0, 4.0]\n",
    "print(classification_report(np.array(Label), np.array(prediction), target_names= classes))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "classes\n",
    "cm = confusion_matrix(prediction, Label ,labels= range(0,9))\n",
    "print(cm)\n",
    "import seaborn as sns\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cellsplt.xticks(rotation=90)\n",
    "ax.xaxis.set_ticklabels(classes, fontsize = 10)\n",
    "ax.xaxis.tick_bottom()\n",
    "plt.xticks(rotation=90)\n",
    "ax.set_ylabel('True', fontsize=20)\n",
    "ax.yaxis.set_ticklabels(classes, fontsize = 10)\n",
    "plt.yticks(rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5d980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "pred = []\n",
    "for i in range(10):\n",
    "    label.append(np.random.rand(9))\n",
    "    pred.append(np.random.rand(9))\n",
    "print(label)\n",
    "print(pred)\n",
    "print(classification_report(label, pred, target_names= classes, labels= classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c580ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.tensor(8, device = \"cuda\")\n",
    "print(label)\n",
    "label_cpu = label.cpu().detach()\n",
    "print(label_cpu)\n",
    "label_np = label_cpu.numpy()\n",
    "print(type(label_np))\n",
    "label_np_item = label_np.item()\n",
    "print(type(label_np_item))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f8f23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.randn(4,9)\n",
    "y_pred.shape\n",
    "#y_pred_np = y_pred.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8867ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_np\n",
    "# y_pred_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f981e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.argmax(y_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b2505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9de668",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4f57c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,(x,y) in enumerate(test_loader):\n",
    "    print(\"idx = \" + str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb63e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6268fa46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed84cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
