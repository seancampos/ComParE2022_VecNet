{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49315c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html\n",
      "Requirement already satisfied: torch==1.11.0+cu113 in /opt/conda/lib/python3.8/site-packages (1.11.0+cu113)\n",
      "Requirement already satisfied: torchvision==0.12.0+cu113 in /opt/conda/lib/python3.8/site-packages (0.12.0+cu113)\n",
      "Requirement already satisfied: torchaudio==0.11.0+cu113 in /opt/conda/lib/python3.8/site-packages (0.11.0+cu113)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.11.0+cu113) (4.0.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.8/site-packages (from torchvision==0.12.0+cu113) (9.1.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from torchvision==0.12.0+cu113) (2.26.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchvision==0.12.0+cu113) (1.21.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision==0.12.0+cu113) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision==0.12.0+cu113) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision==0.12.0+cu113) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision==0.12.0+cu113) (2.0.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# I had to find the right version of pytorch with the widget here https://pytorch.org/\n",
    "# I *think* this will work with AWS\n",
    "!pip3 install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49543289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: timm in /opt/conda/lib/python3.8/site-packages (0.5.4)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (7.7.0)\n",
      "Requirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.8/site-packages (from timm) (1.11.0+cu113)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (from timm) (0.12.0+cu113)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.4.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (6.15.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.3.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (1.1.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.6.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.8.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.1.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (21.3)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: debugpy>=1.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.5)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.22)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (4.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (4.6.0)\n",
      "Requirement already satisfied: fastjsonschema in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (2.15.3)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (5.8.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (21.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (3.8.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.4->timm) (4.0.1)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.8/site-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.4.12)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.15.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.14.1)\n",
      "Requirement already satisfied: nbconvert>=5 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.5.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.3.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.0.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.6.4)\n",
      "Requirement already satisfied: tinycss2 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.10.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.8/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.3)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel>=4.5.1->ipywidgets) (3.0.6)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchvision->timm) (1.21.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from torchvision->timm) (2.26.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.8/site-packages (from torchvision->timm) (9.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->timm) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->timm) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->timm) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->timm) (2022.6.15)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# other dependencies\n",
    "!pip install timm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ef18da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting git+https://github.com/KinWaiCheuk/nnAudio.git#subdirectory=Installation\n",
      "  Cloning https://github.com/KinWaiCheuk/nnAudio.git to /tmp/pip-req-build-tcx1o9kn\n",
      "  Running command git clone -q https://github.com/KinWaiCheuk/nnAudio.git /tmp/pip-req-build-tcx1o9kn\n",
      "  Resolved https://github.com/KinWaiCheuk/nnAudio.git to commit 744fab12497a5316153978de2e97422c9c7389e0\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from nnAudio==0.3.1) (1.6.3)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /opt/conda/lib/python3.8/site-packages (from scipy->nnAudio==0.3.1) (1.21.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## nnAudio\n",
    "!pip install git+https://github.com/KinWaiCheuk/nnAudio.git#subdirectory=Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bd10b3",
   "metadata": {},
   "source": [
    "### 1 Import the kitchen sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ebaabb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
     ]
    }
   ],
   "source": [
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a992a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug main imports\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../lib'))\n",
    "import config\n",
    "from evaluate import get_results\n",
    "import numpy as np\n",
    "\n",
    "# Troubleshooting and visualisation\n",
    "import IPython.display as ipd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76d683ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug lib imports\n",
    "from sklearn.metrics import accuracy_score\n",
    "from PyTorch import config_pytorch\n",
    "from datetime import datetime\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6312c175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional pytorch tools\n",
    "import random\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchvision.transforms as VT\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "import timm\n",
    "import timm.optim\n",
    "from timm.loss import BinaryCrossEntropy\n",
    "from timm.utils import NativeScaler\n",
    "from timm.models import model_parameters\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73657390",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nnAudio\n",
    "from nnAudio import features\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7531480c",
   "metadata": {},
   "source": [
    "### Run all these function definition cells\n",
    "These have been extracted from the lib folder and are here to make them more easily editable.  Most of the action happens in *get_feat_torch*, which does feature extraction and *train_model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d7915f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filepath, model):\n",
    "    # Instantiate model to inspect\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "    print(f'Training on {device}')\n",
    "        \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "    model = model.to(device)\n",
    "    # Load trained parameters from checkpoint (may need to download from S3 first)\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        map_location=lambda storage, loc: storage.cuda()\n",
    "    else:\n",
    "        map_location='cpu'\n",
    "        \n",
    "    checkpoint = model.load_state_dict(torch.load(filepath))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f650c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion, class_threshold=0.5, device=None):\n",
    "    with torch.no_grad():\n",
    "        if device is None:\n",
    "            torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        sigmoid = nn.Sigmoid()\n",
    "        test_loss = 0.0\n",
    "        model.eval()\n",
    "        \n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        counter = 1\n",
    "        for x, y, idx in tqdm(test_loader, desc='validation', leave=True):\n",
    "            \n",
    "            x, y = x.to(device), y.unsqueeze(1).float().to(device)\n",
    "            \n",
    "            y_pred = model(x)['prediction']\n",
    "                        \n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            all_y.append(y.cpu().detach())\n",
    "            all_y_pred.append(y_pred.cpu().detach())\n",
    "            \n",
    "            del x\n",
    "            del y\n",
    "            del y_pred\n",
    "            \n",
    "            counter +=1\n",
    "\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        \n",
    "        test_loss = test_loss/len(test_loader)\n",
    "        test_acc = accuracy_score(all_y.numpy(), (sigmoid(all_y_pred).numpy() > class_threshold).astype(float))\n",
    "    \n",
    "    \n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "224fb99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, val_loader, model = None, n_channels = 1):\n",
    "    # Creates a GradScaler once at the beginning of training.\n",
    "    loss_scaler = NativeScaler()\n",
    "    global_step = 0\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Training on {device}')    \n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Change compatibility to other loss function, cross-test with main.\n",
    "    #criterion = nn.BCEWithLogitsLoss()\n",
    "    criterion = BinaryCrossEntropy(smoothing=0.1)\n",
    "    #optimiser = optim.Adam(model.parameters(), lr=config_pytorch.lr)\n",
    "#     optimiser = timm.optim.AdamP(model.parameters(), lr=config_pytorch.lr/10)\n",
    "    optimiser = timm.optim.RAdam(model.parameters(), lr=config_pytorch.lr/10)\n",
    "    #optimiser = timm.optim.Lookahead(optimiser)\n",
    "    \n",
    "    num_steps = len(train_loader)\n",
    "    num_epochs = config_pytorch.epochs\n",
    "    \n",
    "    # 2 cycle CosineLR\n",
    "#     scheduler = scheduler = CosineLRScheduler(\n",
    "#             optimiser,\n",
    "#             t_initial= num_steps * (num_epochs // 2) + 1,\n",
    "#             lr_min=5e-7,\n",
    "#             noise_range_t=(0, num_steps*(num_epochs // 2)),\n",
    "#             noise_pct=0.1,\n",
    "#             warmup_lr_init=5e-7,\n",
    "#             warmup_t= num_steps * config_pytorch.n_warmup_epochs + 1, cycle_limit=20)\n",
    "    \n",
    "\n",
    "\n",
    "    all_train_loss = []\n",
    "    all_train_acc = []\n",
    "    all_val_loss = []\n",
    "    all_val_acc = []\n",
    "    best_val_loss = np.inf\n",
    "    best_val_acc = -np.inf\n",
    "\n",
    "    # best_train_loss = np.inf\n",
    "    best_train_acc = -np.inf\n",
    "\n",
    "    best_epoch = -1\n",
    "    checkpoint_name = None\n",
    "    overrun_counter = 0\n",
    "    \n",
    "    sigmoid = nn.Sigmoid()\n",
    "    lr_log = []\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        tk0 = tqdm(train_loader, total=int(len(train_loader)))\n",
    "        for batch_i, inputs in enumerate(tk0):\n",
    "#             scheduler.step(global_step)\n",
    "\n",
    "            x = inputs[0].to(device)\n",
    "            y = inputs[1].unsqueeze(1).float().to(device)\n",
    "\n",
    "            global_step += 1\n",
    "            \n",
    "            if len(x) == 1:\n",
    "                x = x[0]\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            # AMP\n",
    "            with autocast():\n",
    "                # outputs = model(inputs)\n",
    "                y_pred = model(x)['prediction']\n",
    "                loss = criterion(y_pred, y)\n",
    "                \n",
    "            loss_scaler(\n",
    "                loss, optimiser,\n",
    "                parameters=model_parameters(model))\n",
    "    \n",
    "#             scaler.scale(loss).backward()\n",
    "#             scaler.step(optimiser)\n",
    "#           # Updates the scale for next iteration.\n",
    "#             scaler.update()\n",
    "\n",
    "#             loss.backward()\n",
    "#             optimiser.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            all_y.append(y.cpu().detach())\n",
    "            all_y_pred.append(y_pred.cpu().detach())\n",
    "            \n",
    "            lr_log.append(optimiser.param_groups[0]['lr'])\n",
    "            \n",
    "            tk0.set_postfix(training_loss=(train_loss / (batch_i+1)), lr=optimiser.param_groups[0]['lr'])\n",
    "            \n",
    "            del x\n",
    "            del y\n",
    "        \n",
    "#         optimiser.sync_lookahead()\n",
    "        \n",
    "        all_train_loss.append(train_loss/len(train_loader))\n",
    "\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        train_acc = accuracy_score(all_y.numpy(), (all_y_pred.numpy() > 0.0).astype(float))\n",
    "        all_train_acc.append(train_acc)\n",
    "\n",
    "\n",
    "        # Can add more conditions to support loss instead of accuracy. Use *-1 for loss inequality instead of acc\n",
    "        val_loss, val_acc = test_model(model, val_loader, criterion, 0.5, device=device)\n",
    "        all_val_loss.append(val_loss)\n",
    "        all_val_acc.append(val_acc)\n",
    "\n",
    "        acc_metric = val_acc\n",
    "        best_acc_metric = best_val_acc\n",
    "        \n",
    "        if acc_metric > best_acc_metric:  \n",
    "            # if checkpoint_name is not None:\n",
    "                # os.path.join(os.path.pardir, 'models', 'pytorch', checkpoint_name)\n",
    "\n",
    "            checkpoint_name = f'model_e{e}_{datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")}.pth'\n",
    "\n",
    "            torch.save(model.state_dict(), os.path.join(config.model_dir, 'pytorch', checkpoint_name))\n",
    "            print('Saving model to:', os.path.join(config.model_dir, 'pytorch', checkpoint_name)) \n",
    "            best_epoch = e\n",
    "#             best_train_acc = train_acc\n",
    "#             best_train_loss = train_loss\n",
    "#             if x_val is not None:\n",
    "            best_val_acc = val_acc\n",
    "            best_val_loss = val_loss\n",
    "            overrun_counter = -1\n",
    "\n",
    "        overrun_counter += 1\n",
    "        #if x_val is not None:\n",
    "        print('Epoch: %d, Train Loss: %.8f, Train Acc: %.8f, Val Loss: %.8f, Val Acc: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader), train_acc, val_loss/len(val_loader), val_acc,  overrun_counter))\n",
    "#         else:\n",
    "#             print('Epoch: %d, Train Loss: %.8f, Train Acc: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader), train_acc, overrun_counter))\n",
    "        if overrun_counter > config_pytorch.max_overrun:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    return model, lr_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "437be538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, n_samples, n_channels=3):\n",
    "    with torch.no_grad():\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f'Evaluating on {device}')\n",
    "\n",
    "\n",
    "        y_preds_all = np.zeros([n_samples, len(test_loader.dataset), 2])\n",
    "        model.eval() # Important to not leak info from batch norm layers and cause other issues\n",
    "        \n",
    "        sigmoid = nn.Sigmoid()\n",
    "\n",
    "        for n in range(n_samples):\n",
    "            all_y_pred = []\n",
    "            all_y = []\n",
    "            for x, y, idx in tqdm(test_loader):\n",
    "                x, y = x.to(device), y.unsqueeze(1).float().to(device)\n",
    "\n",
    "                y_pred = model(x)['prediction']\n",
    "                all_y.append(y.cpu().detach())\n",
    "\n",
    "                all_y_pred.append(y_pred.cpu().detach())\n",
    "\n",
    "                del x\n",
    "                del y\n",
    "                del y_pred\n",
    "\n",
    "            all_y_pred = torch.cat(all_y_pred)\n",
    "            all_y = torch.cat(all_y)\n",
    "\n",
    "            y_preds_all[n,:,1] = np.array(sigmoid(all_y_pred).squeeze())\n",
    "            y_preds_all[n,:,0] = 1-np.array(sigmoid(all_y_pred).squeeze()) # Check ordering of classes (yes/no)\n",
    "            test_acc = accuracy_score(all_y.numpy(), (sigmoid(all_y_pred).numpy() > 0.5).astype(float))\n",
    "            print(test_acc)\n",
    "    return y_preds_all, all_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83691205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offsets_df(df, short_audio=False):\n",
    "    audio_offsets = []\n",
    "    min_length = config.win_size*config.NFFT/(((1/config.n_hop)*config.NFFT)*config.rate)\n",
    "    step_frac = config.step_size/config.win_size\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        if row['length'] > min_length:\n",
    "            step_size = step_frac*min_length\n",
    "            audio_offsets.append({'index':row.name,'id':row['id'], 'offset':0,'sound_type': row['sound_type'], 'length': row['length']})\n",
    "            for i in range(1, int((row['length']-min_length)//step_size)):\n",
    "                audio_offsets.append({'index':row.name, 'id': row['id'], 'offset':int(min_length+(i*step_size)*config.rate),'sound_type':row['sound_type'], 'length': row['length']})\n",
    "        elif short_audio:\n",
    "            audio_offsets.append({'index':row.name,'id':row['id'], 'offset':0,'sound_type': row['sound_type'], 'length': min_length})\n",
    "    return pd.DataFrame(audio_offsets)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5521acbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be01aa16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.92"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the min length based on config params\n",
    "min_length = (config.win_size * config.n_hop) / config.rate\n",
    "min_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70cc5ff",
   "metadata": {},
   "source": [
    "### 3 The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011ec2c0",
   "metadata": {},
   "source": [
    "### Read CSV and get train/test groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bc6e5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config.data_df)\n",
    "\n",
    "# To be kept: please do not edit the test set: these paths select test set A, test set B as described in the paper\n",
    "idx_test_A = np.logical_and(df['country'] == 'Tanzania', df['location_type'] == 'field')\n",
    "idx_test_B = np.logical_and(df['country'] == 'UK', df['location_type'] == 'culture')\n",
    "idx_train = np.logical_not(np.logical_or(idx_test_A, idx_test_B))\n",
    "df_test_A = df[idx_test_A]\n",
    "df_test_B = df[idx_test_B]\n",
    "\n",
    "\n",
    "df_train = df[idx_train]\n",
    "\n",
    "# Modify by addition or sub-sampling of df_train here\n",
    "# df_train ... \n",
    "\n",
    "# Assertion to check that train does NOT appear in test:\n",
    "assert len(np.where(pd.concat([df_train,df_test_A,\n",
    "                               df_test_B]).duplicated())[0]) == 0, 'Train dataframe contains overlap with Test A, Test B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aed1b623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d835cfe7f14948ebb08a54014c64d6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# come on feel the noise\n",
    "\n",
    "# first get the length of all the MUSAN noises https://www.openslr.org/17/ --> data/noise\n",
    "files_noise = glob('../data/noise/**/*.wav')+glob('../data/audioset/*.wav')\n",
    "df_noise = pd.DataFrame([{'path': f, 'length': torchaudio.sox_effects.apply_effects_file(f, effects=[[\"rate\", f'{config.rate}']])[0].shape[1]/config.rate} for f in tqdm(files_noise)])\n",
    "# make the noise in the training data match the format\n",
    "df_noise_train = df_train[(df_train['sound_type']!='mosquito')][['id','length']]\n",
    "df_noise_train = df_noise_train[df_noise_train['length']>min_length].sample(frac=0.3)\n",
    "df_noise_train['path'] = df_noise_train['id'].apply(lambda x: os.path.join(config.data_dir,f'{x}.wav'))\n",
    "df_noise = pd.concat([df_noise,df_noise_train[['path','length']]],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad5a326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use audio clips shorter than min length ?\n",
    "USE_SHORT_AUDIO = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbbfe8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d8b2868c97b4dc6b902294ebd7d461d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7934 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104305d412a6484fa0a1ae5372bc1c12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c92141b1f8347448320c9a08113eb9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the frame offsets for each audio file into dataframes\n",
    "audio_df_train = get_offsets_df(df_train, short_audio=USE_SHORT_AUDIO)\n",
    "audio_df_test_A = get_offsets_df(df_test_A, short_audio=False)\n",
    "audio_df_test_B = get_offsets_df(df_test_B, short_audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e6bb068",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MozTrainDataset(Dataset):\n",
    "\n",
    "    def __init__(self, audio_df, noise_df, data_dir, min_length, cache=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            noise_df (DataFrame): the df of noise files and lengths\n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        self.noise_df = noise_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    \n",
    "    def _get_sample_(self, path, resample=None):\n",
    "        if self.cache is not None and path in self.cache:\n",
    "            return self.cache[path], resample\n",
    "        else:\n",
    "            effects = [\n",
    "                [\"remix\", \"1\"]\n",
    "            ]\n",
    "            if resample:\n",
    "                effects.extend(\n",
    "                  [[\"rate\", f'{resample}'],\n",
    "                  ['gain', '-n'],\n",
    "#                 [\"highpass\", \"600\"],\n",
    "                ])\n",
    "\n",
    "            waveform, rate = torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n",
    "            if waveform.shape[1] < config.rate*self.min_length:\n",
    "                r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "                f = torch.cat([waveform for _ in range(r)],dim=1)[:,:int(config.rate*self.min_length)][0]\n",
    "            else:    \n",
    "                f = waveform[0]\n",
    "            mu = torch.std_mean(f)[1]\n",
    "            st = torch.std_mean(f)[0]\n",
    "            # clip amplitudes\n",
    "            f_out = torch.clamp(f, min=mu-st*3, max=mu+st*3).unsqueeze(0)\n",
    "            if self.cache is not None:\n",
    "                self.cache[path] = f_out\n",
    "        return f_out, rate\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = idx % len(self.audio_df)\n",
    "        x, _ = self._get_sample_(os.path.join(self.data_dir,f\"{self.audio_df.loc[real_idx]['id']}.wav\"), resample=config.rate)\n",
    "        \n",
    "        # random noise on even number indexes\n",
    "        if real_idx % 2 == 0:\n",
    "            sample_length = self.audio_df.loc[real_idx]['length']\n",
    "            if len(self.noise_df[self.noise_df['length'] >= sample_length]) > 0:\n",
    "                noise_path = self.noise_df[self.noise_df['length'] >= sample_length].sample(1, random_state=idx).iloc[0]['path']\n",
    "                noise, _ = self._get_sample_(noise_path, resample=config.rate)\n",
    "                                   \n",
    "                noise = noise[:, :x.shape[1]]\n",
    "\n",
    "                mos_power = x.norm(p=2)\n",
    "                noise_power = noise.norm(p=2)\n",
    "\n",
    "                snr = math.exp(random.randrange(1, 5) / 10)\n",
    "                scale = snr * noise_power / mos_power\n",
    "                x = (scale * x / 10 + noise) / 2\n",
    "        \n",
    "        offset = self.audio_df.loc[real_idx]['offset']\n",
    "        return (x[:,offset:int(offset+config.rate*self.min_length)], 1 if self.audio_df.loc[real_idx]['sound_type'] == 'mosquito' else 0, idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d079067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MozTestDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, audio_df, data_dir, min_length, cache=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    \n",
    "    def _get_sample_(self, path, resample=None):\n",
    "        if self.cache is not None and path in self.cache:\n",
    "            return self.cache[path], resample\n",
    "        else:\n",
    "            effects = [\n",
    "                [\"remix\", \"1\"]\n",
    "            ]\n",
    "            if resample:\n",
    "                effects.extend([                 \n",
    "                  [\"rate\", f'{resample}'],\n",
    "                  ['gain', '-n'],\n",
    "#                 [\"highpass\", \"600\"],\n",
    "                ])\n",
    "            waveform, rate = torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n",
    "            f = waveform[0]\n",
    "            mu = torch.std_mean(f)[1]\n",
    "            st = torch.std_mean(f)[0]\n",
    "            #return waveform, rate, waveform\n",
    "            f_out = torch.clamp(f, min=mu-st*3, max=mu+st*3).unsqueeze(0)\n",
    "            if self.cache is not None:\n",
    "                self.cache[path] = f_out\n",
    "        return f_out, rate\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = idx % len(self.audio_df)\n",
    "        x, _ = self._get_sample_(os.path.join(self.data_dir,f\"{self.audio_df.loc[real_idx]['id']}.wav\"), resample=config.rate)\n",
    "                                   \n",
    "        offset = self.audio_df.loc[real_idx]['offset']\n",
    "        return x[:,offset:int(offset+config.rate*self.min_length)], 1 if self.audio_df.loc[real_idx]['sound_type'] == 'mosquito' else 0, idx\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6710e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty cache\n",
    "# it would be great if this was something cooler than a dictionary\n",
    "audio_cache={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b805445",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_pytorch.batch_size = 128\n",
    "config_pytorch.epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57716ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 0.2\n",
    "shuffle = True\n",
    "random_seed = 42\n",
    "min_length = (config.win_size * config.n_hop) / config.rate\n",
    "# needed for cache\n",
    "\n",
    "num_workers=4\n",
    "pin_memory=False\n",
    "\n",
    "num_train = len(audio_df_train)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "if shuffle:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_dataset = MozTrainDataset(audio_df_train, df_noise, config.data_dir, min_length, audio_cache)\n",
    "\n",
    "val_dataset = MozTrainDataset(audio_df_train, df_noise, config.data_dir, min_length, audio_cache)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=config_pytorch.batch_size, sampler=train_sampler,\n",
    "        num_workers=num_workers, pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=config_pytorch.batch_size, sampler=valid_sampler,\n",
    "        num_workers=num_workers, pin_memory=pin_memory,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9e30be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization():\n",
    "    \"\"\"This class is for normalizing the spectrograms batch by batch. The normalization used is min-max, two modes 'framewise' and 'imagewise' can be selected. In this paper, we found that 'imagewise' normalization works better than 'framewise'\"\"\"\n",
    "    def __init__(self, mode='framewise'):\n",
    "        if mode == 'framewise':\n",
    "            def normalize(x):\n",
    "                size = x.shape\n",
    "                x_max = x.max(1, keepdim=True)[0] # Finding max values for each frame\n",
    "                x_min = x.min(1, keepdim=True)[0]  \n",
    "                output = (x-x_min)/(x_max-x_min) # If there is a column with all zero, nan will occur\n",
    "                output[torch.isnan(output)]=0 # Making nan to 0\n",
    "                return output\n",
    "        elif mode == 'imagewise':\n",
    "            def normalize(x):\n",
    "                size = x.shape\n",
    "                x_max = x.reshape(size[0], size[1]*size[2]).max(1, keepdim=True)[0]\n",
    "                x_min = x.reshape(size[0], size[1]*size[2]).min(1, keepdim=True)[0]\n",
    "                x_max = x_max.unsqueeze(1) # Make it broadcastable\n",
    "                x_min = x_min.unsqueeze(1) # Make it broadcastable \n",
    "                return (x-x_min)/(x_max-x_min)\n",
    "        else:\n",
    "            print(f'please choose the correct mode')\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.normalize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "674a4117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcen(x, eps=1e-6, s=0.025, alpha=0.98, delta=2, r=0.5, training=False):\n",
    "    frames = x.split(1, -2)\n",
    "    m_frames = []\n",
    "    last_state = None\n",
    "    for frame in frames:\n",
    "        if last_state is None:\n",
    "            last_state = s * frame\n",
    "            m_frames.append(last_state)\n",
    "            continue\n",
    "        if training:\n",
    "            m_frame = ((1 - s) * last_state).add_(s * frame)\n",
    "        else:\n",
    "            m_frame = (1 - s) * last_state + s * frame\n",
    "        last_state = m_frame\n",
    "        m_frames.append(m_frame)\n",
    "    M = torch.cat(m_frames, 1)\n",
    "    if training:\n",
    "        pcen_ = (x / (M + eps).pow(alpha) + delta).pow(r) - delta ** r\n",
    "    else:\n",
    "        pcen_ = x.div_(M.add_(eps).pow_(alpha)).add_(delta).pow_(r).sub_(delta ** r)\n",
    "    return pcen_\n",
    "\n",
    "\n",
    "class PCENTransform(nn.Module):\n",
    "\n",
    "    def __init__(self, eps=1e-6, s=0.025, alpha=0.98, delta=2, r=0.5, trainable=True):\n",
    "        super().__init__()\n",
    "        if trainable:\n",
    "            self.log_s = nn.Parameter(torch.log(torch.Tensor([s])))\n",
    "            self.log_alpha = nn.Parameter(torch.log(torch.Tensor([alpha])))\n",
    "            self.log_delta = nn.Parameter(torch.log(torch.Tensor([delta])))\n",
    "            self.log_r = nn.Parameter(torch.log(torch.Tensor([r])))\n",
    "        else:\n",
    "            self.s = s\n",
    "            self.alpha = alpha\n",
    "            self.delta = delta\n",
    "            self.r = r\n",
    "        self.eps = eps\n",
    "        self.trainable = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x.permute((0,2,1)).squeeze(dim=1)\n",
    "        if self.trainable:\n",
    "            x = pcen(x, self.eps, torch.exp(self.log_s), torch.exp(self.log_alpha), torch.exp(self.log_delta), torch.exp(self.log_r), self.training and self.trainable)\n",
    "        else:\n",
    "            x = pcen(x, self.eps, self.s, self.alpha, self.delta, self.r, self.training and self.trainable)\n",
    "#         x = x.unsqueeze(dim=1).permute((0,1,3,2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1f083e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass the pretrained model and make it a binary classification\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, model_name, image_size):\n",
    "        super().__init__()\n",
    "        # num_classes=0 removes the pretrained head\n",
    "        self.backbone = timm.create_model(model_name,\n",
    "                        pretrained=True, num_classes=2, in_chans=1, \n",
    "                        drop_path_rate=0.05, global_pool='max',\n",
    "                        drop_rate=0.05)\n",
    "        #####  This section is model specific\n",
    "        #### It freezes some fo the layers by name\n",
    "        #### you'll have to inspect the model to see the names\n",
    "        for name, param in self.backbone.named_parameters():\n",
    "            if param.requires_grad and 'head' not in name \\\n",
    "                and not name.startswith('norm') \\\n",
    "                and 'stages.3' not in name \\\n",
    "                and 'blocks.26' not in name and 'blocks.26' not in name \\\n",
    "                and 'blocks.24' not in name and 'blocks.25' not in name \\\n",
    "                and 'blocks.22' not in name and 'blocks.23' not in name \\\n",
    "                and 'blocks.20' not in name and 'blocks.21' not in name \\\n",
    "                and 'blocks.22' not in name and 'blocks.23' not in name \\\n",
    "                and 'blocks.19' not in name and 'blocks.18' not in name \\\n",
    "                and 'blocks.17' not in name:\n",
    "                param.requires_grad = False\n",
    "        #### end layer freezing\n",
    "        self.spec_layer = features.STFT(n_fft=config.NFFT, freq_bins=None, hop_length=config.n_hop,\n",
    "                              window='hann', freq_scale='linear', center=True, pad_mode='reflect',\n",
    "                          fmin=400, fmax=2000, sr=config.rate, output_format=\"Magnitude\", trainable=True)\n",
    "        self.out = nn.Linear(self.backbone.num_features, 1)\n",
    "        self.sizer = VT.Resize((image_size,image_size))\n",
    "        self.timeMasking = T.TimeMasking(time_mask_param=int(config.win_size*0.4), iid_masks=True)\n",
    "        self.freqMasking = T.FrequencyMasking(freq_mask_param=int((config.NFFT//4)*0.15), iid_masks=True)\n",
    "        self.norm_layer = Normalization(mode='framewise')\n",
    "        self.pcen_layer = PCENTransform(eps=1e-6, s=0.025, alpha=0.6, delta=0.1, r=0.2, trainable=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # first compute spectrogram\n",
    "        spec = self.spec_layer(x)  # (B, F, T)\n",
    "        # normalize\n",
    "#         spec = spec.transpose(1,2) # (B, T, F)\n",
    "        spec = self.pcen_layer(spec)\n",
    "        spec = self.norm_layer(spec)\n",
    "        \n",
    "#         if self.training:\n",
    "#             spec = self.timeMasking(spec)\n",
    "#             spec = self.freqMasking(spec)\n",
    "\n",
    "        # then size for CNN model\n",
    "        # and create a channel\n",
    "        spec = self.sizer(spec)\n",
    "        x = spec.unsqueeze(1)\n",
    "        # then repeat channels\n",
    "        x = self.backbone(x)\n",
    "        \n",
    "        pred = self.out(x)\n",
    "        \n",
    "        output = {\"prediction\": pred,\n",
    "                  \"spectrogram\": spec}\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d35661c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling rate = 8000. Please make sure the sampling rate is correct in order toget a valid freq range\n",
      "STFT kernels created, time used = 0.0772 seconds\n"
     ]
    }
   ],
   "source": [
    "model = Model('convnext_small',224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46ca528e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcac04a767cd4524b54a788742f4fe55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1842 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (128x2 and 768x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_483/2187717205.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_483/3543513541.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, val_loader, model, n_channels)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;31m# outputs = model(inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_483/488045245.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         output = {\"prediction\": pred,\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (128x2 and 768x1)"
     ]
    }
   ],
   "source": [
    "model, lr_log = train_model(train_loader, val_loader, model = model, n_channels = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d0769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate schedule\n",
    "plt.plot([i for i in range(len(lr_log))], lr_log);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139beaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_A = MozTestDataset(audio_df_test_A, config.data_dir, min_length, audio_cache)\n",
    "\n",
    "test_loader_A = torch.utils.data.DataLoader(\n",
    "        dataset_test_A, batch_size=config_pytorch.batch_size,\n",
    "        num_workers=num_workers, pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "\n",
    "dataset_test_B = MozTestDataset(audio_df_test_B, config.data_dir, min_length, audio_cache)\n",
    "\n",
    "test_loader_B = torch.utils.data.DataLoader(\n",
    "        dataset_test_B, batch_size=config_pytorch.batch_size,\n",
    "        num_workers=num_workers, pin_memory=pin_memory,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061188ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path and model name are used to save the PDF output\n",
    "# but loading the model isn't needed unless the notebook\n",
    "# has been restarted.  \n",
    "path = '../outputs/models/pytorch/'\n",
    "# model_name = 'model_e0_2022_03_19_17_06_24.pth'\n",
    "model_name = 'model_e0_2022_03_19_17_15_45.pth'\n",
    "\n",
    "model = load_model(path + model_name, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1593a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## feat_type is a unique name for the combination of paramters and pre-processing\n",
    "feat_type = 'CDL_convnext_lrg_stfs_512_more_noise_rand_small_snd_framewise'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb68563",
   "metadata": {},
   "source": [
    "### Test B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dee5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_preds_all, y_test_B = evaluate_model(model, test_loader_B, 1, n_channels=1)  # Predict directly over feature windows (1.92 s)\n",
    "\n",
    "PE, MI, log_prob = get_results(y_preds_all, y_test_B, filename = feat_type + '_' + model_name +'_Test_B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4680ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_preds_all, y_test_B = evaluate_model(model, test_loader_B, 1, n_channels=1)  # Predict directly over feature windows (1.92 s)\n",
    "\n",
    "PE, MI, log_prob = get_results(y_preds_all, y_test_B, filename = feat_type + '_' + model_name +'_Test_B')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a675de",
   "metadata": {},
   "source": [
    "### Test A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1ee3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_all, y_test_A = evaluate_model(model, test_loader_A, 1, n_channels=3)  # Predict directly over feature windows (1.92 s)\n",
    "PE, MI, log_prob = get_results(y_preds_all, y_test_A, filename = feat_type + '_' + model_name +'_Test_A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33c2833",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a521ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for i, val in enumerate(y_test_B.squeeze()):\n",
    "    if val != np.argmax(y_preds_all[0],-1)[i]:\n",
    "        errors.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158cb234",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(errors), len(y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d58156",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_df_test_B.iloc[1934]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a359c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_df_test_B[audio_df_test_B['sound_type']=='mosquito'].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebadaffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader_B.dataset[1656][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3693ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d78021",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38c5bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram(model(test_loader_B.dataset[1910][0].to('cuda:0'))['spectrogram'][0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b502f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0cf08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.dataset[2001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69c901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_df_train[(audio_df_train['sound_type']=='mosquito') & (audio_df_train['length']>4)].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a67d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(_get_sample('../data/audio/220371.wav', resample=8000)[0].numpy()[0], rate=8000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb4f6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(_get_noisy_sample('../data/audio/220371.wav', noise_df=df_noise).numpy()[0], rate=8000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b482c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_get_noisy_sample('../data/audio/220371.wav', noise_df=df_noise).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb60a2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "_get_sample('../data/audio/199900.wav', resample=8000)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8beed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't run this stuff.  Saved for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f34556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb721c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram(spec.cpu().detach().numpy()[0],title='512')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3c2451",
   "metadata": {},
   "outputs": [],
   "source": [
    "sound, _ = _get_sample('../data/audio/199900.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8381495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(sound.numpy()[0], rate=8000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedad37e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e4718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a75575",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = spec_layer(sound)  # (B, F, T)\n",
    "spec = pcen_layer(spec)\n",
    "spec = norm_layer(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49558406",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcen_layer = PCENTransform(eps=1e-6, s=0.025, alpha=0.6, delta=0.1, r=0.2, trainable=True)\n",
    "norm_layer = Normalization(mode='framewise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc5233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_sample(path, resample=None):\n",
    "    effects = [\n",
    "        [\"remix\", \"1\"]\n",
    "    ]\n",
    "    if resample:\n",
    "        effects.extend([\n",
    "          #[\"lowpass\", f\"{resample // 2}\"],\n",
    "#             [\"bandpass\", f\"600\",\"1000\"],\n",
    "          [\"rate\", f'{resample}'],\n",
    "          ['gain', '-n'],\n",
    "         [\"highpass\", \"600\"],\n",
    "        ])\n",
    "    #returns waveform [1,XXXXX], sample_rate\n",
    "    \n",
    "    return torchaudio.sox_effects.apply_effects_file(path, effects=effects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf119487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_noisy_sample(path, noise_df):\n",
    "    x, _ = _get_sample(path, resample=8000)\n",
    "    sample_length = x.shape[1]/8000\n",
    "    if len(noise_df[noise_df['length'] >= sample_length]) > 0:\n",
    "        noise_path = noise_df[noise_df['length'] >= sample_length].sample(1).iloc[0]['path']\n",
    "        noise, _ = _get_sample(noise_path, resample=8000)\n",
    "        \n",
    "        noise = noise[:, :x.shape[1]]\n",
    "\n",
    "        mos_power = x.norm(p=2)\n",
    "        noise_power = noise.norm(p=2)\n",
    "\n",
    "        snr = math.exp(1 / 10)\n",
    "        scale = snr * noise_power / mos_power\n",
    "        x = (scale * x + noise) / 2\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7301f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = _get_sample('../data/audio/220371.wav', resample=8000)\n",
    "sample_length = x.shape[1]/8000\n",
    "noise_path = df_noise[df_noise['length'] >= sample_length].sample(1).iloc[0]['path']\n",
    "noise, _ = _get_sample(noise_path, resample=8000)\n",
    "\n",
    "noise = noise[:, :x.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b0516",
   "metadata": {},
   "outputs": [],
   "source": [
    "mos_power = x.norm(p=2)\n",
    "noise_power = noise.norm(p=2)\n",
    "mos_power, noise_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7b8369",
   "metadata": {},
   "outputs": [],
   "source": [
    "snr = math.exp(3 / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd21bc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = snr * noise_power / mos_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089df2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5c36cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_n = (scale * x / 10 + noise) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2fe1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2221f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(x_n, rate=8000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a92785",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(_get_sample(noise_path, resample=8000)[0].numpy()[0], rate=8000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8371ff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(_get_noisy_sample('../data/audio/220371.wav', noise_df=df_noise).numpy()[0], rate=8000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb94ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.set_title(title or 'Spectrogram (db)')\n",
    "    axs.set_ylabel(ylabel)\n",
    "    axs.set_xlabel('frame')\n",
    "#     im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n",
    "    im = axs.imshow(spec, origin='lower', aspect=aspect)\n",
    "    if xmax:\n",
    "        axs.set_xlim((0, xmax))\n",
    "    fig.colorbar(im, ax=axs)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d543ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram(F.normalize(spec_layer(x.to('cuda:0')),dim=1).cpu().detach().numpy()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f3a473",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram(spec_layer(x.to('cuda:0')).cpu().detach().numpy()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b26e6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram(tm(s[aug > 1]).cpu().detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b376862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run this cell to list pretrained models\n",
    "avail_pretrained_models = timm.list_models(pretrained=True)\n",
    "avail_pretrained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## put a model name in here to downlad a pretrained model and convert it\n",
    "## to binary classification\n",
    "pt_model = Model('convnext_small',224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a953c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at layer names to figure out what to freeze\n",
    "# You'll have to go edit the Model class and reload it based \n",
    "# on the model names you're freezing here\n",
    "[name for name, _ in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd214b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff56208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
