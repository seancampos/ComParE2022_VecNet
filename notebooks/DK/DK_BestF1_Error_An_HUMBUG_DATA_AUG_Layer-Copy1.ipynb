{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0aaead0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dli/task/ComParE2022_VecNet/notebooks/DK\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbdf5867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".CodeMirror{\n",
       "font-size: 14px;\n",
       "</style>\n",
       "CUDA_LAUNCH_BLOCKING=1\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style type='text/css'>\n",
    ".CodeMirror{\n",
    "font-size: 14px;\n",
    "</style>\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e84d228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5a2515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://zenodo.org/record/4904800/files/humbugdb_neurips_2021_1.zip?download=1\n",
    "# !wget https://zenodo.org/record/4904800/files/humbugdb_neurips_2021_2.zip?download=1\n",
    "# !wget https://zenodo.org/record/4904800/files/humbugdb_neurips_2021_3.zip?download=1\n",
    "# !wget https://zenodo.org/record/4904800/files/humbugdb_neurips_2021_4.zip?download=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3eaa0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip /content/humbugdb_neurips_2021_1.zip?download=1 -d '/content/HumBugDB/data/audio'\n",
    "# !unzip /content/humbugdb_neurips_2021_2.zip?download=1 -d '/content/HumBugDB/data/audio'\n",
    "# !unzip /content/humbugdb_neurips_2021_3.zip?download=1 -d '/content/HumBugDB/data/audio'\n",
    "# !unzip /content/humbugdb_neurips_2021_4.zip?download=1 -d '/content/HumBugDB/data/audio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ee4088d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch_audiomentations in /opt/conda/lib/python3.8/site-packages (0.11.0)\n",
      "Requirement already satisfied: torch>=1.7.0 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (1.11.0+cu113)\n",
      "Requirement already satisfied: torch-pitch-shift>=1.2.2 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (1.2.2)\n",
      "Requirement already satisfied: librosa>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (0.8.1)\n",
      "Requirement already satisfied: torchaudio>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (0.11.0+cu113)\n",
      "Requirement already satisfied: julius<0.3,>=0.2.3 in /opt/conda/lib/python3.8/site-packages (from torch_audiomentations) (0.2.6)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (2.1.9)\n",
      "Requirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (1.5.2)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (0.2.2)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (0.24.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (21.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (1.21.4)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (5.1.0)\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (1.6.3)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (0.10.3.post1)\n",
      "Requirement already satisfied: numba>=0.43.0 in /opt/conda/lib/python3.8/site-packages (from librosa>=0.6.0->torch_audiomentations) (0.53.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from numba>=0.43.0->librosa>=0.6.0->torch_audiomentations) (59.4.0)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /opt/conda/lib/python3.8/site-packages (from numba>=0.43.0->librosa>=0.6.0->torch_audiomentations) (0.36.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->librosa>=0.6.0->torch_audiomentations) (3.0.6)\n",
      "Requirement already satisfied: appdirs in /opt/conda/lib/python3.8/site-packages (from pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (1.4.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (2.26.0)\n",
      "Requirement already satisfied: six>=1.3 in /opt/conda/lib/python3.8/site-packages (from resampy>=0.2.2->librosa>=0.6.0->torch_audiomentations) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa>=0.6.0->torch_audiomentations) (3.0.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.8/site-packages (from soundfile>=0.10.2->librosa>=0.6.0->torch_audiomentations) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa>=0.6.0->torch_audiomentations) (2.21)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.7.0->torch_audiomentations) (4.0.1)\n",
      "Requirement already satisfied: primePy>=1.3 in /opt/conda/lib/python3.8/site-packages (from torch-pitch-shift>=1.2.2->torch_audiomentations) (1.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (2022.6.15)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.8/site-packages (0.11.2)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.3.4)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/conda/lib/python3.8/site-packages (from seaborn) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.6.3)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.21.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (3.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (4.28.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: setuptools-scm>=4 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (6.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (9.1.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from setuptools-scm>=4->matplotlib>=2.2->seaborn) (59.4.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from setuptools-scm>=4->matplotlib>=2.2->seaborn) (1.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_audiomentations\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "889ab838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_audiomentations import Compose,AddBackgroundNoise , AddColoredNoise , ApplyImpulseResponse,PeakNormalization,TimeInversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e47858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had to find the right version of pytorch with the widget here https://pytorch.org/\n",
    "# I *think* this will work with AWS\n",
    "#!pip3 install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b99492da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other dependencies\n",
    "#!pip install timm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ab58169",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nnAudio\n",
    "#!pip install git+https://github.com/KinWaiCheuk/nnAudio.git#subdirectory=Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5ea687",
   "metadata": {},
   "source": [
    "### 1 Import the kitchen sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9471fdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
     ]
    }
   ],
   "source": [
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36382799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dli/task/ComParE2022_VecNet/notebooks/DK\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ead16f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug main imports\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../../src'))\n",
    "import config ,config_pytorch\n",
    "from evaluate import get_results\n",
    "import numpy as np\n",
    "\n",
    "# Troubleshooting and visualisation\n",
    "import IPython.display as ipd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e88fcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# humbug lib imports\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "#from PyTorch import config_pytorch\n",
    "from datetime import datetime\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09b483ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional pytorch tools\n",
    "import random\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchvision.transforms as VT\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "import timm\n",
    "import timm.optim\n",
    "from timm.loss import BinaryCrossEntropy\n",
    "from timm.utils import NativeScaler\n",
    "from timm.models import model_parameters\n",
    "from glob import glob\n",
    "from torch_audiomentations import Compose, Gain, PolarityInversion,AddColoredNoise,ApplyImpulseResponse,PeakNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6be6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nnAudio\n",
    "from nnAudio import features\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568f72ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702dede3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c9259f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Training variables \n",
    "USE_SHORT_AUDIO = True\n",
    "num_workers=8\n",
    "pin_memory=True\n",
    "#train_size = 100\n",
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    batch_size = 4\n",
    "    test_batch_size = 4\n",
    "    num_workers=1\n",
    "    \n",
    "     \n",
    "\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0603b3d",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3972c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4744d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates 1.92 secs rows of audio in a data frame format\n",
    "def get_offsets_df(df, short_audio=False):\n",
    "    audio_offsets = []\n",
    "    min_length = config.win_size*config.NFFT/(((1/config.n_hop)*config.NFFT)*config.rate)\n",
    "    step_frac = config.step_size/config.win_size\n",
    "    for _,row in df.iterrows():\n",
    "        if row['length'] > min_length:\n",
    "            step_size = step_frac*min_length\n",
    "            audio_offsets.append({'id':row['id'], 'offset':0, 'length': row['length'],'specie_ind': row['specie_ind']})\n",
    "            for i in range(1, int((row['length']-min_length)//step_size)):\n",
    "                audio_offsets.append({'id': row['id'], 'offset':int(min_length+(i*step_size)*config.rate), 'length': row['length'],'specie_ind': row['specie_ind']})\n",
    "        elif short_audio:\n",
    "            audio_offsets.append({'id':row['id'], 'offset':0,'length': row['length'],'specie_ind': row['specie_ind']})\n",
    "    return pd.DataFrame(audio_offsets)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "025de43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['an arabiensis','culex pipiens complex', 'ae aegypti','an funestus ss','an squamosus',\n",
    "               'an coustani','ma uniformis','ma africanus' ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3990c7e9",
   "metadata": {},
   "source": [
    "### Read CSV and get train/test groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "503efadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>record_datetime</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>species</th>\n",
       "      <th>gender</th>\n",
       "      <th>fed</th>\n",
       "      <th>plurality</th>\n",
       "      <th>age</th>\n",
       "      <th>method</th>\n",
       "      <th>mic_type</th>\n",
       "      <th>device_type</th>\n",
       "      <th>country</th>\n",
       "      <th>district</th>\n",
       "      <th>province</th>\n",
       "      <th>place</th>\n",
       "      <th>location_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>0.463456</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>0.170249</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>0.104041</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69</td>\n",
       "      <td>0.274290</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56</td>\n",
       "      <td>0.420894</td>\n",
       "      <td>CDC_Ae-aegypti_labelled_800.wav</td>\n",
       "      <td>8000</td>\n",
       "      <td>8/9/2016 8:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ae aegypti</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Plural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phone</td>\n",
       "      <td>Alcatel 4009X</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>CDC insect cultures, Atlanta</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>3562</td>\n",
       "      <td>6.083093</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>3556</td>\n",
       "      <td>6.719908</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an maculatus</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9009</th>\n",
       "      <td>3553</td>\n",
       "      <td>6.128580</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an maculatus</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9011</th>\n",
       "      <td>3561</td>\n",
       "      <td>11.614280</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9012</th>\n",
       "      <td>3552</td>\n",
       "      <td>2.920249</td>\n",
       "      <td>#988-1001.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>1/7/2018 12:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an harrisoni</td>\n",
       "      <td>Female</td>\n",
       "      <td>t</td>\n",
       "      <td>Single</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>olympus</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>Sai Yok District</td>\n",
       "      <td>Kanchanaburi Province</td>\n",
       "      <td>field site near Pu Teuy Village</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6008 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     length                             name  sample_rate  \\\n",
       "1       53   0.463456  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "2       57   0.170249  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "3       61   0.104041  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "4       69   0.274290  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "5       56   0.420894  CDC_Ae-aegypti_labelled_800.wav         8000   \n",
       "...    ...        ...                              ...          ...   \n",
       "8999  3562   6.083093                    #988-1001.wav        44100   \n",
       "9000  3556   6.719908                    #988-1001.wav        44100   \n",
       "9009  3553   6.128580                    #988-1001.wav        44100   \n",
       "9011  3561  11.614280                    #988-1001.wav        44100   \n",
       "9012  3552   2.920249                    #988-1001.wav        44100   \n",
       "\n",
       "     record_datetime sound_type       species  gender  fed plurality  age  \\\n",
       "1      8/9/2016 8:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "2      8/9/2016 8:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "3      8/9/2016 8:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "4      8/9/2016 8:00   mosquito    ae aegypti     NaN  NaN    Single  NaN   \n",
       "5      8/9/2016 8:00   mosquito    ae aegypti     NaN  NaN    Plural  NaN   \n",
       "...              ...        ...           ...     ...  ...       ...  ...   \n",
       "8999  1/7/2018 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "9000  1/7/2018 12:00   mosquito  an maculatus  Female    t    Single  NaN   \n",
       "9009  1/7/2018 12:00   mosquito  an maculatus  Female    t    Single  NaN   \n",
       "9011  1/7/2018 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "9012  1/7/2018 12:00   mosquito  an harrisoni  Female    t    Single  NaN   \n",
       "\n",
       "     method mic_type    device_type   country          district  \\\n",
       "1       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "2       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "3       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "4       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "5       NaN    phone  Alcatel 4009X       USA           Georgia   \n",
       "...     ...      ...            ...       ...               ...   \n",
       "8999    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9000    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9009    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9011    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "9012    ABN  telinga        olympus  Thailand  Sai Yok District   \n",
       "\n",
       "                   province                            place location_type  \n",
       "1                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "2                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "3                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "4                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "5                   Atlanta     CDC insect cultures, Atlanta       culture  \n",
       "...                     ...                              ...           ...  \n",
       "8999  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9000  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9009  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9011  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "9012  Kanchanaburi Province  field site near Pu Teuy Village           cup  \n",
       "\n",
       "[6008 rows x 19 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    df = pd.read_csv(config.data_df_msc_test)\n",
    "else:\n",
    "    df = pd.read_csv(config.data_df)\n",
    "\n",
    "#df = df.loc[df['Grade'].notnull()]\n",
    "df = df.loc[df['species'].notnull()]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b25ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a colum for specie encoding\n",
    "df['specie_ind'] = \"NULL_VAL\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c42c49e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specie = an arabiensisand its index = 0\n",
      "specie = culex pipiens complexand its index = 1\n",
      "specie = ae aegyptiand its index = 2\n",
      "specie = an funestus ssand its index = 3\n",
      "specie = an squamosusand its index = 4\n",
      "specie = an coustaniand its index = 5\n",
      "specie = ma uniformisand its index = 6\n",
      "specie = ma africanusand its index = 7\n"
     ]
    }
   ],
   "source": [
    "# Adding a new column to encode specie_index in the same order as the list \"classes\"\n",
    "ind = 0\n",
    "for specie in classes:\n",
    "    print(\"specie = \" + str(specie) + \"and its index = \" + str(ind) )\n",
    "    row_indexes=df[df['species']==specie].index \n",
    "    df.loc[row_indexes,'specie_ind']= ind\n",
    "    ind+=1\n",
    "\n",
    "    \n",
    "# other_df_ind = df[df['specie_ind'] == \"NULL_VAL\"].index\n",
    "# df.loc[other_df_ind,'specie_ind']= other_ind                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd339f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['specie_ind'] == \"NULL_VAL\"].index, inplace=True)\n",
    "#other_df_ind = df[df['specie_ind'] == \"NULL_VAL\"].index\n",
    "#df.loc[other_df_ind,'specie_ind']= other_ind        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c777f6f5",
   "metadata": {},
   "source": [
    "At this stage we have all extracted the data with specie information and have encoded the specie encoding in a col = 'specie_ind'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8881da72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the TZ and Cup data- this is as per the humbug paper\n",
    "\n",
    "idx_multiclass = np.logical_and(df['country'] == 'Tanzania', df['location_type'] == 'cup')\n",
    "df_all = df[idx_multiclass]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4dd8ff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d6bb29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>record_datetime</th>\n",
       "      <th>sound_type</th>\n",
       "      <th>species</th>\n",
       "      <th>gender</th>\n",
       "      <th>fed</th>\n",
       "      <th>...</th>\n",
       "      <th>age</th>\n",
       "      <th>method</th>\n",
       "      <th>mic_type</th>\n",
       "      <th>device_type</th>\n",
       "      <th>country</th>\n",
       "      <th>district</th>\n",
       "      <th>province</th>\n",
       "      <th>place</th>\n",
       "      <th>location_type</th>\n",
       "      <th>specie_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1879</td>\n",
       "      <td>221103</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_24_664.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1880</td>\n",
       "      <td>221111</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_25_665.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1881</td>\n",
       "      <td>221110</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_25_665.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>ma africanus</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1882</td>\n",
       "      <td>221149</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_26_666.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an arabiensis</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1883</td>\n",
       "      <td>221150</td>\n",
       "      <td>2.56</td>\n",
       "      <td>IFA_17_26_666.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>30-01-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an arabiensis</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HBN</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>4546</td>\n",
       "      <td>222615</td>\n",
       "      <td>30.72</td>\n",
       "      <td>IFA_86_39_3439.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2284</th>\n",
       "      <td>4547</td>\n",
       "      <td>222585</td>\n",
       "      <td>25.60</td>\n",
       "      <td>IFA_86_40_3440.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2285</th>\n",
       "      <td>4548</td>\n",
       "      <td>222586</td>\n",
       "      <td>40.90</td>\n",
       "      <td>IFA_87_10_3450.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2286</th>\n",
       "      <td>4549</td>\n",
       "      <td>222596</td>\n",
       "      <td>40.90</td>\n",
       "      <td>IFA_87_11_3451.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287</th>\n",
       "      <td>4550</td>\n",
       "      <td>222614</td>\n",
       "      <td>38.40</td>\n",
       "      <td>IFA_87_12_3452.wav</td>\n",
       "      <td>44100</td>\n",
       "      <td>23-08-20 00:00</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>an funestus ss</td>\n",
       "      <td>Female</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LT</td>\n",
       "      <td>telinga</td>\n",
       "      <td>tascam</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Kilombero District</td>\n",
       "      <td>Morogoro</td>\n",
       "      <td>Ifakara</td>\n",
       "      <td>cup</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2288 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index      id  length                name  sample_rate record_datetime  \\\n",
       "0      1879  221103    2.56   IFA_17_24_664.wav        44100  30-01-20 00:00   \n",
       "1      1880  221111    2.56   IFA_17_25_665.wav        44100  30-01-20 00:00   \n",
       "2      1881  221110    2.56   IFA_17_25_665.wav        44100  30-01-20 00:00   \n",
       "3      1882  221149    2.56   IFA_17_26_666.wav        44100  30-01-20 00:00   \n",
       "4      1883  221150    2.56   IFA_17_26_666.wav        44100  30-01-20 00:00   \n",
       "...     ...     ...     ...                 ...          ...             ...   \n",
       "2283   4546  222615   30.72  IFA_86_39_3439.wav        44100  23-08-20 00:00   \n",
       "2284   4547  222585   25.60  IFA_86_40_3440.wav        44100  23-08-20 00:00   \n",
       "2285   4548  222586   40.90  IFA_87_10_3450.wav        44100  23-08-20 00:00   \n",
       "2286   4549  222596   40.90  IFA_87_11_3451.wav        44100  23-08-20 00:00   \n",
       "2287   4550  222614   38.40  IFA_87_12_3452.wav        44100  23-08-20 00:00   \n",
       "\n",
       "     sound_type         species  gender fed  ... age  method mic_type  \\\n",
       "0      mosquito    ma africanus  Female   f  ... NaN     HBN  telinga   \n",
       "1      mosquito    ma africanus  Female   f  ... NaN     HBN  telinga   \n",
       "2      mosquito    ma africanus  Female   f  ... NaN     HBN  telinga   \n",
       "3      mosquito   an arabiensis  Female   f  ... NaN     HBN  telinga   \n",
       "4      mosquito   an arabiensis  Female   f  ... NaN     HBN  telinga   \n",
       "...         ...             ...     ...  ..  ...  ..     ...      ...   \n",
       "2283   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2284   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2285   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2286   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "2287   mosquito  an funestus ss  Female   f  ... NaN      LT  telinga   \n",
       "\n",
       "     device_type   country            district  province    place  \\\n",
       "0         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "1         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "3         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "4         tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "...          ...       ...                 ...       ...      ...   \n",
       "2283      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2284      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2285      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2286      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "2287      tascam  Tanzania  Kilombero District  Morogoro  Ifakara   \n",
       "\n",
       "     location_type specie_ind  \n",
       "0              cup          7  \n",
       "1              cup          7  \n",
       "2              cup          7  \n",
       "3              cup          0  \n",
       "4              cup          0  \n",
       "...            ...        ...  \n",
       "2283           cup          3  \n",
       "2284           cup          3  \n",
       "2285           cup          3  \n",
       "2286           cup          3  \n",
       "2287           cup          3  \n",
       "\n",
       "[2288 rows x 21 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9569db5d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1ec45e5",
   "metadata": {},
   "source": [
    "### Train-Test split( avoiding sklearn )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3836133",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "msk_test = np.random.rand(len(df_all)) < 0.2\n",
    "df_test = df_all[msk_test]\n",
    "df_train_temp  = df_all[~msk_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e6ff51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "msk_train = np.random.rand(len(df_train_temp)) < 0.2\n",
    "df_val = df_train_temp[msk_train]\n",
    "df_train  = df_train_temp[~msk_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16b0444",
   "metadata": {},
   "source": [
    "## Let's verify for data leakage by performing an inner-join on id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f5521bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, index_y, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_test,df_train, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15dd186a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, index_y, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_test,df_val, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2516e9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>length_x</th>\n",
       "      <th>name_x</th>\n",
       "      <th>sample_rate_x</th>\n",
       "      <th>record_datetime_x</th>\n",
       "      <th>sound_type_x</th>\n",
       "      <th>species_x</th>\n",
       "      <th>gender_x</th>\n",
       "      <th>fed_x</th>\n",
       "      <th>...</th>\n",
       "      <th>age_y</th>\n",
       "      <th>method_y</th>\n",
       "      <th>mic_type_y</th>\n",
       "      <th>device_type_y</th>\n",
       "      <th>country_y</th>\n",
       "      <th>district_y</th>\n",
       "      <th>province_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>location_type_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, length_x, name_x, sample_rate_x, record_datetime_x, sound_type_x, species_x, gender_x, fed_x, plurality_x, age_x, method_x, mic_type_x, device_type_x, country_x, district_x, province_x, place_x, location_type_x, specie_ind_x, index_y, length_y, name_y, sample_rate_y, record_datetime_y, sound_type_y, species_y, gender_y, fed_y, plurality_y, age_y, method_y, mic_type_y, device_type_y, country_y, district_y, province_y, place_y, location_type_y, specie_ind_y]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_train,df_val, on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e06bf82",
   "metadata": {},
   "source": [
    "We've confirmed that there is no recording that is common in Train,Test,val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0f43fa",
   "metadata": {},
   "source": [
    "### Next, we perform \"offsets\", spliting each(long) recording into multiple 1.92 secs chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f99ebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_offset = get_offsets_df(df_train, short_audio=USE_SHORT_AUDIO)\n",
    "df_test_offset = get_offsets_df(df_test, short_audio=USE_SHORT_AUDIO)\n",
    "df_val_offset = get_offsets_df(df_val, short_audio=USE_SHORT_AUDIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "367d1907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train offset = 32239\n",
      "length of test offset = 10087\n",
      "length of val offset = 8692\n"
     ]
    }
   ],
   "source": [
    "print(\"length of train offset = \" +str(len(df_train_offset)))\n",
    "print(\"length of test offset = \" +str(len(df_test_offset)))\n",
    "print(\"length of val offset = \" +str(len(df_val_offset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d20479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce73e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee4783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9539dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temp.reset_index(inplace = True)\n",
    "df_train_offset.reset_index(inplace = True)\n",
    "df_test_offset.reset_index(inplace = True)\n",
    "df_val_offset.reset_index(inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cff291b",
   "metadata": {},
   "source": [
    "### Let's check for data leakage in offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "113a5aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>offset_x</th>\n",
       "      <th>length_x</th>\n",
       "      <th>specie_ind_x</th>\n",
       "      <th>index_y</th>\n",
       "      <th>offset_y</th>\n",
       "      <th>length_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, offset_x, length_x, specie_ind_x, index_y, offset_y, length_y, specie_ind_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_train_offset , df_test_offset , on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9bb4fb49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>offset_x</th>\n",
       "      <th>length_x</th>\n",
       "      <th>specie_ind_x</th>\n",
       "      <th>index_y</th>\n",
       "      <th>offset_y</th>\n",
       "      <th>length_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, offset_x, length_x, specie_ind_x, index_y, offset_y, length_y, specie_ind_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_train_offset , df_val_offset , on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01edefbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>offset_x</th>\n",
       "      <th>length_x</th>\n",
       "      <th>specie_ind_x</th>\n",
       "      <th>index_y</th>\n",
       "      <th>offset_y</th>\n",
       "      <th>length_y</th>\n",
       "      <th>specie_ind_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index_x, id, offset_x, length_x, specie_ind_x, index_y, offset_y, length_y, specie_ind_y]\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(df_test_offset , df_val_offset , on = 'id', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be83f47",
   "metadata": {},
   "source": [
    "### At this stage we've a dataframe of recordin ids and each row corresponds to a 1.92 secs recording or shorter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c932a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specie_distri(df , classes , type_df = None):\n",
    "    \"\"\"This function takes a dataframe and provides a count of each specie class\"\"\"\n",
    "    for i in range(len(classes)):\n",
    "        print(\"DF type = \" + str(type_df))\n",
    "        df_temp = df[df['specie_ind'] == i]\n",
    "        print(\"i = \" +str(i))\n",
    "        print(len(df_temp))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d07b9747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32298429 0.56615271 3.6109991  0.61779473 1.98809817 4.24197368\n",
      " 3.08802682 5.57382434]\n"
     ]
    }
   ],
   "source": [
    "#Class imbalance \n",
    "np.array(df_train_offset.specie_ind)\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',classes=np.unique(np.array(df_train_offset.specie_ind)),y=np.array(np.array(df_train_offset.specie_ind)))\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11ff83b",
   "metadata": {},
   "source": [
    "Let us now get the class distribution for each of the dataframes- train,test and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca75c915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF type = train\n",
      "i = 0\n",
      "12477\n",
      "DF type = train\n",
      "i = 1\n",
      "7118\n",
      "DF type = train\n",
      "i = 2\n",
      "1116\n",
      "DF type = train\n",
      "i = 3\n",
      "6523\n",
      "DF type = train\n",
      "i = 4\n",
      "2027\n",
      "DF type = train\n",
      "i = 5\n",
      "950\n",
      "DF type = train\n",
      "i = 6\n",
      "1305\n",
      "DF type = train\n",
      "i = 7\n",
      "723\n"
     ]
    }
   ],
   "source": [
    "get_specie_distri(df_train_offset , classes , type_df = \"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e71e879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF type = Val\n",
      "i = 0\n",
      "3613\n",
      "DF type = Val\n",
      "i = 1\n",
      "1994\n",
      "DF type = Val\n",
      "i = 2\n",
      "230\n",
      "DF type = Val\n",
      "i = 3\n",
      "1855\n",
      "DF type = Val\n",
      "i = 4\n",
      "280\n",
      "DF type = Val\n",
      "i = 5\n",
      "228\n",
      "DF type = Val\n",
      "i = 6\n",
      "426\n",
      "DF type = Val\n",
      "i = 7\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "get_specie_distri(df_val_offset , classes , type_df = \"Val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ee5f5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF type = test\n",
      "i = 0\n",
      "4356\n",
      "DF type = test\n",
      "i = 1\n",
      "1879\n",
      "DF type = test\n",
      "i = 2\n",
      "439\n",
      "DF type = test\n",
      "i = 3\n",
      "1959\n",
      "DF type = test\n",
      "i = 4\n",
      "507\n",
      "DF type = test\n",
      "i = 5\n",
      "312\n",
      "DF type = test\n",
      "i = 6\n",
      "441\n",
      "DF type = test\n",
      "i = 7\n",
      "194\n"
     ]
    }
   ],
   "source": [
    "get_specie_distri(df_test_offset , classes , type_df = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2187a3dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7d4734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function pads a short-audio tensor with its mean to ensure that it becomes a 1.92 sec long audio equivalent\n",
    "def pad_mean(x_temp,rate = config.rate, min_length = config.min_duration ):\n",
    "    if DEBUG:\n",
    "        print(\"inside padding mean...\")\n",
    "    x_mean = torch.mean(x_temp)\n",
    "    #x_mean.cuda()\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"X_mean = \" + str(x_mean))\n",
    "    left_pad_amt = int((rate*min_length-x_temp.shape[1])//2)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_amt = \" + str(left_pad_amt))\n",
    "    left_pad = torch.zeros(1,left_pad_amt) #+ (0.1**0.5)*torch.randn(1, left_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"left_pad shape = \" + str(left_pad.shape))\n",
    "    left_pad_mean_add = left_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"left_pad_mean shape = \" + str(left_pad_mean_add))\n",
    "        print(\"sum of left pad mean add = \" + str(torch.sum(left_pad_mean_add)))\n",
    "    \n",
    "    right_pad_amt = int(rate*min_length-x_temp.shape[1]-left_pad_amt)\n",
    "    right_pad = torch.zeros(1,right_pad_amt)# + (0.1**0.5)*torch.randn(1, right_pad_amt)\n",
    "    if DEBUG:\n",
    "        print(\"right_pad shape = \" + str(right_pad.shape))\n",
    "    right_pad_mean_add = right_pad + x_mean\n",
    "    if DEBUG:\n",
    "        print(\"right_pad_mean shape = \" + str(right_pad_mean_add))\n",
    "        print(\"sum of right pad mean add = \"  + str(torch.sum(right_pad_mean_add)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    f = torch.cat([left_pad,x_temp,right_pad],dim=1)[0]\n",
    "    f = f.unsqueeze(dim = 0)\n",
    "    #print(\"returning a tensor of shape = \" + str(f.shape))\n",
    "    return(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5993de2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d120151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_hat,y_true,classes):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_hat, y_true ,labels= range(len(classes)))\n",
    "    import seaborn as sns\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cellsplt.xticks(rotation=90)\n",
    "    ax.xaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    ax.xaxis.tick_bottom()\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.set_ylabel('True', fontsize=20)\n",
    "    ax.yaxis.set_ticklabels(classes, fontsize = 10)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39778529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e7a5895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.92"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the min length based on config params\n",
    "min_length = (config.win_size * config.n_hop) / config.rate\n",
    "min_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdedeb93",
   "metadata": {},
   "source": [
    "### Class Defintions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "94e13c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization():\n",
    "    \"\"\"This class is for normalizing the spectrograms batch by batch. The normalization used is min-max, two modes 'framewise' and 'imagewise' can be selected. In this paper, we found that 'imagewise' normalization works better than 'framewise'\"\"\"\n",
    "    def __init__(self, mode='framewise'):\n",
    "        if mode == 'framewise':\n",
    "            def normalize(x):\n",
    "                size = x.shape\n",
    "                x_max = x.max(1, keepdim=True)[0] # Finding max values for each frame\n",
    "                x_min = x.min(1, keepdim=True)[0]  \n",
    "                output = (x-x_min)/(x_max-x_min) # If there is a column with all zero, nan will occur\n",
    "                output[torch.isnan(output)]=0 # Making nan to 0\n",
    "                return output\n",
    "        elif mode == 'imagewise':\n",
    "            def normalize(x):\n",
    "                size = x.shape\n",
    "                x_max = x.reshape(size[0], size[1]*size[2]).max(1, keepdim=True)[0]\n",
    "                x_min = x.reshape(size[0], size[1]*size[2]).min(1, keepdim=True)[0]\n",
    "                x_max = x_max.unsqueeze(1) # Make it broadcastable\n",
    "                x_min = x_min.unsqueeze(1) # Make it broadcastable \n",
    "                return (x-x_min)/(x_max-x_min)\n",
    "        else:\n",
    "            print(f'please choose the correct mode')\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.normalize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a709edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcen(x, eps=1e-6, s=0.025, alpha=0.98, delta=2, r=0.5, training=False):\n",
    "    frames = x.split(1, -2)\n",
    "    m_frames = []\n",
    "    last_state = None\n",
    "    for frame in frames:\n",
    "        if last_state is None:\n",
    "            last_state = s * frame\n",
    "            m_frames.append(last_state)\n",
    "            continue\n",
    "        if training:\n",
    "            m_frame = ((1 - s) * last_state).add_(s * frame)\n",
    "        else:\n",
    "            m_frame = (1 - s) * last_state + s * frame\n",
    "        last_state = m_frame\n",
    "        m_frames.append(m_frame)\n",
    "    M = torch.cat(m_frames, 1)\n",
    "    if training:\n",
    "        pcen_ = (x / (M + eps).pow(alpha) + delta).pow(r) - delta ** r\n",
    "    else:\n",
    "        pcen_ = x.div_(M.add_(eps).pow_(alpha)).add_(delta).pow_(r).sub_(delta ** r)\n",
    "    return pcen_\n",
    "\n",
    "\n",
    "class PCENTransform(nn.Module):\n",
    "\n",
    "    def __init__(self, eps=1e-6, s=0.025, alpha=0.98, delta=2, r=0.5, trainable=True):\n",
    "        super().__init__()\n",
    "        if trainable:\n",
    "            self.log_s = nn.Parameter(torch.log(torch.Tensor([s])))\n",
    "            self.log_alpha = nn.Parameter(torch.log(torch.Tensor([alpha])))\n",
    "            self.log_delta = nn.Parameter(torch.log(torch.Tensor([delta])))\n",
    "            self.log_r = nn.Parameter(torch.log(torch.Tensor([r])))\n",
    "        else:\n",
    "            self.s = s\n",
    "            self.alpha = alpha\n",
    "            self.delta = delta\n",
    "            self.r = r\n",
    "        self.eps = eps\n",
    "        self.trainable = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x.permute((0,2,1)).squeeze(dim=1)\n",
    "        if self.trainable:\n",
    "            x = pcen(x, self.eps, torch.exp(self.log_s), torch.exp(self.log_alpha), torch.exp(self.log_delta), torch.exp(self.log_r), self.training and self.trainable)\n",
    "        else:\n",
    "            x = pcen(x, self.eps, self.s, self.alpha, self.delta, self.r, self.training and self.trainable)\n",
    "#         x = x.unsqueeze(dim=1).permute((0,1,3,2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aeddce21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>offset</th>\n",
       "      <th>length</th>\n",
       "      <th>specie_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>221103</td>\n",
       "      <td>0</td>\n",
       "      <td>2.56</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>221111</td>\n",
       "      <td>0</td>\n",
       "      <td>2.56</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>221110</td>\n",
       "      <td>0</td>\n",
       "      <td>2.56</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>221149</td>\n",
       "      <td>0</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>221144</td>\n",
       "      <td>0</td>\n",
       "      <td>2.56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index      id  offset  length  specie_ind\n",
       "0      0  221103       0    2.56           7\n",
       "1      1  221111       0    2.56           7\n",
       "2      2  221110       0    2.56           7\n",
       "3      3  221149       0    2.56           0\n",
       "4      4  221144       0    2.56           1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_offset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bf3de674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_df(loader , trained_model, DEBUG = False):\n",
    "    err_dict = {'id': None,\n",
    "               'label': None,\n",
    "               'offset':None,\n",
    "               'y_hat':None}\n",
    "    model = trained_model\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if device is None:\n",
    "            torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        model.eval()\n",
    "        if DEBUG:\n",
    "            print(\"inside test....\")\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        all_wav_id = []\n",
    "        all_offset = []\n",
    "        if DEBUG:\n",
    "            print(\"length of loader = \" + str(len(loader)))\n",
    "        for idx,(x,y,offset,wav_id) in enumerate(loader):\n",
    "            if DEBUG:\n",
    "                print(\"loader index = \" + str(idx))\n",
    "                print(\"y = \" + str(y))\n",
    "                print(\"offset = \" + str(offset))\n",
    "                print(\"wav_id = \" + str(wav_id))\n",
    "                \n",
    "            x = x.to(device).float() \n",
    "            y = y.type(torch.LongTensor).to(device)\n",
    "            y_pred = model(x)['prediction']\n",
    "            preds = torch.argmax(y_pred, axis = 1)\n",
    "            y_pred_cpu = y_pred.cpu().detach()\n",
    "            if DEBUG:\n",
    "                print(\"y_pred_cpu = \" + str(y_pred_cpu))\n",
    "            preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "            if DEBUG:\n",
    "                print(\"preds = \" +str(preds))\n",
    "            all_y_pred.append(preds.cpu().detach())\n",
    "            all_y.append(y.cpu().detach())\n",
    "            all_wav_id.append(wav_id.cpu().detach())\n",
    "            all_offset.append(offset.cpu().detach())\n",
    "            #all_y_pred.append(np.argmax(y_pred.cpu().detach().numpy()))\n",
    "            \n",
    "            del x\n",
    "            del y\n",
    "            del y_pred\n",
    "        all_y = torch.cat(all_y).numpy()\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        all_wav_id = torch.cat(all_wav_id)\n",
    "        all_offset = torch.cat(all_offset)\n",
    "        \n",
    "        err_dict['id'] = all_wav_id\n",
    "        err_dict['label'] = all_y\n",
    "        err_dict['offset'] = all_offset\n",
    "        err_dict['y_hat'] = all_y_pred\n",
    "        df_err = pd.DataFrame.from_dict(err_dict)\n",
    "        df_err_uniq = df_err[df_err['label']!= df_err['y_hat']]\n",
    "        df_err_uniq.sort_values(by=['id','offset'])\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        if DEBUG:\n",
    "            print(\"inside error ....\")\n",
    "            print(\"y = \" + str(all_y))\n",
    "            print(\"y_pred  = \" + str(all_y_pred))\n",
    "        \n",
    "        #test_loss = test_loss/len(test_loader)\n",
    "        #test_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')\n",
    "    \n",
    "    \n",
    "    return df_err_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c1ecae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, loader, criterion,  classes = classes,device=None , call = \"val\"):\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"calling for ...\" +str(call))\n",
    "    with torch.no_grad():\n",
    "        if device is None:\n",
    "            torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        sigmoid = nn.Sigmoid()\n",
    "        test_loss = 0.0\n",
    "        model.eval()\n",
    "        if DEBUG:\n",
    "            print(\"inside test....\")\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        counter = 1\n",
    "        if DEBUG:\n",
    "            print(\"length of loader = \" + str(len(loader)))\n",
    "        for idx,(x,y) in enumerate(loader):\n",
    "            if DEBUG:\n",
    "                print(\"loader index = \" + str(idx))\n",
    "                            \n",
    "            x = x.to(device).float() \n",
    "            y = y.type(torch.LongTensor).to(device)\n",
    "            if DEBUG:\n",
    "                print(\"y = \" + str(y))\n",
    "            y_pred = model(x)['prediction']\n",
    "            preds = torch.argmax(y_pred, axis = 1)\n",
    "            y_pred_cpu = y_pred.cpu().detach()\n",
    "            if DEBUG:\n",
    "                print(\"y_pred_cpu = \" + str(y_pred_cpu))\n",
    "            preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "            if DEBUG:\n",
    "                print(\"preds = \" +str(preds))\n",
    "            all_y_pred.append(preds.cpu().detach())\n",
    "                                   \n",
    "            loss = criterion(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "            all_y.append(y.cpu().detach())\n",
    "            #all_y_pred.append(np.argmax(y_pred.cpu().detach().numpy()))\n",
    "            \n",
    "            del x\n",
    "            del y\n",
    "            del y_pred\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        if DEBUG:\n",
    "            print(\"inside test....\")\n",
    "            print(\"y = \" + str(all_y))\n",
    "            print(\"y_pred  = \" + str(all_y_pred))\n",
    "        \n",
    "        test_loss = test_loss/len(test_loader)\n",
    "        test_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')\n",
    "    \n",
    "    \n",
    "    return test_loss, test_f1 , all_y,all_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ee169a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(train_loader, val_loader, test_loader,model, classes ,class_weights ,num_epochs = num_epochs )\n",
    "def train_model(train_loader, val_loader,test_loader, model = None,  classes = classes,class_weights = class_weights,num_epochs = num_epochs ,n_channels = 1):\n",
    "    # Creates a GradScaler once at the beginning of training.\n",
    "    loss_scaler = NativeScaler()\n",
    "    global_step = 0\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Training on {device}')    \n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "\n",
    "    model = model.to(device)\n",
    "    weights_adj = torch.tensor(class_weights).type(torch.float).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights_adj)\n",
    "    optimiser = timm.optim.RAdam(model.parameters(), lr=config_pytorch.lr/10)\n",
    "    num_epochs = num_epochs\n",
    "    all_train_loss = []\n",
    "    all_train_f1 = []\n",
    "    all_val_loss = []\n",
    "    all_val_f1 = []\n",
    "    best_val_loss = np.inf\n",
    "    best_val_f1 = -np.inf\n",
    "    best_train_f1 = -np.inf\n",
    "    best_epoch = -1\n",
    "    checkpoint_name = None\n",
    "    overrun_counter = 0\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    lr_log = []\n",
    "    for e in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_y = []\n",
    "        all_y_pred = []\n",
    "        tk0 = tqdm(train_loader, total=int(len(train_loader)))\n",
    "        for batch_i, inputs in enumerate(tk0):\n",
    "            if DEBUG:\n",
    "                print(\"inside train loop.. batch_ind = \" +str(batch_i))\n",
    "            if batch_i % 200 == 0:\n",
    "                bat_time = time.time()\n",
    "                durn = (bat_time - start_time)/60\n",
    "                print(\"epoch = \" +str(e) + \"batch = \" +str(batch_i) + \" of \" + str(len(train_loader)) + \"duraation = \" + str(durn))\n",
    "            x = inputs[0].to(device).float()\n",
    "            y = inputs[1].type(torch.LongTensor).to(device)\n",
    "            global_step += 1\n",
    "            optimiser.zero_grad()\n",
    "            # AMP\n",
    "            with autocast():\n",
    "                y_pred = model(x)['prediction']\n",
    "                preds = torch.argmax(y_pred, axis = 1)\n",
    "                if DEBUG:\n",
    "                    print(\"y_pred  = \" +str(y_pred))\n",
    "                    print(\"preds = \" +str(preds))\n",
    "                loss = criterion(y_pred, y)\n",
    "            loss_scaler(loss, optimiser,parameters=model_parameters(model))\n",
    "            train_loss += loss.item()\n",
    "            all_y.append(y.cpu().detach())\n",
    "            y_pred_cpu = y_pred.cpu().detach()\n",
    "            preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "            if DEBUG:\n",
    "                print(\"batch_ind = \" +str(batch_i))\n",
    "                print(\"y_pred_cpu = \" + str(y_pred_cpu))\n",
    "                \n",
    "            all_y_pred.append(preds.cpu().detach())\n",
    "            lr_log.append(optimiser.param_groups[0]['lr'])\n",
    "            tk0.set_postfix(training_loss=(train_loss / (batch_i+1)), lr=optimiser.param_groups[0]['lr'])\n",
    "            del x\n",
    "            del y\n",
    "            del y_pred,preds\n",
    "        \n",
    "        all_train_loss.append(train_loss/len(train_loader))\n",
    "        all_y = torch.cat(all_y)\n",
    "        all_y_pred = torch.cat(all_y_pred)\n",
    "        if DEBUG:\n",
    "            print(\"y = \" + str(all_y))\n",
    "            print(\"y_pred  = \" + str(all_y_pred))\n",
    "        \n",
    "        train_f1 = f1_score(all_y.numpy(), all_y_pred.numpy(),average='weighted')\n",
    "        if DEBUG:\n",
    "            print(\"train acc = \" +str(train_acc))\n",
    "        all_train_f1.append(train_f1)\n",
    "        val_loss, val_f1 , _,_ = test_model(model, val_loader, criterion = nn.CrossEntropyLoss(), classes = classes ,device=device, call = \"val\")\n",
    "        if DEBUG:\n",
    "            print(\"val F1 = \" + str(val_f1))\n",
    "        all_val_loss.append(val_loss)\n",
    "        all_val_f1.append(val_f1)\n",
    "        \n",
    "        acc_metric = val_f1\n",
    "        best_acc_metric = best_val_f1\n",
    "        if acc_metric > best_acc_metric:  \n",
    "            overrun_counter = -1\n",
    "            checkpoint_name = f'model_e{e}_{datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")}.pth'\n",
    "            torch.save(model.state_dict(), os.path.join(config.model_dir,  checkpoint_name))\n",
    "            print('Saving model to:', os.path.join(config.model_dir,  checkpoint_name)) \n",
    "            print(\"Now printing classification rport... \")\n",
    "            print(\"********************************\")\n",
    "            from sklearn.metrics import classification_report\n",
    "            _, _ , all_y_test,all_y_pred_test = test_model(model, test_loader, criterion = nn.CrossEntropyLoss(), classes = classes ,device=device, call = \"test\")\n",
    "            # at times output is not getting printed. Could be due to multi threading and hence adding sleep\n",
    "            time.sleep(2)\n",
    "            print(classification_report(all_y_test.numpy(), all_y_pred_test.numpy(), target_names= classes))\n",
    "            print(\"********************************\")\n",
    "            time.sleep(2)\n",
    "            plot_confusion_matrix(all_y_pred_test.numpy(), all_y_test.numpy() , classes)\n",
    "            print('Epoch: %d, Train Loss: %.8f, Train f1: %.8f, Val Loss: %.8f, Val f1: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader), train_f1, val_loss/len(val_loader), val_f1,  overrun_counter))\n",
    "            best_epoch = e\n",
    "            best_val_f1 = val_f1\n",
    "            best_val_loss = val_loss\n",
    "            \n",
    "        else:\n",
    "            print(\"..Overrun....no improvement\")\n",
    "            overrun_counter += 1\n",
    "            print('Epoch: %d, Train Loss: %.8f, Train f1: %.8f, Val Loss: %.8f, Val f1: %.8f, overrun_counter %i' % (e, train_loss/len(train_loader), train_f1, val_loss/len(val_loader), val_f1,  overrun_counter))\n",
    "        if overrun_counter > config_pytorch.max_overrun:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    return model, lr_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd7aaf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_audiomentations import Compose, Gain, PolarityInversion,AddColoredNoise,ApplyImpulseResponse,PeakNormalization\n",
    "# #apply_augmentation = Compose(transforms=[PolarityInversion(p=0.5 ,output_type = 'tensor'),AddColoredNoise(), PeakNormalization(apply_to=\"only_too_loud_sounds\"),TimeInversion(output_type = 'tensor')  ])\n",
    "\n",
    "\n",
    "# apply_augmentation = Compose(transforms=[AddColoredNoise(p = 1) ,TimeInversion( p = 1) ,PolarityInversion(p = 1)])\n",
    "\n",
    "# #apply_augmentation = Compose(transforms=[PolarityInversion(p=0.5 ,output_type = 'tensor'),AddColoredNoise(), PeakNormalization(apply_to=\"only_too_loud_sounds\"),TimeInversion(output_type = 'tensor')  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8dfe1364",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MozTrainDataset(Dataset):\n",
    "\n",
    "    def __init__(self, audio_df, data_dir, min_length, cache=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            noise_df (DataFrame): the df of noise files and lengths\n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        #self.noise_df = noise_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    \n",
    "    def _get_sample_(self, path, resample=None):\n",
    "        \n",
    "        waveform, inp_rate = torchaudio.load(path)\n",
    "        \n",
    "        if inp_rate != config.rate:\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_rate, config.rate, dtype=waveform.dtype)\n",
    "            waveform = resampler(waveform)\n",
    "    \n",
    "        \n",
    "        #waveform, rate = torchaudio.load(path)\n",
    "                \n",
    "        if waveform.shape[1] < config.rate*self.min_length:\n",
    "            #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "            f_out = pad_mean(waveform)\n",
    "        else:\n",
    "            f = waveform[0]\n",
    "            f_out = f.unsqueeze(0)\n",
    "            \n",
    "        return f_out\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #real_idx = idx % len(self.audio_df)\n",
    "                   \n",
    "        x = self._get_sample_(os.path.join(self.data_dir,f\"{int(self.audio_df.loc[idx]['id'])}.wav\"), resample=config.rate)\n",
    "       # random noise on even number indexes\n",
    "        offset = int(self.audio_df.loc[idx]['offset'])\n",
    "        if DEBUG:\n",
    "            print(\"idx = \" + str(idx))\n",
    "            print(\"offset = \" + str(offset))\n",
    "            print(\"shape of x post augmentation = \" + str(x.shape))\n",
    "            print(\"from get_item of train, returning  x of shape = \" +str(x[:,offset:int(offset+config.rate*self.min_length)].shape))\n",
    "        \n",
    "        return (x[:,offset:int(offset+config.rate*self.min_length)],self.audio_df.loc[idx]['specie_ind'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "032845c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MozErrAnalysisDataset(Dataset):\n",
    "\n",
    "    def __init__(self, audio_df, data_dir, min_length, cache=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            noise_df (DataFrame): the df of noise files and lengths\n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        #self.noise_df = noise_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    \n",
    "    def _get_sample_(self, path, resample=None):\n",
    "        \n",
    "        waveform, inp_rate = torchaudio.load(path)\n",
    "        \n",
    "        if inp_rate != config.rate:\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_rate, config.rate, dtype=waveform.dtype)\n",
    "            waveform = resampler(waveform)\n",
    "    \n",
    "        \n",
    "        #waveform, rate = torchaudio.load(path)\n",
    "                \n",
    "        if waveform.shape[1] < config.rate*self.min_length:\n",
    "            #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "            f_out = pad_mean(waveform)\n",
    "        else:\n",
    "            f = waveform[0]\n",
    "            #mu = torch.std_mean(f)[1]\n",
    "            #st = torch.std_mean(f)[0]\n",
    "            # clip amplitudes\n",
    "            f_out = f.unsqueeze(0)\n",
    "            if self.cache is not None:\n",
    "                self.cache[path] = f_out\n",
    "        return f_out\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #real_idx = idx % len(self.audio_df)\n",
    "        if DEBUG:\n",
    "            print(\"idx = \" + str(idx))\n",
    "        x = self._get_sample_(os.path.join(self.data_dir,f\"{int(self.audio_df.loc[idx]['id'])}.wav\"), resample=config.rate)\n",
    "        \n",
    "        # random noise on even number indexes\n",
    "        offset = int(self.audio_df.loc[idx]['offset'])\n",
    "        \n",
    "        return (x[:,offset:int(offset+config.rate*self.min_length)],self.audio_df.loc[idx]['specie_ind'],offset, self.audio_df.loc[idx]['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8acb3650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MozTestDataset(Dataset):\n",
    "\n",
    "    def __init__(self, audio_df, data_dir, min_length, cache=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_df (DataFrame): from get_offsets_df function \n",
    "            noise_df (DataFrame): the df of noise files and lengths\n",
    "            data_dir (string): Directory with all the wavs.\n",
    "            cache (dict): Empty dictionary used as cache\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.audio_df = audio_df\n",
    "        #self.noise_df = noise_df\n",
    "        self.data_dir = data_dir\n",
    "        self.min_length = min_length\n",
    "        self.transform = transform\n",
    "        self.cache = cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    \n",
    "    def _get_sample_(self, path, resample=None):\n",
    "        \n",
    "        waveform, inp_rate = torchaudio.load(path)\n",
    "        \n",
    "        if inp_rate != config.rate:\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_rate, config.rate, dtype=waveform.dtype)\n",
    "            waveform = resampler(waveform)\n",
    "    \n",
    "        \n",
    "        #waveform, rate = torchaudio.load(path)\n",
    "                \n",
    "        if waveform.shape[1] < config.rate*self.min_length:\n",
    "            #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "            f_out = pad_mean(waveform)\n",
    "        else:\n",
    "            f = waveform[0]\n",
    "            #mu = torch.std_mean(f)[1]\n",
    "            #st = torch.std_mean(f)[0]\n",
    "            # clip amplitudes\n",
    "            f_out = f.unsqueeze(0)\n",
    "            if self.cache is not None:\n",
    "                self.cache[path] = f_out\n",
    "        return f_out\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #real_idx = idx % len(self.audio_df)\n",
    "        x = self._get_sample_(os.path.join(self.data_dir,f\"{int(self.audio_df.loc[idx]['id'])}.wav\"), resample=config.rate)\n",
    "        \n",
    "        # random noise on even number indexes\n",
    "        offset = int(self.audio_df.loc[idx]['offset'])\n",
    "        if DEBUG:\n",
    "            print(\"idx = \" + str(idx))\n",
    "            print(\"shape of x post augmentation = \" + str(x.shape))\n",
    "            print(\"offset = \" + str(offset))\n",
    "            print(\"from get_item of train, returning  x of shape = \" +str(x[:,offset:int(offset+config.rate*self.min_length)].shape))\n",
    "        \n",
    "        return (x[:,offset:int(offset+config.rate*self.min_length)],self.audio_df.loc[idx]['specie_ind'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "868d9ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_aug(x,rate):\n",
    "        apply_augmentation = Compose(transforms=[AddColoredNoise(p = .75) ,TimeInversion( p = .75) ,PolarityInversion(p = .25)])\n",
    "        aug_audio = apply_augmentation(x,sample_rate = rate)\n",
    "        return(aug_audio)\n",
    "    \n",
    "\n",
    "class augment_audio(nn.Module):\n",
    "    \"\"\"This is a class to introduce randomness in the data.\n",
    "    We implement it as a layer in the NN to ensure that it learns from the propertis of the data\"\"\"\n",
    "    def __init__(self , trainable = True, sample_rate = config.rate):\n",
    "        super().__init__()\n",
    "        self.trainable = trainable\n",
    "        self.rate = sample_rate\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(dim = 1)\n",
    "           \n",
    "        if self.trainable:\n",
    "            x = apply_aug(x , self.rate)\n",
    "        else:\n",
    "            x = x\n",
    "#         x = x.unsqueeze(dim=1).permute((0,1,3,2))\n",
    "        return x.squeeze(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fb741d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass the pretrained model and make it a binary classification\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, model_name, image_size):\n",
    "        super().__init__()\n",
    "        # num_classes=0 removes the pretrained head\n",
    "        self.backbone = timm.create_model(model_name,\n",
    "                        pretrained=True, num_classes=8, in_chans=1, \n",
    "                        drop_path_rate=0.2, global_pool='max',\n",
    "                        drop_rate=0.2)\n",
    "        #####  This section is model specific\n",
    "        #### It freezes some fo the layers by name\n",
    "        #### you'll have to inspect the model to see the names\n",
    "                #### end layer freezing\n",
    "        self.spec_layer = features.STFT(n_fft=int(config.NFFT), freq_bins=None, hop_length=int(config.n_hop),\n",
    "                              window='hann', freq_scale='linear', center=True, pad_mode='reflect',\n",
    "                           sr=config.rate, output_format=\"Magnitude\", trainable=True,)\n",
    "        self.out = nn.Linear(self.backbone.num_features, 1)\n",
    "        self.sizer = VT.Resize((image_size,image_size))\n",
    "        self.timeMasking = T.TimeMasking(time_mask_param=int(config.win_size*0.4), iid_masks=True)\n",
    "        self.freqMasking = T.FrequencyMasking(freq_mask_param=int((config.NFFT//4)*0.15), iid_masks=True)\n",
    "        self.norm_layer = Normalization(mode='framewise')\n",
    "        self.pcen_layer = PCENTransform(eps=1e-6, s=0.025, alpha=0.6, delta=0.1, r=0.2, trainable=True)\n",
    "        self.augment_layer = augment_audio(trainable = True, sample_rate = config.rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # first compute spectrogram\n",
    "        if DEBUG:\n",
    "            print(\"input shape that goes for augmentation = \" + str(x.squeeze().shape))\n",
    "        spec = self.augment_layer(x.squeeze())\n",
    "        if DEBUG:\n",
    "            print(\"Out put of augment and input shape that goes for STFT = \" + str(spec.shape))\n",
    "        spec = self.spec_layer(x)  # (B, F, T)\n",
    "        # normalize\n",
    "#         spec = spec.transpose(1,2) # (B, T, F)\n",
    "        if DEBUG:\n",
    "            print(\"Out put of STFT and input shape that goes for PCEN = \" + str(spec.shape))\n",
    "        spec = self.pcen_layer(spec)\n",
    "        if DEBUG:\n",
    "            print(\"Out put of PCEN and input shape that goes for NORM = \" + str(spec.shape))\n",
    "        spec = self.norm_layer(spec)\n",
    "        \n",
    "        if DEBUG:\n",
    "            print(\"Out put of NORM and input shape that goes for time mask = \" + str(spec.shape))\n",
    "        spec = self.timeMasking(spec)\n",
    "        if DEBUG:\n",
    "            print(\"Out put of timemask and input shape that goes for freq mask = \" + str(spec.shape))\n",
    "        spec = self.freqMasking(spec)\n",
    "\n",
    "        # then size for CNN model\n",
    "        # and create a channel\n",
    "        spec = self.sizer(spec)\n",
    "        x = spec.unsqueeze(1)\n",
    "        # then repeat channels\n",
    "        if DEBUG:\n",
    "            print(\"Final shape that goes to backbone = \" + str(x.shape))\n",
    "        \n",
    "        x = self.backbone(x)\n",
    "        #print(\"x shape = \" + str(x.shape))\n",
    "        #print(\"x = \" +str(x))\n",
    "        #pred = nn.Softmax(x)\n",
    "        pred = x\n",
    "        #print(np.argmax(pred.detach().cpu().numpy()))\n",
    "        #print(pred)\n",
    "        output = {\"prediction\": pred,\n",
    "                  \"spectrogram\": spec}\n",
    "        #print(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0e7566e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/audio\n"
     ]
    }
   ],
   "source": [
    "print(config.data_dir)\n",
    "train_dataset = MozTrainDataset(df_train_offset,  config.data_dir, min_length , transform = None)\n",
    "val_dataset = MozTestDataset(df_val_offset,  config.data_dir, min_length)\n",
    "test_dataset = MozTestDataset(df_test_offset,  config.data_dir, min_length)\n",
    "error_dataset = MozErrAnalysisDataset(df_val_offset,  config.data_dir, min_length = config.min_duration)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, num_workers=num_workers,batch_size = batch_size,shuffle = True\n",
    "    , pin_memory=True )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size,\n",
    "        num_workers=num_workers, pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size,\n",
    "        num_workers= num_workers, pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "error_loader = torch.utils.data.DataLoader(\n",
    "        error_dataset, batch_size=batch_size,\n",
    "        num_workers= num_workers, pin_memory=pin_memory,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046358e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0dd0dad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset = 32239\n",
      "Length of train loader = 504\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of train dataset = \" +str(len(train_dataset)))\n",
    "print(\"Length of train loader = \" +str(len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b886dc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_itr = iter(train_loader)\n",
    "# a,b = train_itr.next()\n",
    "# print(a.shape)\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6ed6fb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spec_layer = features.STFT(n_fft=int(config.NFFT), freq_bins=None, hop_length=int(config.n_hop),\n",
    "#                               window='hann', freq_scale='linear', center=True, pad_mode='reflect',\n",
    "#                            sr=config.rate, output_format=\"Magnitude\", trainable=True,)\n",
    "# x = spec_layer(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd4b13da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_mod = Model('convnext_small',224)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4fbb3bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_mod(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70507228",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "84ad550c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling rate = 8000. Please make sure the sampling rate is correct in order toget a valid freq range\n",
      "STFT kernels created, time used = 0.0785 seconds\n"
     ]
    }
   ],
   "source": [
    "def load_model(filepath, model=Model('convnext_small',224)):\n",
    "    # Instantiate model to inspect\n",
    "    print(\"Filepath = \" + str(filepath))\n",
    "    print(\"model = \" +str(model))\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "    print(f'Training on {device}')\n",
    "        \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using data parallel\")\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "    model = model.to(device)\n",
    "    # Load trained parameters from checkpoint (may need to download from S3 first)\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        map_location=lambda storage, loc: storage.cuda()\n",
    "    else:\n",
    "        map_location='cpu'\n",
    "        \n",
    "    checkpoint = model.load_state_dict(torch.load(filepath))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa94dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling rate = 8000. Please make sure the sampling rate is correct in order toget a valid freq range\n",
      "STFT kernels created, time used = 0.0856 seconds\n",
      "Filepath = ../../models/model_e15_2022_09_30_03_47_23.pth\n",
      "model = Model(\n",
      "  (backbone): ConvNeXt(\n",
      "    (stem): Sequential(\n",
      "      (0): Conv2d(1, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "    (stages): Sequential(\n",
      "      (0): ConvNeXtStage(\n",
      "        (downsample): Identity()\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "            (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ConvNeXtStage(\n",
      "        (downsample): Sequential(\n",
      "          (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "            (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ConvNeXtStage(\n",
      "        (downsample): Sequential(\n",
      "          (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (3): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (4): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (5): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (6): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (7): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (8): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (9): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (10): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (11): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (12): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (13): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (14): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (15): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (16): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (17): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (18): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (19): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (20): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (21): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (22): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (23): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (24): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (25): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (26): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "            (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): ConvNeXtStage(\n",
      "        (downsample): Sequential(\n",
      "          (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (1): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "          (2): ConvNeXtBlock(\n",
      "            (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "            (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm_pre): Identity()\n",
      "    (head): Sequential(\n",
      "      (global_pool): SelectAdaptivePool2d (pool_type=max, flatten=Identity())\n",
      "      (norm): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (drop): Dropout(p=0.2, inplace=False)\n",
      "      (fc): Linear(in_features=768, out_features=8, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (spec_layer): STFT(n_fft=2048, Fourier Kernel size=(1025, 1, 2048), iSTFT=False, trainable=True)\n",
      "  (out): Linear(in_features=768, out_features=1, bias=True)\n",
      "  (sizer): Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
      "  (timeMasking): TimeMasking()\n",
      "  (freqMasking): FrequencyMasking()\n",
      "  (pcen_layer): PCENTransform()\n",
      "  (augment_layer): augment_audio()\n",
      ")\n",
      "Training on cuda:0\n",
      "Training on cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07fd1e725fb643a390776ba391183942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0batch = 0 of 504duraation = 0.058877766132354736\n",
      "epoch = 0batch = 200 of 504duraation = 1.8957700570424398\n",
      "epoch = 0batch = 400 of 504duraation = 3.7006539662679034\n",
      "Saving model to: ../../models/model_e0_2022_09_30_04_47_29.pth\n",
      "Now printing classification rport... \n",
      "********************************\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "        an arabiensis       0.70      0.76      0.73      4356\n",
      "culex pipiens complex       0.51      0.59      0.55      1879\n",
      "           ae aegypti       0.60      0.33      0.43       439\n",
      "       an funestus ss       0.60      0.54      0.57      1959\n",
      "         an squamosus       0.28      0.20      0.23       507\n",
      "          an coustani       0.39      0.34      0.36       312\n",
      "         ma uniformis       0.32      0.27      0.29       441\n",
      "         ma africanus       0.39      0.30      0.34       194\n",
      "\n",
      "             accuracy                           0.60     10087\n",
      "            macro avg       0.47      0.42      0.44     10087\n",
      "         weighted avg       0.59      0.60      0.59     10087\n",
      "\n",
      "********************************\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAFdCAYAAAAwtwU9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAACCGElEQVR4nO3dd3gUVRfA4d8hofcmVQUBsdKk9450UAQVRRRFEcSGiooFbKCiop8NVJpIU3rvXSCU0AkgWOiogDSBJOf7YyZhEwIksju7hPPy7MPundm5Z0v2zr1zi6gqxhhjjAmeNMEOwBhjjLnaWWFsjDHGBJkVxsYYY0yQWWFsjDHGBJkVxsYYY0yQhQc7AJO6nP1zZ0h0z69ZulOwQ2D1n9uDHQIAN+W8Ntgh8PeZY8EOAYATZ/4NdgjEaGywQwDg1NnTwQ6Bs2f2yGUfIwW/OWnz3HDZ+QWKFcbGGGOuXLExwY7AL6wwNsYYc+UKkZaGy2WFsTHGmCtXrBXGxhhjTFCp1YyNMcaYIIuJDnYEfmGFsTHGmCuXdeAyxhhjgsyaqY0xxpggsw5cxhhjTHCllg5cNh3mFUJEiojIxgts+0ZEbvFjXgVF5Ed/He/06TPc++jT3PXQk7Rs/zj/+2Y4AK+99zF3PfQkrTt04dlX3+bkyVMAjB4/ldYPduHuh7ryYJfn+WXXbwAcOfoPD3d7iQr1W/NO/y8uO640adIwdOZAPhz6LgB3VCvLkBlf8/3c73jtk56EhTl/HlmzZ6HvN30YPvsbvp3yBTeULHLZeSdWuHABZs0cw7rIeUSunUu3bs4MYiO+/4KIlTOJWDmTbVE/E7Fypt/zBue9GD17KJ8N/xCAQtcVYMS0b5jy81je//otwtM65+1p06Xl/a/fYsrPYxkx7RsKXpvfL/kXKJSfMRO/Y97PE5m7bAKdHn8AgB6vdGP24nHMXPgjI34aSL78eQHInj0b3wwbwOzF45gyeyQlby7ulzg+++I9tu1awbKV0+LTXnntGZYsn8KiZZP4aeIQ8ue/BoB72rZgyfIpLF0xlZlzxnDbbTf5JYbPv+zHL7+uZHnE9Pi0Vq0bsyJiBkeO7aBs2dvj0+vUrc7CJRP5eeV0Fi6ZSM1aVfwSQ2KDBvZnz+51rF07N0F61ycfZsOGhURGzuO9914NSN6XFBub/FsIE9WQmL3QACISrqpJdg0UkSLAFFW9zduoUiapqelUlVOn/iVTpoycjY6mQ5ce9Hz6cYoVvY4smTMD8P6nA8mVMwePPtiW4ydOxKfPX7ycUeOn8PVHb3Py1L9s3baD7Tt/Y8fO33j1+ScvGEdypsO8t/M93FyqJJmzZuKFjq8yfuUonmr3PH/s3M1jPR5m/+4DTB41jW69HufkiVN89/Ewri92LT3efYan2j1/yeOnZDrM/PmvIX/+a4iM3EiWLJlZsXw6bdp0YsvWc8fo1+81/jl6jHfe/STZx4XkTYf54OP3cmvpm8mcNTNPPdiDDwa+zdypC5gxcQ69+r3Its3bGTN0PO063kWJm4vz9kvvc2fL+tRtUosXH3/tkse/1HSY1+TLwzX58rJx/RYyZ8nE9Hlj6PRgd/btPcDxYycAeKRze0qULMbLz/ehV+/nOXHiJB+//yXFShTlnfdf5d7Wj14yjktNh1m1WgWOHz/JV4M+oGrFJgBkzZqFY8eOA9C5Swduuqk4zz39OhUrlSUq6heOHvmH+g1q8tIr3WlQp80lY7jUdJhVq1XgxImTfD3oQypXaAzAjSWLERsby4BP36HXK++xdu0GAEqVvoWDB/5k//6D3HzLjYyfOISbSlS9ZAyQsukwq1evxInjJ/hu8ADKlq0HQK1aVXm5Z3datOzAmTNnyJs3N4cO/ZXsY4J/psM8vXVhsgux9DfVCtnpMK1mnAIiMkFEVovIJhHp7JN+XETeEZF1IrJcRPIl8dyKIvKziKwVkWUiUtJN7ygik0RkHjBXRLKIyFwRWSMiG0Skpc9hwkVkhIhsEZEfRSSTe4wFIlLevd/QzWeNiIwVkSxu+q8i0tvnuDe56bVEJNK9rRWRrL61cBG5VURWutvXi0iJ//C+kSlTRgCio6OJjo5GROILXFXl39OnEffPJC4d4NS//yLuhkwZM1Cu9G2kT5cupSGcJ2+BPFSrV5lJI6cCkD1nNs6eOcsfO3cDsHLRKmo3qQFAkRuLsHrpWgB+++UP8hfOR848OS87Bl/79x8kMtJp+Dh+/ARbt26nYKGEtc42dzdn9JiJfs0XIF+BvNSsX41xIybFp1Wsdgezp8wHYNKYadS5syYAtRvVYNIYp9Y4e8p8KlUv75cYDh74k43rtwBw4vhJtm/bSf4C+eILYoCMmTISV3koUbIYSxetAOCX7bsofF0h8uTNfdlxLFsaweHDRxKkxRXEAJkzZYqPYeWKtRw98g8AERGR531elxXD3wlj2Bb1Czu27zpv3/XrNrN//0EAtmzeRsYMGUjnh7+PxJYsWcHfid6Xxx/vwPsffM6ZM2cAUlwQ+43GJv8WwqwwTplHVPUOoDzQXUTi/vozA8tVtTSwCHgsieduBWqoalngdeBdn23lgDaqWgv4F2itquWAOkB/iSuNoCTwhareDPwDJKgaikgeoBdQ333+KuA5n13+dNO/BHq4aT2ArqpaBqgBnEoU9xPAAHd7eWD3Rd6fC4qJieHuh7pSs9l9VKlQllK3Ok16vd75iFrN72fXb7u5v02L+P1H/jSZO+95mP5ffMvLzzzxX7K8qGd6d+N/b39NrNt0deTvo4SFh3FTqRsBqNO0FvkKOs2ROzb/El8w31LmJvIXzs81BfL6PaY4119fmNKlb2PlyrXxadWrV+LgwUPs2HH+D/LlevGtZ/jorf8R6/5Y5ciVnWP/HCcmxhkycmDfQfK5rzdfgbwc2HsAcD7T48eOkyNXdr/GU/jagtxW6mbWrl7vxPdqd1ZumEPre5ry4Xv/A2DzxigaN68PQJlyt1H42gIUKHjeObDf9HrjOTZuXcw97Vrw7tsDztv+YId7mDNrUcDyT46WrRoTuW5TfOEYaDeWuIHq1SuydMlk5s75kfJ3lPYk3/OkkmZqK4xTpruIrAOWA9cCcbXEM8AU9/5qoEgSz80OjHVrnB8Dt/psm62qf7v3BXhXRNYDc4BCQNyvzB+qutS9/z1QPVEelYFbgKUiEgk8BFzvs31cEjEuBT4Ske5AjiSayX8GXhGRl4DrVTVxYY2IdBaRVSKy6pthI5N46RAWFsZPQz9n7vjhbNi8je07fwXg7VefY/7E77mhyLXMmHvux+y+u5szY+xgnuvyCF8PSfqY/1W1+pU5/OcRojZsS5D++pNv8fSbXfl2yhecPHGSGPePd9j/fiBLtiwMnTWINo+0ZtvG7cQGaGxj5syZGD1qID16vJmgRtauXcuA1IprNqjG338eZsv6KL8f+7/IlDkjA4d+zJuv9IuvFb//zqdUvL0+48dO5eHH7gfg8wHfkC17VmYu/JGHH2vPxvVb408eAuHt3h9x2001GDt6Eo89/mCCbdVrVuaBh+7hzdffD1j+l3LTzSXo89aLPPOUd9dtw8LDyJUzB9WqN6dnz7f54YevPMs7gVRSM7be1MkkIrWB+kAVVT0pIguADO7ms3ru4nsMSb+vbwHzVbW1e/13gc+2Ez732wN5gTtU9ayI/OqTT+JrI4kfC07Bft8FXkbcRaL4GFW1r4hMBZrgFOKNcGrnuNt/EJEVQFNgmog8rqrzEgShOhAYCJdezixb1ixULFeKJctXUeKGIoBTUDeuX4vvRvxI66YNE+zfuH4t3vrwfxc7ZIqVKn8bNRpWpWrdSqRLn47MWTPxxqev0Lv7u3S562kAKtYsz3U3ONdaTx4/yTvPnfuhHbd8JHt+2+fXmADCw8MZPXogI0eNZ8LEc513wsLCaNWyMZWrNPF7nmUqlKJ2wxpUr1eV9OnTkTlLZl5661myZstCWFgYMTEx5CtwDQf2HQLgwL5D5CuYjwP7DhEWFkaWrFk48vdRv8QSHh7OwKGfMP7HqUyfMue87ePHTmHYmC/p3/dzjh87wfPdzl2r/jlyJr//9p8abVJk7OiJjBn3LX3fcWrHt95akk//9y733PXIeU3LXilYMD8/jPyKzo/1YNeu3z3Ld8/ufYyf4HxPI1ZFEhsbS548ufjzz78v8Uw/C/Eab3JZzTj5sgOH3YL4JpxaaEqfv8e93/ES+x10C+I6JKzZXicicd0l7weWJHrucqCaiBQHEJHMInLjxYISkWKqukFV+wERwE2Jtt8A7FTVT4GJQKmLHS8pfx8+wj9uLe/f06f5OWItRa8rzO+79wLONeP5S5ZT9PrCAPz2x5745y5atpLrChdKaZYX9WXfb2hZvi13Vb6P157sw+qla+nd/V1y5s4BOD2GH+x6H+OHO9dQs2TLHN+buMX9TYlcsZ6Tx0/6NSaAgV9/yNatOxgwYFCC9Hr1ahAV9Qt79vj/BODTd7+kQbmWNK5wFy8+8Rorl67m5a5vErFsDQ2a1QGgRdsmLJi5GIAFs5bQoq1zUtCgWR1WLl3tt1g+/LQPO7btZNAXw+LTit5wXfz9Rk3q8ot73TRbtqykdT+T+zvczYplqxNcX/anG4qd+xNs3Kw+27btBJwe8MN++IInHnueX3b8GpC8LyV79qyMHfctb7z+PiuW+++zSI5Jk2ZSu7bTWaxEiRtIly6d9wUxoLFnk30LZVYzTr4ZwBMisgWIwin4UuJ9YKiI9AKmXmS/EcBkEdmAc813q8+2KKCriHwHbMa59htPVQ+JSEdgpIikd5N7AQnbYxN6xi30Y4FNwHSggM/2tsCDInIW2E/Ca93Jcuivw7z69ofExMaisUqjujWoWbUiHZ58gRMnTqKqlCxelNde6AbADz9NZnnEWsLDw8mWNQvv9jrXc7nh3Q9x/MRJzkZHM2/xMgZ+/A7Fil5/oaxTpH2XdlSrXwVJI4wfNim+01aREtfz2ic9UVV2Rf3Kuz0+8Et+vqpWrcADD7Rhw4Yt8cOXXnu9HzNmzKPtPS0YPWaC3/O8mI/f+pz3v36Lbj0fZ+vGbYz7YTIA43+YzLv/e4MpP4/l6JF/ktWTOjkqVCpLm3tbsGXTNmYudEbV9XtrAPc+eBc3FC+Cxiq7/9jLy8/3AaB4yRv45PN3UFW2bf2FHt1f90sc3wz+mGo1KpE7d042Ri2h7zsDaNCoFiVK3EBsbCx//L6X5552XvMLPZ8iV64cfPhxbwCio2OoW7P1Zcfw3ZABVHdj2LJtKe++PYDDh4/wQf83yJMnF2PHfcuG9Ztp3bIjnR/vwA03XM9LLz/FSy8/BUCrFg/xp587Uw0f/jm1alYhT55c7Nq5ij59PmTwkFF8M6g/a9fO5eyZszzS6Rm/5plsqaRmbEObjF9dqpnaK8kZ2hRoKRnaFEjJGdoUaJca2uSVSw1t8sKlhjZ5JSVDmwLFH0Ob/l09Idm/ORnuaGVDm4wxxhi/i41J/u0iRCSDO4xznTt8tbebXlREVojIDhEZLSLp3PT07uMd7vYiPsd62U2PcvvhXJIVxsYYY65c/utNfRqo6w5RLQPcKSKVgX7Ax6paHDgMxDW7dcLpR1QcZ4RMPwB3NsR7cUbM3Al8ISJhl8rcCmNjjDFXLj+NM1ZH3HjCtO5NgbpA3PTAQ4FW7v2W7mPc7fXcOSFaAqNU9bSq7gJ2ABUv9TKsMDbGGHPliolO9s13TgT31tn3UCIS5s7RcBCYDfwCHPGZf2E3ztwPuP//AeBuPwrk9k1P4jkXZL2pjTHGXLlS0Jvad06EC2yPAcqISA5gPImGegaSFcbGGGOuWE756e9j6hERmQ9UAXLIuUV8CnNuvog9ODMx7haRcJw5Iv7ySY/j+5wLsmZqY4wxVy4/XTMWkbxujRgRyQg0ALYA84G45bgewpn8CGCS+xh3+zx3JsZJwL1ub+uiONMmr7zUy7CasTHGmCuX/8ZtF8CZmCkMp6I6RlWniMhmYJSIvA2sBb519/8WGC4iO4C/cXpQo6qbRGQMzsRM0TgL8Vyy+m6FsTHGmCuXn2bgUtX1QNkk0neSRG9oVf0XuOcCx3oHeCcl+VthbPyqbumkVo/0XrPwgsEOgbXyS7BDAEAI/qRDB44fDnYIwPkrqwRDWBq7OuhXMYkXmrsyWWFsjDHmyhUi04teLiuMjTHGXLlSyUIRVhgbY4y5cllhbIwxxgSZNVMbY4wxQWYduIwxxpggs2ZqY4wxJsismdoYY4wJslRSM75iRp+LSBER2RjgPJYlY59pcfOXXulEZIGIlPc63zHLRzBkziC+m/U1g6Z9AcCTvTrz/cLBDJk9iHe+6U2WbJkTPOeagtcwc9sU7n08yQlvkqXZB4/xzOoveGxW3/i0m5pUpPPsfryyazgFbi8an54xRxbaj3qVFzZ/S6M+DyU4zgOjXuWJeR/w6LR3eXTau2TKne0/x+QrKmopq1bNYsWK6SxdOgWAN954noiImaxYMZ0pU76nQIF8fskrsTRp0jB69hA+G/4BAIWuK8D30wYx+ecxvP91H8LTOuft+Qvl45ufPmP07CGMnTeM6vWq+D2WwoULMnvWWNatm09k5Dye6tYpflvXJx9mw4aFREbO4733XvV73hfzVLdORK6dy7rIeXR/6lFP806TJg0rlk9n/LjBABQpci2LF01i86bFfD/8C9KmTRvQ/AcN7M+e3etYu3ZufNqIEV+yKmIWqyJmsX3bclZFzApoDBfkp7mpg+2KKYy9oKpVk7FPE1U94kE4qdrT9zzPIw0f57EmTwIQsWg1D9XtRMcGj/HHzt080O3+BPs/9WYXVsy/5FzrF7Vu7GJGPfR+grRD23bz4+Of8PuKrQnSo0+fZeGHY5n7zg9JHmvi01/wTZNX+KbJK5z865/ListXo0btqFSpMdWqNQPgo4++pkKFRlSq1Jhp0+byyitP+y0vX+0fa8vO7b/GP36615N8//Vomldpyz9HjtH6/uYAPPZMR2ZOmke7Bh156YnXeaVvD7/HEh0dzYsv9qZ06TpUr96cJ7p05OabS1CrVlWaN2/EHXc0oEyZunz00Vd+z/tCbr21JJ063U+Vqk0pd0cDmjapT7FiRTzL/6lundgatSP+8Ttvv8ynn33DLbfW4MiRIzzc8d6A5j902BiaNWufIK19+y6Ur9CQ8hUaMn78NMZPmBbQGC5INfm3EBbUwlhEOojIehFZJyLD3bQhItLGZ5/jSTwvTEQ+EJEI9/mPu+nPish37v3bRWSjiGRK9NyOIjLRrRVuF5E3EuclIrVFZJGITBWRKBH5SkTSuNt+FZE87v0HRGSliESKyNfuBOOIyHERecd9XctFJJ+bfo8b0zoRWXSB9+QlEdng7tPXTSvjHme9iIwXkZxu+gIR+dhdJHuLiFQQkXHu63rb3aeIiGwVkRHuPj8mfk/c/RqKyM8iskZExopIFhG53j1WHhFJIyKLRaRhcj/flIhYtJqYGOfMddOazeQtkCd+W41G1dj3+z52Rf16WXn8sXIrp44k/Dr9tWMvf+/cd96+Z0+dZveqbUSfPntZeV6uY8fOxZs5cyY0AD8o1xTIS436VRk/YnJ8WsVqdzB7ynwAJo2ZTt07azobVMmS1Wm1yJI1C4f2/+n3ePbvP8jaSKcR7PjxE2zdup2CBfPz+OMdeP+Dzzlz5gwAhw795fe8L+Smm0qwcuVaTp36l5iYGBYtXk7rVo09ybtQofw0blyXwYNHxqfVrl2NceOmAjD8+x9p0aJRQGNYsmQFfx8+csHtbdo0Z/ToiRfcHlDR0cm/hbCgFcYicivQC6irqqWBlJzydwKOqmoFoALwmLtU1QCguIi0BgYDj6vqySSeXxG4GygF3HOBptqKwFPALUAx4K5E8d8MtAOqqWoZIAaIO3XMDCx3X9ciIG7C5teBRm56i8QZikhjoCVQyd0nrho3DHhJVUsBG4A3fJ52RlXLA1/hLO3VFbgN6Cgiud19SgJfqOrNwD/Ak4nyzYPzWdRX1XLAKuA5Vf0N6Ad8CTwPbFbVy26LUlU+Gvk+30z/kubtm563vem9jVkxPwKAjJkycH/Xexn80bDLzdavmn34OI9Oe5fq3Vv57ZiqypQp37Ns2VQ6dTrXMtC79wvs2LGce+9tRZ8+/f2WX5wX33qGj9/6nFi3I0yOXNk59s9xYmKchWYO7DvINQXyAvDlh9/S9O5GzFozgc9HfEjfVz/yezy+rr++MGVK38bKlWu5scQNVK9ekaVLJjN3zo+Uv6N0QPP2tWnTVqpXr0SuXDnJmDEDje+sS+HC3sx//uEHb/LyK+8S6zaz5s6dk6NH/4n/fPbs2UfBgvk9iSUp1atX4uDBQ+zYsSs4AWhs8m8hLJg147rAWFX9E0BV/07BcxsCHUQkElgB5AZKqGos0BEYDixU1aUXeP5sVf1LVU8B44DqSeyzUlV3uktfjUxin3rAHUCEG0c94AZ32xlgint/NVDEvb8UGCIijwFhSeRZHxgcdwKhqn+LSHYgh6oudPcZCtT0ec4k9/8NwCZV3aeqp4GdnFvg+g+f9+L7JF5LZZyTjqXua3kIuN6N4RsgG/AEkGSbpIh0dmvnq/afuOQa2nRt/Qyd7nyCHg+8zF0dW1K60u3x2x7sfj8x0THMGjcHgIeff4gxg37k1Ml/L3lcr0x8+gsGNerJsHv6cG2Fm7j9rqS+PilXt+7dVKnSlJYtO/D44x2oXt1ZKOaNNz6gePHKjBo1gS5dOvolrzg1G1Tl7z8Ps2V9VLL2b9y6AZNGT6NhuVZ0bd+Dd/73OiKBWYgic+ZMjBk9iOd7vMGxY8cJCw8jV84cVKvenJ493+aHH7xrpt66dQcffPA506f9wLQpI4hctym+JSeQmjSux6FDf7F27YaA5/Vf3duuFaOCVSuGVHPNOBR7U0fjniS4TcPpkthHgKdUdWYS20oAx4GLnbYmbutLqu3vUvsIMFRVX07iuWf1XHtiDO77rKpPiEgloCmwWkTuUNXLbWs77f4f63M/7nHc55uc1zJbVe9LfHC3Sbuw+zALcCzxPqo6EBgIUKNQvUu2o/7pNm0e+esIi6Yv4eYyN7FuxQYat21E1fpVeKbtuTL/lrI3U7tpTbq82pks2bKgsbGcOX2GcUOC98d/7ICzAtGZE/+yaeIyCpYpxoZxSy77uHv3HgCc5tdJk2ZSvnwZliw5d5181KjxTJgwlLfe8l9ttEyFUtRuWJ3q9aqQPn06MmfJzItvPUPWbFkICwsjJiaGfAWu4eC+QwC0vr8ZXe57DoD1qzeSPn06cubOwd9/+ndVpvDwcMaMHsTIkeOZMGE6AHt272O8ez9iVSSxsbHkyZOLP/9MyXn8fzd4yCgGDxkFwNtv9WT37vMvb/hblarladq0AY3urEOG9OnJli0r/fv3Jnv2bPGfT6FCBdi7d3/AY0lKWFgYrVo1plJlb5rskxTi14KTK5g143k4TcS5AUQkl5v+K06NE5ym3KS6Cc4EuohIWve5N4pIZrcW+SlOzTG377XnRBqISC4RyQi0wqmxJlZRRIq6JwTtgMS/tnOBNiJyTVz8InL9xV6wiBRT1RWq+jpwiHM11zizgYfjrumKSC5VPQocFpEa7j4PAgtJmetEJK7b6/1JvJblQDURKe7mm1lEbnS39QNG4DSxD0phvufJkDEDGTNnjL9foVZ5dkb9SsXaFbi/Szte7tiL0/+eO6fodtcztK3cnraV2zP2m58Y/tkPQS2IJSwNGXNmASBNeBjF65XlUNTuyz5upkwZyZIlc/z9evVqsGlTVIJOQs2aNSQqyr/LMn767lc0LNeKJhXu5qUnXidi6Wpe6dqbiGVraNCsDgAt2jZm/szFAOzbc4BKNZyrOkVLXE+69On8XhCD03t369YdfDJgYHzapEkzqV3b6WNZosQNpEuXzrOCGCBvXueqz7XXFqRVq8aMHDU+4Hm+9lo/ihWvSMmSVXmwQ1cWLFhKx47dWbhwGXfd5VziefCBNkyeHJyezPXq1SAqagd79gT+xOSCrGZ8eVR1k4i8AywUkRhgLU4T8yBgooisA2YAJ5J4+jc4Tb9rxGkjO4RTqH4MfK6q20SkEzBfRBap6sFEz18J/IRT4/teVVclkUcE8D+gODAfSPCXp6qbRaQXMMstsM/iXK/97SIv+wMRKYFTE50LrEt0zBkiUgZYJSJngGnAKzjNxl+5hfRO4OGL5JGUKKCrOJ3bNuNcA/bN95CIdARGikh6N7mXiBTAuSZfTVVjRORuEXlYVQenMP94OfPm5N1vewPOWfXsCXNZuSCCkUuGkTZ9Wj4a5Vwm37RmC/17fvJfs0lSq0+7cn2Vm8mYMytPLf+MRR//yL9HTtCw90NkypWVtoNf4MDm3xjVoR8AXZd8QvqsGQlLG86NDcsz8sG+HN39J/cN70ma8DDShKVh15KNrB0577Jjy5cvL6NHOwVPeHg4o0dPYPbshYwc+RU33liM2NhYfv99D089lVRDjP998tYXvP91H7r27MzWjdsY/4PTuav/m5/x+oc9eaBzO1SV159O0frpyVKtagUeeKANGzZsjh8u0+u1vgweMopvBvVn7dq5nD1zlkc6PeP3vC9m7OhB5Mqdk7Nno+ne/VWOHvVfL/qUerXXewwf9jm933yByMiN8TX2QBk+/HNq1axCnjy52LVzFX36fMjgIaNo17Zl8DpuxQnxQja5JBC9M0OZW+iUV9VuF9mnNtBDVZt5FFbAiEgRYIqq3uZFfslppvbCneHB69AS560Di4MdAgAlcxS+9E4Btvnvi52jeicUvpxhaUJjRGlsCBRiZ8/suewOBycHPpvsjzVT548D08HBD0LxmrExxhiTPCFwUuEPV11hrKpDgCGX2GcBsCDw0QSeqv6KM9TJGGNSnxAfspRcV11hbIwxJhWJDYWLD5fPCmNjjDFXLmumNsYYY4LMnYnsShca3fqMMcaY/8JP44xF5FoRmS8im0Vkk4g87aa/KSJ73DUIIkWkic9zXhaRHeKsYdDIJ/1ON22HiPRMzsuwmrExxpgrl/+uGUcDz6vqGhHJijNL4mx328eq+qHvziJyC3AvcCvOjI9zfCZL+hxoAOzGmTJ5kqpuvljmVhgbY4y5cvmpN7Wq7gP2ufePicgWoNBFntISGOWuBbBLRHbgLDAEsENVdwKIyCh334sWxtZMbYwx5soVq8m/JZM7WVJZnIWIALq5S9h+F7eELU5B/YfP03a7aRdKvyirGRu/Wn5oa7BDAGA5wY8ja/rzlo0Oiq1H/rj0TgGWOgaf+EdMKun9Gyo0Be+niHQGOvskDXQXuvHdJwvOdMnPqOo/IvIl8BbO1/gtoD/wyOXGnZgVxsYYY65cKehN7bvCXFLcxYd+Akao6jj3OQd8tg/i3PK4e0i42E9hN42LpF+QNVMbY4y5cvmpmdpddOhbYIuqfuSTXsBnt9bARvf+JOBeEUkvIkVxlu9dibPIUAl31b90OJ28JnEJVjM2xhhz5fJfs381nCVqN4hIpJv2CnCfu5qe4izx+zjErzw4BqdjVjTQVVVjAESkG85Sv2HAd6q66VKZW2FsjDHmyuWnoU2qugRnedvEpl3kOe8A560jqqrTLva8pFhhbIwx5splC0UYY4wxQWYLRRhjjDHBpdGpY25qK4yNMcZcuVJJzdiGNpkLEpFXEj1e5u88Bg3sz57d61i7dm58WqlSt7B40STWrpnD+PFDyJo1i7+zTaBw4YLMnjWWdevmExk5j6e6dQKg73u92LBhIWtWz2bs2G/Inj2b3/P+7Iv3iNq5nKUrpp63retTj/D3se3kyp0zPq1a9YosXDqJZSunMXn6CL/HEydNmjSsWD6d8eMGAzBo0EdEbV3KyhUzWLliBqVK3RKwvC8UT8TKmUwcP9TTfH01alibTRsXsXXzEl58oWtQYsiePRujRw1k44aFbFi/gMqV7ghKHKHwXsTT2OTfQpgVxuZiEhTGqlrV3xkMHTaGZs3aJ0j7+qsPeOXVdylbrj4TJ0zn+ee7+DvbBKKjo3nxxd6ULl2H6tWb80SXjtx8cwnmzF1EmTJ1KXdHA7Zv38lLL3Xze94/jBjHPa3Pn8ynUKH81KlbnT9+PzdXQLbsWfnw497c3+5xqlZswsMPPuX3eOI81a0TW6N2JEjr+fI7VKx0JxUr3cn69RedZtfvuj/1KFu3bvc0T19p0qTh0wHv0Kz5A9xeug7t2rXi5ptLeB7Hxx/1YebM+dx2ey3K3dGALUF4T0LlvYgXgOkwg8EKYw+IyAQRWe0uy9XZJ72hiPwsImtEZKw7DVvi5z4mIhEisk5EfhKRTG56XvdxhHur5pM+283rGxH5TUTyiEgfEXnG57jviMjTIlJbRBaJyFR3ya+vRCSNiPQFMrpLho1wn3Pc3+/NkiUr+PvwkQRpJUrcwOLFywGYM3cxrVs3SeKZ/rN//0HWRjrj+I8fP8HWrdspWDA/c+YsIsad3WfFijUULlTgYof5T35eGsHhw0fPS3+n76u88dr7qJ77AWlzT3MmT5rFnt37APjzz7/9Hg84JwKNG9dl8OCRATl+ShUqVIAmjevx3XfBi6dihbL88suv7Nr1O2fPnmXMmIm0aN7o0k/0o2zZslKjeiW+cz+Xs2fPcvToP57GAKHxXvjSWE32LZRZYeyNR1T1DqA80F1EcotIHqAXUF9VywGrgOeSeO44Va2gqqWBLUAnN30AzrJeFYC7gW/c9DeAeap6K/AjcJ2b/h3QAUBE0uDMCvO9u60i8BRwC1AMuEtVewKnVLWMqiasugbY5s3baNHC+eNuc3czri1c0LO8r7++MGVK38bKlWsTpHfseC8zZs73JIbGTeuxb+8BNm1MOL928eJFyZEjG5Omfc+8ReNpd1+rgOT/4Qdv8vIr7xKbaDKFPr1fZFXELD54/w3SpUsXkLyT8lH/3vR8+e3z4vFSwUL5+WP33vjHu/fso2DB/J7GULTodfz55198+83HRKycyddffUCmTBk9jQFC471IIDom+bcQZoWxN7qLyDpgOc6cpSWAyjiF31J3tpeHgOuTeO5tIrJYRDYA7XHWzgSoD/zPfe4kIJtbs64OjAJQ1RnAYff+r8BfIlIWaAisVdW/3GOtVNWd7uwxI91jJJuIdBaRVSKyKjb2REqemqTHOj/HE48/xIrl08mSNTNnzpy97GMmR+bMmRgzehDP93iDY8fONQL07Nmd6OhofvhhXMBjyJgxA88934V33/nkvG1h4WGUKXsb97Z5jDatH6HHi10pVryIX/Nv0rgehw79xdq1GxKkv/ZaX24vVZuq1ZqRM1d2evQI7KWDOE2b1OfgwT9Zkyieq1F4WBhly97O118Po0LFRpw4cZKXXvT/pZMrTippprbe1AEmIrVxCs4qqnpSRBYAGXBmepmtqvdd4hBDgFaquk5EOgK13fQ0QGVV/TdRfhc71jdARyA/Tk05TuJvaYq+tb6Tr6dNV+iyv/FRUb/QpOn9gNNk3aRxvcs95CWFh4czZvQgRo4cz4QJ0+PTOzzYlqZN6tOwUduAxwBQpOh1XFekMIuXTQacWsiCxROoX/tu9u7dz+G/j3Dy5ClOnjzFz8siuO22m/hlx69+y79K1fI0bdqARnfWIUP69GTLlpXBgwfw8MNPA3DmzBmGDRvDs8887rc8L6Zq1fI0b9aQxnfWJUMGJ56hQz7loY7dPck/zt49+xO00BQuVIC9e/d7GsPuPfvYvXsfKyOcVptx46by4gveF8ah8F4kEOKFbHJZzTjwsgOH3YL4JpwaMTi15GoiUhxARDKLyI1JPD8rsM9dTcS3uXgWTtMy7vPLuHeXAm3dtIZATp/njAfuBCrgzJsap6I7qXkaoB2wxE0/6+brqbx5cwPOicUrLz/NwIHDA57noIH92bp1B58MOLegS8OGtXm+Rxda39WRU6f+vciz/WfL5m2UvKEyZW6rQ5nb6rB3z35q12jFwYN/Mn3qXCpVuYOwsDAyZszAHeVLsy3qF7/m/9pr/ShWvCIlS1blwQ5dWbBgKQ8//DT5818Tv0+L5o3YtCnKr/leyKu9+lLkhvIUv7Ey7R94kvnzl3peEANErIqkePGiFClyLWnTpqVt25ZMnjLL0xgOHDjE7t17ufHGYgDUrVudLVu2eRoDhMZ74UtVk30LZVYzDrwZwBMisgWIwimEUdVDbk13pIikd/ftBST+63oNZ4HrQ+7/Wd307sDnIrIe53NcBDwB9HaP+SDwM7AfOObmeUZE5gNH4iY0d0UA/wOKA/NxCm1warvrRWRNoK4bDx/+ObVqViFPnlzs2rmKPn0+JEuWzDzRpSMAEyZMY8jQ0YHIOl61qhV44IE2bNiwmVURzo9Kr9f68vFHfUifPj0zpo8CnE5cXbv19Gveg777mGo1KpI7d042bl1M33cH8P2wH5Pcd1vUL8ybs5gly6cQGxvL8KFj2bLFm960Q4Z8St48uRER1q3fRLduL3uSb6iIiYnh6Wd6MW3qD4SlScOQoaPZvNn7gvDpZ19j2NDPSJcuLbt2/U6nR5PqZhJYofJexEslNWMJ9bMFkzJuwR6jqtEiUgX4UlXLuNvSAGuAe1R1u5tWG+ihqs38kb8/mqlTi6zpMwU7BABOnPWmVn8xMUHsfGVCV/SZPRe9rpYc/3RqkOzfnGzfzr7s/ALFasapz3XAGLfgPQM8BiAit+Asij0+riA2xpgrnUanjhM9K4xTGbegLZtE+mbghiTSFwALAh6YMcYEQuooi60wNsYYc+UK9ck8kssKY2OMMVcuK4yNMcaYILNmamOMMSa4rJnaGGOMCTKNtsLYGGOMCS5rpjbGGGOCS60wNuZ812TOEewQADh59nSwQyBdWGj8eT2Zu0awQ6Dv3oXBDiFkZAj3bvnJi/k3+kywQ/APK4yNMcaY4EotNWNbtckYY8wVS6OTf7sYEblWROaLyGYR2SQiT7vpuURktohsd//P6aaLiHwqIjtEZL2IlPM51kPu/ttF5KHkvA4rjI0xxlyxNDb5t0uIBp5X1Vtwlrrt6s7p3xOYq6olgLnuY4DGQAn31hn4EpzCG3gDqARUBN6IK8AvxgpjY4wxVyx/Fcaquk9V17j3jwFbgEJAS2Cou9tQoJV7vyUwTB3LgRwiUgBoBMxW1b9V9TAwG2cd+YuywtgYY8yVSyXZNxHpLCKrfG6dkzqkiBTBWXBnBZBPVfe5m/YD+dz7hYA/fJ622027UPpFWQcuY4wxV6yUdOBS1YHAwIvtIyJZgJ+AZ1T1H5FzSyCrqopIQGYZsZqxMcaYK5bGSrJvlyIiaXEK4hGqOs5NPuA2P+P+f9BN3wNc6/P0wm7ahdIvKsWFsYiUEpG+IjJRROb4pBcRkbbJuVBtjDHG+ENsjCT7djHiVIG/Bbao6kc+myYBcT2iHwIm+qR3cHtVVwaOus3ZM4GGIpLTLQ8bumkXlaLCWET6AGuAF4HmQJ1ExxoJPJCSY6ZGIpJeROaISKSItPMgv44iUjDQ+fhDwUL5GTtpMPN/nsS8ZRPp9LjzdenV53kWrpjM7CXj+Gb4ALJlywpAmXK3M2vRT8xa9BOzF4/jzqb1/BLHZ1+8x7ZdK1i2ctp527o+1YnDx3eQK/e588q+H7zG6nVzWbJ8CqVK3+qXGAoWys9Pk4ewaPlkFv48mUefeBCAHDmyM3r8tyxbPYPR478le/ZsADRqUpd5SycwZ/E4Zs4fS8XK5S52+Itq/X5neq76kqdm9otPy5g9Mx2Hv8wz8z+i4/CXyZAtc4LnFCp1A713DOfWxhXj0xr2vJenZvbjqZn9uK1Z5f8cT2KDBvZn7+51RK6dG5/W771ebNywkDWrZ/Pj2G/i3xevPN39MdZFziNy7Vy+H/456dOnD1heX3zVj12/RrAyYkZ8Ws6c2Zk0eTiR6+cxafJwcuRI+PrL3VGKI/9sp1WrxgGLK06jhrXZtHERWzcv4cUXugY8v4vxY2/qasCDQF33tztSRJoAfYEGIrIdqO8+BpgG7AR2AIOAJwFU9W/gLSDCvfVx0y4q2YWxiNwL9MLpGVYGeM93u6ruBFYBLZJ7zFSsLICqllHV0R7k1xG4Igrj6Ohoevd6nzpVWtC84X10fPQ+SpQsxqL5P1O3aisaVL+Lnb/8RrfnHgNg65btNK7TloY176Z9m870+/gNwsLCLjuOkSPG0abVI+elFypUgDr1qvPH7+dalRo0rEWxYkW4o3Q9nnmqF/0/6X3Z+QNER8fwZq/3qVm5OU0atOPhR+/nxpLFeOrZx1i88Geq3nEnixf+zFPPOu/F4oXLqVutFfVr3MUz3V6l/6dv/ee81/64iKEP9UuQVrNLC3Yu28gndZ5j57KN1Hyyefw2SSM06nkfOxZviE+7sU4ZCt5alM+bvMxXrV6n+mNNSZ8l43+OydewYWNo2qx9grQ5cxdRukxdyt3RgO3bd9LzpW5+ySs5ChbMT7euj1CpchPKlK1HWFgY7dq2DFh+I4b/RKtWHROkPfd8FxYsWEqZUnVZsGApzz3fJX5bmjRpeOutl5g7d3HAYvLN69MB79Cs+QPcXroO7dq14uabSwQ83wvxVzO1qi5RVVHVUu5vdxlVnaaqf6lqPVUtoar14wpWtxd1V1Utpqq3q+oqn2N9p6rF3dvg5LyOlNSMu+OcAbRU1fVAUnOpbcEZc3XFE5EJIrLaHfzd2Sf9uIi8IyLrRGS5iORL9LxrgO+BCu6ZVTER+VVE8rjby4vIAvf+myLynYgsEJGdItLd5zgPiMhK9xhfi0iYexsiIhtFZIOIPCsibYDywAh334wXya+WzxnfWhHJmij2zCIy1X1tG+Nq9e5lic3uwPYPL+d9PXjgTzau3wLAieMn2b5tJ/kLXMOi+cuIiYkBYE3EOgoUdN7Wf0/9G5+ePn16VP3Td2LZ0ggOHz5yXvo7/V7lzV79EuTTpFl9Ro0cD8CqiEiyZ89Gvnx5LzuGgwcOsWHdZiDuvfiF/AXy0ahJXcaMdFrCxoycGN8acPLEyfjnZsqU6bLei19XbuXU0eMJ0m5qcAdrfnR+zNf8uJibG5SP31a5YyM2TV/Jib+OxqddU6Iwv67cSmxMLGdPnebA1t8pUavUf47J1+IlK/g70ecze86i+O/C8hVrKFSogF/ySq7w8HAyZsxAWFgYmTJmZN++/QHLa+nSlRz++0iCtKbNGjBixE8AjBjxE82aN4zf9kSXh5g4cQaHDv4VsJjiVKxQll9++ZVdu37n7NmzjBkzkRbNGwU83wtRTf4tlKWkML4dmKmqF5vQdC/nun1f6R5R1TtwCrruIpLbTc8MLFfV0sAi4DHfJ6nqQeBRYLF7ZvXLJfK5CWdcWtzg8LQicjPQDqimqmWAGKA9TotEIVW9TVVvBwar6o84LRLt3fxOXSSvHkBX95g1gMT73gnsVdXSqnobMMN93a2BW1W1FPD2JV5PshW+tiC3lbqZtavXJ0i/94G7mD/n3Bl+2TtuZ96yicxdOoGez/WJ/0H2t8ZN67Nv7342btyaIL1AgXzs2b0v/vHevfvjTxb85drrCnLb7TezZvU68l6Tm4MHDgFOgZ33mtzx+zVuVp/FK6fy/ZgvebZbL7/GkCVvdo4fOgLA8UNHyJI3OwBZ8+XklkYVWPn9nAT779/yGyVqlSJthnRkypmVolVuJXuB3IkPGxAPd7yXGTPne5IXOJ/5Rx9/xa5fVrL797Uc/ecfZs9Z5Fn+ANdck4cD+53vxYH9h7jmmjwAFCiYjxYtGjFo4PeexFGwUH7+2L03/vHuPfsoWDC/J3knxZ8duIIpJYWxcOkpufMB//73cEJKdxFZByzH6RkXV+M/A0xx768GilxmPlNV9bSq/onTSy8fUA+4A4gQkUj38Q041yduEJHPRORO4J8U5rUU+MitgedQPW+CuA0410b6iUgNVT0KHMX5TL8VkbuAk4mek2Ds3onTh5MVSKbMmRg07BPeeLkvx4+diE/v/nxnoqOjGTdmSnza2tUbqFu1JU3qtaPbs4+RPr3/J9rPmDEDz/V4gvfe/sTvx76UTJkz8c2wT3n9lYTvRRzfGvD0KXOoUbEpD7d/ipde7X7evn7lZtv09Q7M7DvyvJr4jsUb2DY/ks7j3qTtp934Y812YmMDP1Hwyz27Ex0dzQ8/jLv0zn6SI0d2WjRvRPEbK3Pt9eXInDkT999/l2f5JyXu83j//dd5rVdfv7UaXWn81YEr2FIyzng7UPVCG0UkDVAd2HS5QQWbiNTGuVBfRVVPus28GdzNZ/Xctz6G5L2H0Zw78cmQaJvv8kJxxxNgqKq+nERspXFq0k8AbYHzL3xeID9V7SsiU4EmwFIRaaSqW322b3PnV20CvC0ic1W1j4hUxDkhaAN0A+r6ZuY7dq9Qzlsv+YsQHh7OoKGfMH7sVKZPOVfbantfK+o3rEXbVp2SfN6ObTs5eeIkJW8uwfpI/37Nit5wHdcXuZbFPzsnAQUL5WfhkonUq3UX+/YdoFDhc02iBQvmZ9/eA37JNzw8nG+HDWDc2MlMmzwbgEMH/+KafHk5eOAQ1+TLy5+Hzu/7sXzZKq4vUphcuXLwd6LmzP/q+KGjZMmbw60V5+D4n06TdKFSRWn32VMAZMqZlRtrlyE2JpYts1ax8POJLPzcaVK/Z0BX/toZuKZbgA4PtqVpk/o0aNQ2oPkkVq9eDXb9+jt//ul8FuMnTKdK5fKenhAcPPgn+fLn5cD+Q+TLn5dDh5wm6bLlbmfIsM8AyJ07J40a1SY6Jpop7vfJ3/bu2c+1hc91USlcqAB79wb2c7+YUK/xJldKasZjgHIi8vwFtr8CFAd+uOyogi87cNgtiG/Cmaf0cvyKU9MFuDsZ+88F2rjXn+MmKr/evQ6cRlV/wulMF9ed9hjge/03yfxEpJiqblDVfji9/G7yzdTtkX1SVb8HPsD5vLMA2VV1GvAsUDp5L/nC+n/Whx3bdjLwi6HxabXrVadL90foeH83/j11rnHl2usKxXfYKnRtAYqVKJqgc5W/bN60jRuLVqL0rbUpfWtt9u7ZT63qLTl48E+mT53Lvfe1BqB8hTL8888xDrjNyJfr4/+9zfZtO/n683Pvxazp82h7n9M5qO19LZk5bR4ARYpeF7/P7aVvIV26dH4riAG2zllDuTbOcovl2tRg6+zVAPSv8Qz9qz9N/+pPs2n6Cia/Npgts1YhaYSMObIAkO+ma8l/03XsWLz+gse/XI0a1qZHjy60uqsjp0552wD3x+97qFSpHBkzOue2detUZ+vW7Z7GMG3qHNq3d/6c27e/m6lTnML2tltqcuvNNbj15hpMGD+dZ595PWAFMUDEqkiKFy9KkSLXkjZtWtq2bcnkKbMClt+lqEqyb6EsJTXjT4B7gPdFpC1uI5bboacGzrXV5VxidpMrxAzgCRHZAkThvK7L0RunmfctYMGldlbVzSLSC5jltjicBbriXOMd7KYBxNWchwBficgpoMpF8ntGROrgXG7YBExPlPXtwAciEuvm2QWnkJ8oIhlwauzPpeB1n6dC5XK0ubclmzdFMWuR0xml71uf0KfvK6RPn5ZR478BYM2qdfR8rg8Vq5Sj69OPEh0dTWxsLK/0eOu8ji3/xTeDP6ZajUrkzp2TjVFL6PvOAL4fNjbJfWfNXECDRrVZs34ep06dousTL112/gAVK5fjHve9mLPYqWG91+cTPvv4GwYO+Yj7H2zD7j/20rnjswA0a9GQe+5tydnos/x76jSPP/LfP4q2n3ajaOWbyZQzKy/8/BnzPv6JRV9O4t7Pu1OubR2O7vmTUV0HXPQYYWnDeWzs6wCcPn6Ksc9+QWyMf5qpvx/+ObVqViFPnlz8unMVvft8yEsvdiN9+vTMmD4KgBUr1tC1W89LHMk/VkasZdy4qUSsnEl0dDSRkZsY9M2IgOU3eMgAatSsTO7cOYnavox33v6Ej/p/ybDh/6PDQ2354/c9dHjQu97kvmJiYnj6mV5Mm/oDYWnSMGToaDZv3haUWCD1LKEoKbnOICLZgQE4nYl8x5fEAiOAbu4E2+YqlZxmai+cPHv60jsFWPrwtMEOAYBHc5QNdgj03bsw2CGEjAzh/u/z8F/8G32xvrjeiD6z57Krq9tuvjPZvzk3bpkRstXjFM1N7Xbo6SgizwEVgNw4HXxWqqp/2u2MMcaYZAr15ufk+k8LRbiDni85vZcxxhgTSKHeSzq5bNUmY4wxV6zU0ps62YWxiHyXzF1VVZMem2KMMcb4UexV2Ezd8RLbFae3rQJWGBtjjAm4q/GacdELpOfA6cz1GrAM8GasgTHGmKteapl4LNmFsar+doFNvwHrRGQmsB6Yg7MmpDHGGBNQqaWZOkXrGV+Mqv4BTAae9tcxjTHGmIuJjZVk30KZv3tTHyCVLKFojDEm9KWWmrHfCmMRCcNZQODopfY1qVcozHwFcCoEZhc6duZiq1l65/1T3i71l5RQmXXqTMzZYIcQEjNfpSZXXQcuEal5kWNcCzyMs97uN5cfljHGGHNpV2PNeAHxK5wmSYBFwAuXE5AxxhiTXKmkM3WKCuM+JP26Y4HDOPNTr/RLVMYYY0wyxMT6rR9yUKVkaNObAYzDGGOMSbFUsoJi8oc2ich3IvJsIIMxxhhjUkKRZN9CWUrq9/cD1wQqEGOMMSalYjX5t1CWkmvGv2KFsTHGmBASG+I13uRKSc34B6CxiOQMVDDGGGNMSvizmdq9HHtQRDb6pL0pIntEJNK9NfHZ9rKI7BCRKBFp5JN+p5u2Q0SStV5DSgrj94BVwHwRaSYi+VLwXGOMMcbvYpBk35JhCHBnEukfq2oZ9zYNQERuAe4FbnWf84WIhLkTYH0ONAZuAe5z972oixbGItJBREq5D/8FmgKlgInAXhGJSeIWnZxXbK5en33xHtt2rWDZymnxaa+89gxLlk9h0bJJ/DRxCPnzO1dEqtWoxG971rJo2SQWLZvECz27BSSmqKilrFo1ixUrprN06RQA3njjeSIiZrJixXSmTPmeAgW8Pf9MkyYNEStnMnH8UM/yTJ8+PUuXTGFVxCwi187l9deeB2DokM/YuGEha9fMYeDXHxIe7u+ZdOGLr/qx69cIVkbMiE/LmTM7kyYPJ3L9PCZNHk6OHNnit33w4Rus2zCf5SumU7rMrX6P50LvxddffciqiFmsXjWbUSO/JnPmTH7P+0J2bFvO2jVzWBUxi+U/T7v0EwJg0MD+7N29jsi1c4OSf2KxKbhdiqouAv5OZtYtgVGqelpVdwE7gIrubYeq7lTVM8Aod9+LulTNeIjPQRbjTOqx0P3/QrfFyXwh5io1csQ42rR6JEHaZ598Q/XKzahZtQUzZ8zjxZfPFbo/L4ugZtUW1Kzagg/6/i9gcTVq1I5KlRpTrVozAD766GsqVGhEpUqNmTZtLq+84u0aKN2fepStW7d7mufp06dp2Kgt5Ss0pHyFRjRsWJuKFcsxctR4bru9FmXL1Sdjxgw88sh9fs97xPCfaNWqY4K0557vwoIFSylTqi4LFizluee7ANCwUW2KFS9C6dvr8FS3l/lkwNt+j+dC70WPF96kfIWG3FG+Ab//sYcnuzzs97wvpn6DeyhfoSGVqzS59M4BMGzYGJo2ax+UvJOSksJYRDqLyCqfW+dkZtNNRNa7zdhxl2oLAX/47LPbTbtQ+kUlp5laAFS1tqrWSc4tWS8txInIBBFZLSKbfD8wETkuIu+IyDoRWZ5Uc72I1PK5vrBWRLKK43/udYQ5IjJNRNq4+/8qInnc++VFZIF7v6KI/OweY5mIlHTTO7rxzXaf201EnnP3Wy4iudz9yriP14vI+LgvkYh0F5HNbvooN+1NEenh8xo2ikgREcksIlPd17tRRNpd7nu7bGkEhw8fSZB27Njx+PuZM2VCQ2CR0gQxZfY2pkKFCtCkcT2++26kZ3nGOXHiJABp04aTNm04qsqMGfPit0esiqRwoQJ+z3fp0pUc/vtIgrSmzRowYsRPAIwY8RPNmjcEoFmzBowcMc6JJyKS7NmzkS9/Xr/HlNR74fu9yJgxQ0h8V720eMkK/k709xtMKblmrKoDVbW8z21gMrL4EiiGM93zPqB/IF5H6pi6JDAeUdU7gPJAdxHJ7aZnBparammcloDHknhuD6CrqpYBagCngNZASZxrCB2AqsmIYStQQ1XLAq8D7/psuw24C6gAvAOcdPf72T0+wDDgJVUtBWwA3nDTewJl3fQnLhHDncBeVS2tqrcBMy6x/3/W643n2Lh1Mfe0a8G7bw+IT69QsSyLf57M2HHfctPNgVkUTFWZMuV7li2bSqdO98en9+79Ajt2LOfee1vRp09A/gaT9FH/3vR8+W1iY72f0iCueXzP7nXMnbuYiIi18dvCw8Npf//dzJy1wJNYrrkmDwf2HwLgwP5DXHNNHgAKFMzH7t374vfbu2cfBQvm93v+F3ovBg3szx+/r6XkjcX5/Ivv/J7vhagq06eNZMXy6TzaKXRqp8EUK8m//ReqekBVY1Q1FhiE0wwNsAdnXYY4hd20C6VflBXGF9ZdRNYBy3He2LhS4Awwxb2/GiiSxHOXAh+JSHcgh6pGAzWBke6HuheYl8TzEssOjHV79n2M01EgznxVPaaqh3BWyprspm8AiohIdjfvhW76UDcGgPXACBF5ALjUNf4NQAMR6SciNVT1vFW5fJt+Tp/9JxkvK2lv9/6I226qwdjRk3js8QedQCM3UeqWWtSo0pyBXw3j+5Ff/ufjX0zdundTpUpTWrbswOOPd6B6defv7Y03PqB48cqMGjWBLl06BiTvxJo2qc/Bg3+yZu0GT/JLLDY2lgoVG1H0hgqUL1+GW28pGb/ts0/fZfGSFSxdGpyZb72uhV7ovXis8/NcX+QOtkZt5557WngWT606ralY6U6aNX+ALl06UqN6Jc/yDlWxSLJv/4WI+DYDtQbielpPAu4VkfQiUhSnjFgJRAAlRKSoiKTD6eQ16VL5JKcwziEi16XklqJXGoJEpDZQH6ji1oDXAhnczWf13C9CDEmM1VbVvsCjQEZgqYjcdIksozn3WWTwSX8Lp9C9DWieaJvvWoWxPo9jk4opkaY4vf3KAREiEp4ohvg4VHWbu98G4G0ReT3xwXybftKnzZZ4c4qNHT2RFi2dUQLHjh2PbyqcPWshadOGkyu3/0fX7d17AIBDh/5i0qSZlC9fJsH2UaPG06pVY7/nm5SqVcvTvFlDdmxbzojvv6BOnWoMHfKpJ3n7Onr0HxYuXEbDRrUB6PXqs+TNm4sXXujtWQwHD/4Z3/ycL39eDh36C4B9ew9QuPC538iChQqwd+/+gMWR+L0Ap6AeM2YSrVt7d+027jUeOvQXEydOp0KFMp7lHapiUnC7FBEZidO6WFJEdotIJ+B9EdkgIuuBOsCzAKq6CRgDbMZpMezqVraigW7ATGALMMbd96KSUxg/DexKwW1nMo4Z6rIDh1X1pFuQVk7Jk0WkmKpuUNV+OGdJN+E0abdzu74XwPlQ4/wK3OHevztRHHHNGx1TEoNbgz0sIjXcpAeBhSKSBrhWVecDL7l5ZHFjKOfGXw4o6t4viNME/j3wQdw+/nZDsevj7zduVp9t25yvUVyzJEC5O0qRJk0a/v7rsF/zzpQpI1myZI6/X69eDTZtiqJYsSLx+zRr1pCoqF/8mu+FvNqrL0VuKE/xGyvT/oEnmT9/KQ917O5J3nny5CJ7dueEKkOGDNSrV4OoqB08/PB9NGhQiwce7OZp7XTa1Dm0b+/8SbRvfzdTp8wGYOrUOdzX/i4AKlQowz//HItvzvaXpN6Lbdt+SfS9aEBU1A6/5nshib+nDerXYtOmKE/yDmWxIsm+XYqq3qeqBVQ1raoWVtVvVfVBVb1dVUupagtV3eez/zuqWkxVS6rqdJ/0aap6o7vtneS8juSMT/gHOJKcg6UiM4AnRGQLEIXTVJ0Sz4hIHZxa6iZgOk7zdl2cs6jfcc6+4vQGvhWRt3CWqozzPjBURHoBU//D63gI+EpEMuGcJD0MhAHfu83YAnyqqkdE5Cegg4hsAlYA29xj3A58ICKxwFmgy3+II4FvBn9MtRqVyJ07JxujltD3nQE0aFSLEiVuIDY2lj9+38tzT78GQMvWjXn40fuJiY7m1KnTdOro/x7N+fLlZfRopx9HeHg4o0dPYPbshYwc+RU33liM2NhYfv99D0899bLf8w41BfLn49tvPyYsLIw0aYQff5zCtGlzOXniV377fTeLF00EYMKE6bzz7id+zXvwkAHUqFmZ3LlzErV9Ge+8/Qkf9f+SYcP/R4eH2vLH73vo8KDTy37mjPk0alSH9RsXcOrkKZ544kW/xgIXfi/mzxtHtmxZEYH167fQzaPvRb58eflx7LcAhIeHMWrUBM+u3fv6fvjn1KpZhTx5cvHrzlX07vMhg4eM8jyOOKml+5xc7CzX/QF+U1X7eBfS1UFEhgBTVPXHYMfiTzmzFA+Jv41T0WeCHQLRsclpGAu8NMmoEQRaurC0wQ4BgDMxZ4MdArFXWe/ri4k+s+eyv5yjC7RP9hvabt+I4P8xXID/R+4bY4wxHvmvvaRDjRXGQaKqHYMdgzHGXOmSOc1lyLPC2BhjzBXLasbGGGNMkHk/LU5gXLQwVlWbFMQYY0zISi3d4axmbIwx5oplzdTGGGNMkF0VzdTGGGNMKIuxmrEx5zsZffrSO3kgJgirHYWqUFjiL1QmQMkQni7YIfBvCExIA6ln8pHU8pduhbExxpgrlhXGxhhjTJCljvq9FcbGGGOuYNab2hhjjAkya6Y2xhhjgiw0ugZePiuMjTHGXLGsmdoYY4wJMmumNsYYY4LMelMbY4wxQRabSopjW5XpKiEiZUSkyWUeY5qI5PBTSPHSpEnDiuXTGT9ucIL0j/r35q8/t/o7u/MMGtifvbvXEbl2bnza3Xc3Y13kPM78+wd3lCsV8BiSE5NX+e7ZvY61PvmWLn0rSxZPZlXELJb/PI0K5csEPI7s2bPxww9fsW7dPCIj51KpUjneffcV1q2bR0TETEaPHkj27Nn8nu/nX/bjl19XsjxienxazpzZmTB5GGvXzWPC5GHkyHEu3+o1KrHk5ymsiJjBtBkj/R5P4cIFmDVzDOsi5xG5di7dunUCoHSpW1i8aBIRK2fy87KplPfgM4mTPn16fl46hdWrZrMuch5vvP68Z3knJSYFt1BmhfHVowxwWYWxqjZR1SN+icbHU906sTVqR4K0cuVKkSNndn9nlaRhw8bQtFn7BGmbNm3lnraPsXjxck9iSE5MXhg6bAzNEuX73ruv8tbbH1G+QkPe7P0h7733asDj6N//TWbPXkDp0nWpUOFOtm7dwbx5iylXrgEVKjRi+/ZdvPBCV7/nO+L7H7mr1cMJ0p59/gkWLlhG2dJ1WbhgGc8+3wWA7Nmz8tHHfbj3ns5UqnAnHR7s5vd4oqNjePGlPpQuU5fqNVrQ5YmHuPmmErz73qu8/c7HVKjYiN59+vPeu4H/TOKcPn2a+g3bckf5BtxRviGNGtamUsVynuWfWGwKbqHMCmM/E5EJIrJaRDaJSGef9OMi8o6IrBOR5SKSL4nnZhGRwSKyQUTWi8jdbvp9btpGEenne0yf+21EZIh7/x5333UiskhE0gF9gHYiEiki7USkooj8LCJrRWSZiJR0n9tRRMaJyAwR2S4i7/vk8auI5PHn+1WoUH4aN67L4MHnahVp0qThvfde5ZVX3vVnVhe0eMkK/j58JEHa1q072LbtF0/yT0pSMXlhSRL5qirZsmUFnAJo774DAY0hW7asVK9ekcGDRwFw9uxZjh79hzlzFhMT49RvVq5cQ+HC+f2e97KlERz++0iCtKZNG/DDiJ8A+GHETzRr1gCAe9q2ZPKkmezevReAPw/95fd49u8/SGTkRgCOHz/B1q3bKVgov/OZZM0CQPZsWdkX4M8ksRMnTgKQNm044WnTBnX+81hJ/i2U2TVj/3tEVf8WkYxAhIj8pKp/AZmB5ar6qlvAPQa8nei5rwFHVfV2ABHJKSIFgX7AHcBhYJaItFLVCReJ4XWgkaruEZEcqnpGRF4HyqtqN/fY2YAaqhotIvWBd4G73eeXAcoCp4EoEflMVf+4zPclSR9+8CYvv/IuWbNmjk97sktHpk6Zzf79BwORpUmh53u8wdQpP9Cv72ukSSPUrNUyoPkVKXIthw79zaBB/bn99ptZu3YDzz//JidPnorf56GH2vHjj5MDGkecvNfk4cD+QwAc2H+IvNc456PFSxQlbXg4U6f/QJasmfnqiyGM/GF8wOK4/vrClC59GytXrqVHjzeZMnkEffu+Rpo0aahVO7CfSWJp0qRh5YoZFC9WhC+/GsLKiLWe5u/Ln9eMReQ7oBlwUFVvc9NyAaOBIsCvQFtVPSwiAgzAaXE8CXRU1TXucx4CermHfVtVh14qb6sZ+193EVkHLAeuBUq46WeAKe791TgfbGL1gc/jHqjqYaACsEBVD6lqNDACqHmJGJYCQ0TkMSDsAvtkB8aKyEbgY+BWn21zVfWoqv4LbAauv1hmItJZRFaJyKqYmOMX2zWBJo3rcejQX6xduyE+rUCBfNx1d1M+/2LwRZ5pvPR45w70eOFNbihWgR4v9Gbg1/0Dml94eDhly97GwIHDqVy5CSdOnOKFF56M3/7SS92Ijo5m5MjAFXwXE1cLDA8Lo0zZ27jn7k60btmRF196iuLFiwYkz8yZMzF61EB69HiTY8eO07lzB154oTfFilfkhRfe5OuvPwxIvhcSGxtL+QoNub5oeSqUL8utt5b0NH9fmoJbMgwB7kyU1hPnN7EEMNd9DNAY5/e9BNAZ+BLiC+83gEpAReANEcl5qYytMPYjEamNU6BWUdXSwFogg7v5rJ5ry4nBP60Svt+vDPGJqk/gnJVdC6wWkdxJPPctYL579tfc9/k4NeI4l4xVVQeqanlVLR8WliXZwVepWp6mTRsQFbWM4cM+p3btaqxdM4diNxRh8+bFREUtI1OmjGzetDjZxzT+9+CD9zB+/DQAfvxxMhUqlAlofnv27GPPnn1EREQCMH78NMqUuc2NpQ2NG9ejY8fuAY3B16GDf5Ivf14A8uXPG98cvWfvfubOWczJk6f4+6/DLF26kttuv8nv+YeHhzN69EBGjhrPhIlOx7IHH2jD+AnuZ/LTFE861SXl6NF/WLBwKY0a1g5K/uDfa8aqugj4O1FySyCuZjsUaOWTPkwdy4EcIlIAaATMVtW/3QrVbM4v4M9jhbF/ZQcOq+pJEbkJqJzC588G4nuluGdTK4FaIpJHRMKA+4CF7i4HRORmEUkDtPZ5XjFVXaGqrwOHcArlY0DWRLHuce93TGGcfvHaa/0oVrwiJUtW5cEOXVmwYCn5C9zO9UXuoGTJqpQsWZWTJ09xy601ghGece3dd4CaNasAUKdOdXbs2BXQ/A4cOMTu3fsoUeIGN89qbNmynQYNavHcc11o06YTp079G9AYfE2bNof72ztXcO5vfzdTp84GYOqU2VSuWp6wsDAyZsxA+QqliYryfz+DgV9/yNatOxgwYFB82r4En0m1gH8mvvLkyRXfkz1DhgzUr1czIK87uWLQZN/+o3yqus+9vx+I6+9TCPC9fLfbTbtQ+kXZNWP/mgE8ISJbgCicpuqUeBv43G06jgF6q+o4EekJzAcEmKqqE939e+I0fR8CVgFx1dIPRKSEu/9cYB3wO9BTRCKB94D3gaEi0guY+l9ebGrx/fDPqVWzCnny5OLXnavo3edD/j58hAEfv03evLmYNHEY69ZtoomHvZuTimnwkFEBz3e4T767dq6iT58P6fLEC3z0UR/Cw8P5999/6dLlxYDH8eyzrzNkyKekS5eWXbt+p3PnHixdOpn06dMxdeoIAFauXMtTT73i13y/GzKA6jUqkTt3TrZsW8q7bw/g4/5fMWT4/+jQoS2//7GHjm6v6W1RvzBn9kJ+XjGNWI1l2JAxbNm8za/xVK1agQceaMOGDVuIWDkTgNde78cTXV7ko/693c/kNF2efMmv+V5MgQL5+O7bTwgLS0OaNGn48cfJTJ02x7P8E0tJL2m3U21nn6SBqjowuc9XVRWRgPRWk2D2gjOpT/oM14bEFyomNtQHMngnFDqRhqW5UNcFb6ULC37949/oM8EOAYDYEPjtjz6z57K/ns8VuTfZL+SjX0ddMj8RKQJM8enAFQXUVtV9bjP0AlUtKSJfu/dH+u4Xd1PVx930BPtdiDVTG2OMuWL5uQNXUiYBD7n3HwIm+qR3EEdlnJEw+4CZQEN3NExOoKGbdlHBP000xhhj/iN/toGJyEicmm0eEdmN0yu6LzBGRDoBvwFt3d2n4Qxr2oEztOlhAHdo61tAhLtfH1VN3CnsPFYYG2OMuWJdRses86jqfRfYVC+JfRWfDreJtn0HfJeSvK0wNsYYc8VKLQtFWGFsjDHmipU6imIrjI0xxlzBrGZsjDHGBFlqGcRohbExxpgrllrN2JjzSUhMMQFpJPhxhMKkChAa19RC5Qfz5NnTl94pwDKny3DpnTyQWibG8Wdv6mCywtgYY8wVK3WcUlhhbIwx5goWKi1Ql8sKY2OMMVes1FEUW2FsjDHmCmZDm4wxxpggC5XOgZfLCmNjjDFXrGgrjI0xxpjgspqxMcYYE2Q2tMkYY4wJMk0lQ5vSBDsAk3wi0kJEerr384rIChFZKyI1/JxPeRH51J/HvJCoqKWsWjWLFSums3TpFACGD/+cFSums2LFdKKilrJixfSAxlC4cAFmzRzDush5RK6dS7dunQAY8f0XRKycScTKmWyL+pmIlTMDGkdiadKkIWLlTCaOH+ppvr52bFvO2jVzWBUxi+U/T/Ms36ioZaxeNZuVK2awbOlUAO66qylr18zh1MnfKFeuVMBjGDSwP3t3ryNy7dz4tN5vvsCa1bNZFTGL6VN/oECBfH7P939f9GXHrpX8vPLc975V68Ysj5jO4X+2U7bs7fHpadOm5fMv+7FsxTSW/DyF6jUq+T2eOE8+2ZGVETOIWDWTJ7s+DMBtt9/M3Pk/sWLldMb8+A1Zs2YJWP4XEosm+xbKrGZ8BVHVScAk92E9YIOqPprc54tImKrGJCOfVcCq/xZlyjVq1I6//joc//jBB8+t1923by/++edYQPOPjo7hxZf6EBm5kSxZMrNi+XTmzllE+weejN+nX7/X+OdoYONIrPtTj7J163ayZc3qab6J1W9wT4LPxysNG7VNkO/mTVG0a9eZ/33e15P8hw0bwxdfDGbw4AHxaR/2/5I33vwAgG5dH6HXq8/StVtPv+b7w4ifGPT1cL4a9GF82ubN23jg/if55NO3E+z70MPtAKhaqQl58ubmp3HfUbtmK7/XFm+55UY6PnwvtWq24syZs0yYOIQZ0+fx+Rfv8erL77FkyQoe7HAPzzzbmbf6fOTXvC8ltUyHaTXjFBCRIiKyVUSGiMg2ERkhIvVFZKmIbBeRiu5+FUXkZ7fWukxESiZxrNoiMsXn8f9EpKN7/1cR6S0ia0Rkg4jc5KZ3dPcrA7wPtBSRSBHJKCL3uftuFJF+Psc9LiL9RWQdUMV9/IGIbBKROW6sC0Rkp4i0SBybiNRy84h0X4+nJUObNs0YPXpiQPPYv/8gkZEbATh+/ARbt26nYKH8CeO4uzmjxwQ2Dl+FChWgSeN6fPfdSM/yDHVbo3awbftOz/JbvGQFfx8+kiDt2LHj8fczZ84UkCbSZUsjOJwo321Rv7Bj+67z9r3ppuIsWvgzAH8e+oujR/+hbLnbz9vvcpUsWZyIVZGcOvUvMTExLFmykhYt76R48aIsWbICgHlzl9Cy5Z1+z/tSUkvN2ArjlCsO9Aducm/3A9WBHsAr7j5bgRqqWhZ4HXj3P+Tzp6qWA750jx1PVSPd445W1TJATqAfUBcoA1QQkVbu7pmBFapaWlWXuI/nqeqtwDHgbaAB0Brok0QcPYCubj41gFP/4bVckKoyZcr3LFs2lU6d7k+wrXr1ihw48Ce//PKrP7O8qOuvL0zp0rexcuVanzgqcfDgIXbsOP/HMFA+6t+bni+/TWyQJ/NXVaZPG8mK5dN5tFN7LzNm6pQR/JzE9yLY3urzErt+ieC++1rzZu8PghrLxg1badK0HmFhYc53t8xtFC5cwO/5bN4cRdWqFcmVKwcZM2agYaPaFC5cgC1bttOseQMAWt/VhEIByPtSVDXZt1BmhXHK7VLVDaoaC2wC5qrzKW8Airj7ZAfGishG4GPg1v+Qzzj3/9U+x72QCsACVT2kqtHACKCmuy0G+Mln3zPADPf+BmChqp5NFL+vpcBHItIdyOEePwER6Swiq0RkVUzM8fOPcBF1695NlSpNadmyA48/3oHq1SvGb2vbtiVjPKyNZs6cidGjBtKjx5sJakDt2rX0tFbctEl9Dh78kzVrN3iW54XUqtOaipXupFnzB+jSpSM1qgfumqSvOnXvpnKVJrRo2YEnHn+I6h7lmxyvvd6PosUqMHLkeLo++XBQYxk+bCx79uxnweIJvNevFytXrCEmxv8ncFFRv/DxR18xcfIwJkwcyob1m4mJieHJJ17kscceZPHSSWTNmpkzZ876Pe9LiU3BLZRZYZxyvmuwxfo8juXcNfi3gPmqehvQHEhqzbRoEr7/ifeJO24Ml3dt/99E14nP6rlTxPj43ZOL8/JR1b7Ao0BGYGlck3mifQaqanlVLR8WlrIOHHv3HgDg0KG/mDRpJuXLlwEgLCyMli3v5McfJ6foeP9VeHg4o0cPZOSo8UyYeK7jTFhYGK1aNmbsWG/iAKhatTzNmzVkx7bljPj+C+rUqcbQIZ70pzvP3r37AefzmThxOhUqlPE+30kzqFDem3xT4oeR42jduklQY4iJieGVnu9Qo2pz7r/3CbJnzxawFpxhQ8dQo1oLGjVsx+EjR9mxYxfbtu2kZYsO1KjWgrFjJrNr1+8ByftiNAX/QpkVxoGRHdjj3u94gX1+A24RkfQikgOnQ9Z/tRKoJSJ5RCQMuA9YeBnHiycixdyWgH5ABE7TvF9kypSRLFkyx9+vV68GmzZFAVC3bnW2bfuFPXv2+yu7ixr49Yds3bqDAQMGJUivV68GUVG/sGfPPk/iAHi1V1+K3FCe4jdWpv0DTzJ//lIe6tjds/zjJP58GtSvFf/5eJlv/Xo1Pck3OYoXLxp/v0XzRkRF/RLEaCBjxgxkypQRgDp1qhEdE03U1h0ByStv3twAFC5ckJYt7mTM6InxaSLCiy9149tvRgQk74tJLdeMrTd1YLwPDBWRXsDUpHZQ1T9EZAywEdgFrE1qv+RQ1X3ukKf5gABTVdVf7arPiEgdnFr0JsBv44zy5cvL6NEDgbia6QRmz3bOIdq2bcHo0ZMu9nS/qVq1Ag880IYNG7bED1967fV+zJgxj7b3tGD0mAmexBFq8uXLy49jvwUgPDyMUaMmMHPWAk/yHTN60Ll8R09k1uwFtGhxJx9/1Ie8eXMxYfwQ1q/fTLPmDwQsju+Hf06tmlXIkycXv+5cRe8+H9K4cV1uvLEYsbGx/P77Hp7s6t+e1ADfDv6E6jUqkTt3TjZHLeG9dwZw+PBR3v/wdfLkycWYn75hw/rN3NXqYfLmzc24CUOI1Vj27T3A448+7/d44oz44Uty5crB2bPRPPfs6xw9eownn+zIY493AGDSxBkMHzY2YPlfSIyGegN08kioX9Q2V5YMGa4LiS9UbAj8gaaWdVb9ISxNaDTCxQS5QxxA5nRJXbXyXii8F8dP7pLLPUbtwvWT/Ye2YPecy84vUELjL8QYY4z5D2JVk327FHdY6QZ3KOcqNy2XiMx2h6/OFpGcbrqIyKciskNE1otIuct5HVYYG2OMuWJpCm7JVEdVy6hqefdxT5xRMyWAue5jgMZACffWGWcY6n9mhbExxpgrlgcduFoCcXPSDgVa+aQPU8dyIIeI/OeB1lYYG2OMuWKlpDD2nRPBvXVOdDgFZonIap9t+VQ1bjjFfiBuQvJCwB8+z93tpv0n1pvaGGPMFSslvalVdSAw8CK7VFfVPSJyDTBbRLYmer6KSEB6ZlrN2BhjzBXLn5N+qOoe9/+DwHigInAgrvnZ/f+gu/se4Fqfpxfm3PwSKWaFsTHGmCuWv+amFpHMcQvhiEhmoCHOPBCTgIfc3R4C4uZwmAR0cHtVVwaO+jRnp5g1UxtjjLli+XFmrXzAeBEBp2z8QVVniEgEMEZEOuHMnNjW3X8a0ATYAZwELmuiciuMjTHGXLH8NXGVqu4ESieR/hdJTFfszvHfNXH6f2WFsfGrmNiYS+/kAZv7KrSEwmxPoeLU2dOX3skDaSR1XKWMCfn1mJLHCmNjjDFXrNQy7awVxsYYY65Yob40YnJZYWyMMeaKZTVjY4wxJsisZmyMMcYEmdWMjTHGmCBLyXSYocwKY2OMMVcsa6Y2xhhjgkxTSc04dYz6ToVEpLuIbBGREUlsKy8inwYjLn8qXLggs2eNZd26+URGzuOpbp0AePPNF1izejarImYxbeoPFCiQ7xJHujyDBvZn7+51RK6dG5/W771ebNywkDWrZ/Pj2G/Inj1bQGNITkxeK1y4IHNmjWX9uvms8/l8vJDU6+/t872YHqTvxd13N2Nd5DzO/PsHd5QrFdD8AQoXLsCsmWNYFzmPyLVz6eZ+BiO+/4KIlTOJWDmTbVE/E7FyZsBjiYpayqpVs1ixYjpLl04BoFSpW1i4cEJ8Wvny501gFXAerGfsCfHXVGLGv9ylu+qr6u5E6eGqGh2ksC4pbbpCyf5C5c9/DQXyX8PayI1kyZKZFStm0KbNI+zevY9jx44D0K3rI9x884107dYzRXGk5Ftdo3oljh8/weDBAyhT1pn1rkH9msybv5SYmBjee/cVAF5+5d0UxXA5korJa4k/n5UrZnB3m0fYsmV7wPNO6vVnzZrlsr8XlxvDTTcVJzZW+fLzvrz40lusXrM+xcdN48x9nCz5819D/vzXEBn3N7J8Om3adGLL1nOfQb9+r/HP0WO88+4nKYwjZXWxqKilVK3ajL/+OhyfNmXK93z66TfMmrWARo3q8PzzT9CwYbtkH/Pff39P/ptxAdfluj3Zf+6//73hsvMLFKsZp5CIFBGRrSIyRES2icgIEakvIktFZLuIVHT3qygiP4vIWhFZJiIlkzhWFhGZKyJrRGSDiLR0078CbgCmi8izIvKmiAwXkaXAcBGpLSJTfI4x2H3+ehG5203/0l08e5OI9PbJ81cR6e2T501u+psi0sNnv43ua80sIlNFZJ2blvy/tEvYv/8gayM3AnD8+Am2bt1OwYL5439wATJlzuS3uWcvZPGSFfx9+EiCtNlzFhET40ztuXzFGgoVKhDQGJITk9eS+nwKFczvSd5JvX7f70XmIH0vtm7dwbZtvwQ0X1/79x8kMvHfSKGEn0Gbu5szeszEpJ4ecKpKtmxZAciePSv79h3wPIbUUjO2a8b/TXHgHuARIAK4H6gOtABeAVoBW4EaqhotIvWBd4G7Ex3nX6C1qv4jInmA5SIySVWfEJE7gTqq+qeIvAncgrPw9SkRqe1zjNdwlu66HUBEcrrpr6rq3yISBswVkVKqGnca/6eqlhORJ4EewKMXea13AntVtal7/OwpeJ+S7frrC1Om9G2sXLkWgD59XuKB9m04+s8/NGhwTyCyTLaHO97LmLGTghpDsMV9PivczydY3vL5XtQP8vfCa9dfX5jSPn8jANWrV+LgwUPs2LEr4PmrKlOmfI8qfPvtCL799gd69OjNlCnD6dv3VUTSUKdO64DHkVhqmffcasb/zS5V3aBOz4FNwFx3BY8NQBF3n+zAWBHZCHwM3JrEcQR4V0TWA3OAQjjLeCVlkqqeSiK9PvB53ANVjWtDaisia4C1bt63+DxnnPv/ap94L2QD0EBE+olIDVU9et6LEOns1sJXxcaeuMThzpc5cybGjB7E8z3eiK/9vP56P24oVoGRI8fz5JOXtTLZZXm5Z3eio6P54Ydxl945lYr7fJ7z+XyC5bXX+1HU/V50DeL3wmuZM2di9KiB9OjxZoLPoF27lp7ViuvWvZsqVZrSsmUHHn+8A9WrV6Rz5wd54YU+FC9emRdf7MNXX33gSSy+NAX/QpkVxv+N77IrsT6PYznX2vAWMF9VbwOaAxmSOE57IC9wh6qWAQ5cYD+AZJdyIlIUp8ZbT1VLAVMTHTcu3hifeKNJ+H3IAKCq24ByOIXy2yLyeuL8VHWgqpZX1fJp0mRObpgAhIeHM2b0IEaOHM+ECdPP2z5y5Dhat26SomP6S4cH29K0SX0e7NAtKPmHgvDwcMZe5PMJlh+C+L3wWnh4OKNHD2TkqPFMmHjuMwgLC6NVy8aMHTvZkzj27nWaoA8d+otJk2ZSvnwZHnjg7vjvxU8/BacDl6om+xbKrDAOnOzAHvd+x4vsc1BVz4pIHeD6/5DPbHzW1HSbqbPhFN5HRSQf0DgZx/kVp9BFRMoBRd37BYGTqvo98EHcPv4yaGB/tm7dwScDBsanFS9eNP5+i+aNiIry7hpdnEYNa9OjRxda3dWRU6f+9Tz/UDFoYH+2JPp8giUUvhfBMPDrD9m6dQcDBgxKkF6vXg2ion5hz559AY8hU6aMZMmSOf5+vXo12LQpin37DlCzZmUA6tSpxo4dvwY8lsTsmrG5lPeBoSLSC6dmmpQRwGQR2QCswrnOnFJvA5+7zeExQG9VHScia93j/QEsTcZxfgI6iMgmYAWwzU2/HfhARGKBs0CX/xBjkqpVrcADD7Rhw4bNrIqYBUCv1/ry8MP3cuONxdDYWH77fQ9duwauxyzA98M/p1bNKuTJk4tfd66id58PeenFbqRPn54Z00cBsGLFmoD23E1OTIOHjPIsf3A+nwcfaMN6n8/ntdf6Mn3GvIDnndTrb9y4LjfeWIzY2Fh+/30PTwbhe/H34SMM+Pht8ubNxaSJw1i3bhNNmrUPWAxV4/9GtsQPX3rt9X7MmDGPtve0YPSYCQHL21e+fHkZPdo5IXNq6hOYPXshTz7Zkw8/fJPw8DD+/fd0wP9WkxLqNd7ksqFNxq9SMrQpkEIiCGOSkJKhTYGU0qFNgeCPoU05sxRP9p/74eM7QuPNT4LVjI0xxlyxQr35ObmsMDbGGHPFSi2tu1YYG2OMuWLZEorGGGNMkIX6+OHkssLYGGPMFctqxsYYY0yQxdoSisYYY0xw+XMGLhG5U0SiRGSHiHg6aNpqxsYYY65Y/upN7S6q8znQANgNRLgL92z2SwaXYDVjY4wxVyxNwe0SKgI7VHWnqp4BRgEtAxJ0EqxmbPzq7Jk9lz3DjYh0VtWgToYcCjGEShyhEEOoxBEKMYRKHKEQA0B0Cn5zRKQz0NknaaDPayiEM31wnN1ApcuPMHmsZmxCUedL7xJwoRADhEYcoRADhEYcoRADhEYcoRBDiviuMOfegn4yEccKY2OMMcZZZe9an8eFObfyXsBZYWyMMcZABFBCRIqKSDrgXmCSV5nbNWMTikKh6SgUYoDQiCMUYoDQiCMUYoDQiCMUYvAbVY0WkW7ATCAM+E5VN3mVvy2haIwxxgSZNVMbY4wxQWaFsTHGGBNkVhgbY4wxQWaFsQk6ESkmIund+7VFpLuI5PA4hvpJpD3kZQyhQkTeF5FsIpJWROaKyCEReSAIcdwjIlnd+71EZJyIlPMo72zu/7mSunkRQ6J4MotIGvf+jSLSQkTSehxD0D6Pq4EVxiYU/ATEiEhxnB6a1wI/eBzD6yLypfujl09EJgPNvcpcRJa4/x8TkX98bsdE5B+v4nA1VNV/gGbAr0Bx4AWPYwB4TVWPiUh1oD7wLfClR3nHff9WA6vc/1f7PPbaIiCDiBQCZgEPAkM8jiGYn0eqZ4WxCQWxqhoNtAY+U9UXgAIex1AL+AWIBJYAP6hqG68yV9Xq7v9ZVTWbzy2rqmbzKg5X3JDHpsBYVT3qcf5xYnziGKiqU4F0XmSsqs3c/4uq6g3u/3G3G7yIIRFR1ZPAXcAXqnoPcKvHMQTt87gaWGFsQsFZEbkPeAiY4qZ52gQH5MSZKP4X4DRwvYhc9jzbKSUiw5OTFmBTRGQrcAcwV0TyAv96HAPAHhH5GmgHTHMvZXj+myUihUSkqojUjLt5HYMThlQB2gNT3bQwj2MIic8jtbJxxiboROQW4AngZ1UdKSJFgbaq2s/DGLYBfVX1OxHJCPQDyqtqVa9icONYo6rlfB6HA+tV9RaP48gFHFXVGBHJBGRT1f0ex5AJuBPYoKrbRaQAcLuqzvIwhn44hc9mztUMVVVbeBWDG0ct4Hlgqar2E5EbgGdUtbuHMQT980jNrDA2BhCR61T190RpNVV1kUf5vwy8AmQETgJxtfIzOE2CL3sRhxvLPcAM9/pgL6Ac8LaqrvEqBjeO65JKT/w5BTiGKKCUqp72Ks9QFQqfR2pmhbEJGhEZo6ptRWQDCZcbFZzaRykPY8mEU/O4TlUfE5ESQElVnXKJp/o7jve8LHgvEMN6VS3ldtR5G/gAeF1VPVtOzo0j7nshQAagKBClqp5dKxWR6cA9qnrcqzwT5f+Jqj7jdig878fayxp6KHweqZnNTW2C6Wn3/2ZBjcIxGKenbBX38R5gLOeuYXvlFRG5C6iO88O3WFUneBzDeR11RORtj2NAVW/3fewOo3nS4zBOApEiMhenL0FcbF41D8f1F/jQo/wuKEQ+j1TLasYm6EQkM3BKVWNF5EbgJmC6qp71MIZVqlpeRNaqalk3bZ2qlvYqBjfPL3CGEo10k9oBv6hqVw9jmIJzMtIAp4n6FLDS6/ciKSKyIXGhEOD8khxrrqpDvYohlHn9eaRmVjM2oWARUENEcuKMoYzAKYTaexjDGbfjloIzEQk+NSEP1QVuVvcsWUSGAp6tHONqi9NR50NVPeJ21PF8nLGIPOfzMA1O7+69XsYQKoWuiDQD3gKux/ndjruU49mwtyQ+j3J4/HmkZlYYm1AgqnpSRDrhjKF8X0QiPY7hDWAGcK2IjACqAR09jgFgB3Ad8Jv7+Fo3zTPueNZxPo/3Afu8jMGV1ed+NM4lg5+8DMDtO/AecAvOdVIAgjDW+BOcMcYbNHjNmYk/j6l4/HmkZlYYm1DgO4ayk5vm6RhKVZ0tImuAyji1jqdV9U8vY3BlBbaIyEqcWnpFYJWITHLj9HRITTCpau+4++5UkFlU1evxzoNxTtQ+BuoADxOcsbV/ABuDWBAn+DyM/9k1YxN07iQKPQjCGMpLza0bhOE8tS62XVUXehVLsInIDzjjz2NwLl1kAwao6gcexrBaVe/wvTYal+ZVDG6eFXCaqReSsCPZRx7GcCPO32kRfCpyqlrXqxhSM6sZm6Bzx/Iu8nm8E/Cqt2r/i2xTnGu4XioFfK+qhz3ON14odKhz3aKq/4hIe2A60BOnx7tnhTFw2q2VbxeRbjgd27J4mH+cd4DjOE3lwZqCcizwFfAN53rcGz+xwtgEXTDPuFW1TqDzSKF8QITbZP4dMDMITZOh0KEOIK27MlEr4H+qelZEvH4vngYy4ZwcvoXTVN3B4xgACqrqbUHI11e0qtrCEAFi84qaUDAWWAv0wum1G3fzjIhkEJHn3GXhfhKRZ0Qkw6Wf6V+q2gsogbMiTkecGtm7bu9ur4TCogQAX+OsGpUZWCQi1wNer2BVRFWPq+puVX1YVe/G6WDntWki0jAI+fqaLCJPikgBCeJykqmVXTM2QReMa3BJxDAGOAZ87ybdD+RwC6JgxFMap7PQncB8nI5ls1X1RQ/yXoszmcPHQCdV3RQq40lFJFydFb68yi/BXOEXSvMgjmM4JyWngbMEZ2jTriSSNUirWKU61kxtQsFkEXkSGE/Czil/exjDbYkWY5gvIps9zB8AEXkapxn0T5xrcy+4zbNpgO1AwAtj4BngZWC8WxDfgHNC4CkRyY7TkzlulaSFQB8g4Es6ikhjoAlQSEQ+9dmUDWdYj2fcz/5OVV3qZb6JqWrRYOaf2lnN2ARdKJxxi8j3ONcll7uPKwFdVdXT64Mi0hv4TlV/S2Lbzaq6xct4gklEfgI2AnETbzwIlFbVuzzIuzRQBqfwf91n0zFgvtcd7HxnhgsmEbmN88dcDwteRKmHFcbGACKyBSgJxK1Acx0QhVML8mzRigtcgzvm8dSg80l6UQJPe5aLSKSqlrlUWoBjSBv33rsd2q5V1fVe5e8Tx4fAz8C4YI01FpE3gNo4hfE0oDGwRFXbBCOe1MaaqU3QuSsmPYezYlLnIK2YdKeHeV3MGpxZtw7jXBfMAewXkQPAY6q62oMYevjczwDcjcdNs65TIlJdVZcAiEg1nHmyvTRbRFrg/FauBg6KyDJVfdbjOB7H+RuJEZFTBOGaMdAGKA2sVdWHRSQf5/pYmMtkhbEJBXErJlV1H3u+YpKq/hZX8yHh8CpPJ/0AZgM/qupMALcH7d0479EXQMCXMUyiwF/qzgjmtS7AUPfasQB/4/0Updndsc6PAsNU9Q0R8bxmrKpZL71XwMWNPY8WkWzAQZy/F+MHVhibUFBMVduJyH3gzI0sIuJlACLyFs4P/S+ca6INxqQflVX1sbgHqjpLRD5U1cdFJL0XASRqKo9boCG7F3n7UtVIoLT7w4+qej2sCSDcXSijLfBqEPKP59bQ4zqzLfC45QicaVlzAINwTp6P4zSdGz+wwtiEglBYMaktzknBGY/zTWyfiLwEjHIftwMOiEgYEOtRDKs5t4h8NLCLc3OGe8b94e+AOxlM3PmZF9Ok+ugDzMS5Nhrh9izf7mH+AIhIX6ACMMJNelpEqqnqy17FoKpxaxd/JSIzgGzBuH6eWlkHLhN0ItIAZ8KPW3BmfKoGdFTVBR7G8BPQRVUPepXnBeLIgzOcpzpOgbiUc8N5rlPVgK/gJCIZEi/IICLpVdXTEyQRWQYsBzbgcyKiIbKsoZfcpvEyqhrrPg7DuXbrScdCN8/WwDxVPeo+zgHUVtUJXsWQmllhbEKCiOTm3IpJy9XjFZNEpDwwEWcoje9Y56CskiQimVX1RJDyDpWJLjzPM4kYBpN0z/JHPI5jPU7B97f7OBdOU7WXhXFSvdtDYshVamDN1CZoROQmVd0q51ZOilsz9zoRuc7jzlNDgX4kqoV5TUSq4kz2kQXnfSgNPO7TRBjIvPMDhYCMIlIW58QInIkuMgU6/yQMF5HHcDryBWsyGN/rshmA1sBeD/OP8x6w1h12JjjXjnt6HENS0ydbGeInVjM2QSMiA92hTEnN7qRejmsVkQhVreBVfheJYwXOEJJJcTUOEdnoxSIBIvIQTie28jiLQ8QVxseAIao6LtAxJIqnK85qRUfw6VQXzOkX3dmwlqhq1Uvu7J/8qqnqUrfzXi6c68YAK1V1vxcx+MTyHc5n8bmb1BXIpaodvYwjtbLC2BhARD7CqX1NImEtzOv1jFeoaiXf5j8RWaeqpT2M4W5V/cmr/C4Sx06goteXLC5GREoCU1W1uEf5xa2nHApN9pmB14D6btJs4O1gXU5JbayJwQSdOKsjPcm5TkuLga8SdyIKsLjrXpV90oIxtOkPt6laxVk+8GnA6ykwC7vDiY7hDGMpB/RU1Vkex7EDOOlxngm4CzTE9SxXYD/wkochnBWRgTifyaeJN3rZs9wtdL1uGr9qWGFsQsEwnB/+z9zH9wPDAc9WTNLQWdf4CWAAzrXbPTi9y7t6HMMjqjpARBoBuXHmhB7uxuKlE0CkexnDt7XCywIo2JNtNMOpiTbCGXLmORH5RFWfEZHJJN2ZLSidHFMbK4xNKAj6ikkXWiEobhiHV9wm2fZe5pmEuGvFTXBmndrk9SQsrgnuLaiCOdmG+30YJSJbVHWdV/kmMtz9/8Mg5X9VsMLYhII1IlI50YpJqzyO4TucYU1t3ccP4kxBGfAVgny5TfadgFtJuDKOl0NpVovILKAo8LKIZCUIPcxDYTzxBSbbqKqqr3iU/4uq+j7wqIgkVSsNeCuBqq52xzV3VtVgnyimWlYYm6ARkQ04zV5pgWUi8rv7+Hpgq8fhFFPVu30e9xaRSI9jAKcWshWnWbIPTi3Z62vGnXCWD9zpTk2aG3jY4xhwFwx5j/OX7POyN3UTEk62MRRYC3hSGHPus/f65DQBVY0RketFJF0IzFKXKllhbIKpWbAD8BEKKwQBFFfVe0SkpaoOFZEfcDq0eUlxCsBmOCcEmfEpDD00GOfSwcdAHZwTgqTGugZaDpxFKsDjObpVdbL7f9BbCYCdOIuGTMK5ng+Aqn4UvJBSDyuMTdCo6m++j0XkGoLzow8JVwgCZwnDjkGII27d4iPiLOS+H7jG4xi+wGmWrotTGB8DfuLcGFevZFTVuSIi7nflTRFZDbzuYQyhMNkGInIjztKWRUi4qpiXvf1/cW9pgGB3bEt1rDA2Qed2kOkPFMRZlu16nOa5W72KIURWCAIYKM5Sjr1wxjxnwRnb6aVKqlpORNYCqOphEUnncQwAp91JNraLSDec3uVZvAxAVUeKyALOnYi85PVkG66xwFc4s7PFeJmxiAxX1QeBI6o6wMu8rybBaPIxJrG3cMb3blPVokA9nAUCPCMi74pIDlX9x12/NqeIvO1lDACq+o2qHlbVRap6g6peo6pfexzGWbfDTtwqWnkJzhShT+NMw9kdZxnHB4GHvAzAXRzhpKpOUtVJwL8i0srLGFzRqvqlqq5U1dVxN4/yvkNECgKPuH8XuXxvHsWQ6tkMXCboRGSVqpYXkXVAWXUWMPd61qnzJrwPhVmPgkFE2uMs3VgOZ87uNkAvVR0b1MCCIFQWRxCRN3Fajcbj8TzdItId5zLODTitE77D3II6PWlqYs3UJhQcEZEswCJghIgcxKeDiEfCxGeZQHHWV07vcQwhQVVHuNdm6+H88LZSVa97dONep01qOI+X10lDZXGEuBaBF3zSFKeADChV/RT4VES+VNUugc7vamU1YxN07py3p3B++Nrj9Fgdoap/eRjDS0BznB684PTcneSO8bzquM3U+UjYWeh3j2O4w+dhBuBunObaFz2MwRZHSCRxR0uvvxeplRXGJqjcH/05oTAdpYjcic8k+Ko6MwgxZAKeB65T1cfcsbYlvZz1SUSewhlSdACns5DgNEd6tnbuhYjISlWt6GF+vosjKM7iCO94vTiCiHRIKl1Vh3kYQ3PgIxJ1tFRVzzpapmbWTG2Cyp1MIFZEsns99WQSscwAZgQzBpya+Wqgivt4D05PWs8KY5yOUyW9bJlISqLOQWlwOnF5Pc43VBZH8B1WlgHnEsIanHndvfI2TkfLOapaVkTqAA94mH+qZoWxCQXHgQ0iMpuEkwl4tiBACCmmqu1E5D4AdwYsr+eF/gMI6omRazXnVkyKBnbhzA521VHVp3wfi0gOYJTHYZxV1b9EJI2IpFHV+SLyiccxpFpWGJtQMM69GTjjdh6LG1ZUDJ/esx7ZCSwQkakk7Lnr6UxL7jA3k7QTOHOHeykUOlqmWlYYm6ALkan+4rmTblyrquuDkP0bOE3l14rICKAa3s8E9rt7S+fegkJELrpIh6peNSdwiZYvTIMzXekYj8NoidPR8lnOdbTs43EMqZZ14DJBFwoLArizLLXAOUFdjdNBZamqPudVDD6x5Ma5NifAcncZvauOWzOvCsxzk+oAy4BDOB3KAr6SlTvhyWOcPw2ll6toISK1fB5GA7+p6m4vYzCBZTVjEwpCYUGA7O7MW4/irOH7hogEo2YMzmxXh3BOTG4REVR1kVeZh8j4XnBW87pFVfe5cRUAhqiqlytITcRZqGMOHk9D6UtVFwYrb+MNK4xNKAiFBQHC3R/7tsCrHuabgHsy8DRQGIjEqSH/jLNog1d6+NyPH9/rYf5xro0riF0HgOs8jiGTqr7kcZ7mKmSFsQkFQV8QAOfa10xgiapGiMgNwHaPYwCnIK6A0zxdR0RuAt71MoAk5jxeKiIrvYzBNVdEZgIj3cf34tRQvTRFRJqo6jSP8zVXGbtmbIJORCrgrNKUA2fRiGzAB6rq6WIRoUBEIlS1gohE4qyedFpENnk5scIFxvd+qqolvYrBJ5bWOMsWAixS1fEe538MZz3n0zjLW8ZNgJLNyzhCQSj07UjNrGZsgk5VI9y7x3GuF3suVDrqALvdMaQTgNkichj47aLP8L+QGN/rzn41SVXHi0hJoKSIpFXVs5d6rr+oakis2xsiBWEo9O1ItaxmbAwgIstwOuqsxqejjqr+FMSYauEMH5mhqmc8yO8eVR0rIjeo6s5A55eMeFYDNYCcwBJgFXBGVdt7HEdOoAQJC0HPOtS5MSzhXEHYHLcgVFXP+lWIyGpVvUNENqjq7b5pXsWQmllhbAxJL5V3tYlbMjJUlo70iecpnE5+73v9OV2oQ53XPctDoSB0T1irAz/iDDfbA/QNxuWL1MiaqY1xWEcd+EtEZgFFRWRS4o2q2sLjeEREquBMMBHXTB7mcQxB71DnCoVOjk8DmYDuOH076nJuaUdzmaxmbIIuFK7X+nTUOePerrqOOiKSDigHDAceTbzd67GuIlITZ5jVUlXt5/Zwf8bLOctDoUOdG0fiTo7Zgfevxk6OqZUVxiboQvF67dVMRPKq6qFgxxEKRGQ8zvXZZ3BqgoeBtKraJJhxBYOIlMcZg389CU+ag760ZmpghbEJulC4XuuujNQeKKqqb4nItUABVQ3G+FoTgrzuUJco76AXhCISBbwAbMCZJS4uBq97+6dKVhiboBORt4FlwbxeKyJf4vzA1FXVm90etLNUtcIlnmpMwIVCQSgiS1S1ulf5XW2sMDZBFwoTK/j03F2rqmXdtHWqWtqrGIy5kFAoCEWkHnAfMJeES2teNatnBZL1pjZBFyITK5wVkTDOrSOcF58ayNUkFDrUhVIcIeINEfmG4BaEDwM34SzgEfe3odha5H5hhbEJCSEwscKnwHjgGhF5B2gD9PIw/1ASEisVhVAcoSAUCsIKNqY4cKyZ2gRdCE2scBNQD6eZfK6qbvEy/1ARCh3qQimOUCAiUcEuCEVkMM6c8ZuDGUdqZfOKmlAQN7HCb6paBygLHPEiYxHJ5v6fCziIs0LQD8CBRAsmXE2miEgoDN0JlThCwTIRuSXIMVQGIkUkSkTWi8iGIK75nepYzdgEXTAnVhCRKaraTER2cW5xhDh6Na5IEwod6kIpjlAgIluAYjiLdpzm3Hvh5dCm65NKt6FN/mHXjE0oCNpKRarazP2/qBf5XQlCpENdyMQRIu4MdgBW6AaW1YxNSAnyxAp34UyEr8BiVZ3gZf6hJAQ61IVUHMYEmhXGxgAi8gVQHOeaMUA74BdV7Rq8qIIjhDrUhUQcxnjBCmNjABHZCtys7h+Eu0LOJlW9ObiReU9ENnBupaIycSsVqepdV2McxnjBelMb49gBXOfz+Fo37Wr0r6r+CyAi6VV1KxCMYTWhEocxAWcduIxxZAW2iMhKnGvGFYFVcev6BmEt32AKWoe6EI3DmICzZmpjiO84dkFer+UbKoLZoS4U4zAmUKwwNsYYY4LMmqnNVS1uNRx3ggnfM9OrdoIJY4z3rGZsjDHGBJnVjI1xiUg5zk36sURV1wY5JGPMVcKGNhkDiMjrwFAgN5AHGCIiV+sSisYYj1kztTE4S9QBpX3GtWYEIoO9bJ0x5upgNWNjHHvxmf8YSA/sCVIsxpirjNWMjQFEZALO1Iuzca4ZNwBWArsBVLV70IIzxqR6VhgbA4jIQxfbrqpDvYrFGHP1scLYGGOMCTK7ZmyMMcYEmRXGxhhjTJBZYWwMICIZkkjLE4xYjDFXHyuMjXFEiEjluAcicjewLIjxGGOuIjYdpjGO+4HvRGQBUBBnJq66QY3IGHPVsN7UxrhEpBUwHDgG1FTVHcGNyBhztbCasTGAiHwLFANKATcCU0TkM1X9PLiRGWOuBnbN2BjHBqCOqu5S1ZlAJaBckGMyxlwlrJnaGJeIXA+UUNU57kIR4ap6LNhxGWNSP6sZGwOIyGPAj8DXblJhYELQAjLGXFWsMDbG0RWoBvwDoKrbgWuCGpEx5qphhbExjtOqeibugYiE46zeZIwxAWeFsTGOhSLyCpBRRBoAY4HJQY7JGHOVsA5cxgAikgboBDQEBJgJfKP2B2KM8YAVxsYYY0yQ2aQf5qomIhu4yLVhVS3lYTjGmKuU1YzNVc0dW3xBqvqbV7EYY65eVhgbY4wxQWbN1MYAInKMc83V6YC0wAlVzRa8qIwxVwsrjI0BVDVr3H0REaAlUPnCzzDGGP+xZmpjLkBE1qpq2WDHYYxJ/axmbAwgInf5PEwDlAf+DVI4xpirjBXGxjia+9yPBn7Faao2xpiAs2ZqY4wxJshsbmpjABEZKiI5fB7nFJHvghiSMeYqYoWxMY5Sqnok7oGqHgas85YxxhNWGBvjSCMiOeMeiEgurE+FMcYj9mNjjKM/8LOIjHUf3wO8E8R4jDFXEevAZYxLRG4B6roP56nq5mDGY4y5elhhbIwxxgSZXTM2xhhjgswKY2OMMSbIrDA2xhhjgswKY2OMMSbI/g8C1RjUKsXZegAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.07883973, Train f1: 0.95799037, Val Loss: 0.01126886, Val f1: 0.61618410, overrun_counter -1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5618fedcab0b4781a0a47910e3afd933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1batch = 0 of 504duraation = 0.054430643717447914\n",
      "epoch = 1batch = 200 of 504duraation = 1.9009079972902934\n",
      "epoch = 1batch = 400 of 504duraation = 3.7100094318389893\n",
      "..Overrun....no improvement\n",
      "Epoch: 1, Train Loss: 0.08012459, Train f1: 0.95671829, Val Loss: 0.01226928, Val f1: 0.59656709, overrun_counter 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a04a7f47304d35b728adfe2c002758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 2batch = 0 of 504duraation = 0.0605602224667867\n",
      "epoch = 2batch = 200 of 504duraation = 1.8975269079208374\n",
      "epoch = 2batch = 400 of 504duraation = 3.704643023014069\n",
      "..Overrun....no improvement\n",
      "Epoch: 2, Train Loss: 0.11191856, Train f1: 0.94389869, Val Loss: 0.01122064, Val f1: 0.60121544, overrun_counter 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78a9be28cce4b2f910ee1611124f192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 3batch = 0 of 504duraation = 0.05736488103866577\n"
     ]
    }
   ],
   "source": [
    "model =Model('convnext_small',224)\n",
    "filepath = \"../../models/model_e15_2022_09_30_03_47_23.pth\"\n",
    "model_epcoh_15 = load_model(filepath,model)\n",
    "model, lr_log = train_model(train_loader, val_loader, test_loader,model_epcoh_15, classes ,class_weights ,num_epochs = num_epochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a400be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x,b = torchaudio.load(\"../../data/audio/221529.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3df68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_new = MozTestDataset(df_val_offset,  config.data_dir, min_length)\n",
    "val_loader_new = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=2,\n",
    "        num_workers=0, pin_memory=pin_memory  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca1b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_iter = iter(val_loader_new)\n",
    "x1,y1 = val_iter.next()\n",
    "print(x1.shape)\n",
    "model = model_epcoh_10\n",
    "model.to('cuda')\n",
    "x_g = x1.to('cuda')\n",
    "model(x_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94583aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error = df_val_offset\n",
    "model = model_epcoh_10\n",
    "model.to('cuda')\n",
    "model.eval()\n",
    "for idx,(x,y) in enumerate(val_dataset):\n",
    "    print(idx)\n",
    "    print(y)\n",
    "    x = x.to('cuda').float()\n",
    "    print(\"x shape = \" +str(x.shape))\n",
    "    #x_new = x.unsqueeze(dim = 1)\n",
    "    print(\"x_new shape = \" +str(x_new.shape))\n",
    "    x_new = x.to('cuda')\n",
    "    y_pred = model(x_new)['prediction']\n",
    "    y_pred_cpu = y_pred.cpu().detach()\n",
    "    preds = torch.argmax(y_pred_cpu, axis = 1)\n",
    "    df_erroriloc[idx]['y_hat'] = preds\n",
    "    del x_new\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada8d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1,15360)\n",
    "x = x.unsqueeze(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b7dcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_offset.head()\n",
    "path_temp = \"../data/audio/\"\n",
    "for i,row in df_val_offset.iterrows():\n",
    "    print(\"i = \" +str(i))\n",
    "    print(\"id = \" + str(int(row['id'])))\n",
    "    file = str(int(row['id']))+\".wav\"\n",
    "    print(file)\n",
    "    path = path_temp + file\n",
    "    waveform, inp_rate = torchaudio.load(path)\n",
    "    if inp_rate != config.rate:\n",
    "        import torchaudio.transforms as T\n",
    "        resampler = T.Resample(inp_rate, config.rate, dtype=waveform.dtype)\n",
    "        waveform = resampler(waveform)\n",
    "    if waveform.shape[1] < config.rate*min_length:\n",
    "        #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "        f_out = pad_mean(waveform)\n",
    "    else:\n",
    "        f = waveform[0]\n",
    "        f_out = f.unsqueeze(0)\n",
    "    \n",
    "    \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff48f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor(df):\n",
    "    \n",
    "    path_name = \"../data/audio/\"\n",
    "    file = df.loc[idx]['id'])}.wav\")\n",
    "    waveform, inp_rate = torchaudio.load(path)\n",
    "        \n",
    "        if inp_rate != config.rate:\n",
    "            import torchaudio.transforms as T\n",
    "            resampler = T.Resample(inp_rate, config.rate, dtype=waveform.dtype)\n",
    "            waveform = resampler(waveform)\n",
    "    \n",
    "        \n",
    "        #waveform, rate = torchaudio.load(path)\n",
    "                \n",
    "        if waveform.shape[1] < config.rate*self.min_length:\n",
    "            #r = math.ceil((config.rate*self.min_length)/waveform.shape[1])\n",
    "            f_out = pad_mean(waveform)\n",
    "        else:\n",
    "            f = waveform[0]\n",
    "            f_out = f.unsqueeze(0)\n",
    "            \n",
    "        return f_out\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #real_idx = idx % len(self.audio_df)\n",
    "        if DEBUG:\n",
    "            print(\"\")\n",
    "            print(\"idx = \" + str(idx))\n",
    "        x = self._get_sample_(os.path.join(self.data_dir,f\"{int(self.audio_df.loc[idx]['id'])}.wav\"), resample=config.rate)\n",
    "        \n",
    "        \n",
    "        if DEBUG:\n",
    "            print(\"shape of x post augmentation = \" + str(x.shape))\n",
    "            \n",
    "        \n",
    "        # random noise on even number indexes\n",
    "        offset = int(self.audio_df.loc[idx]['offset'])\n",
    "        if DEBUG:\n",
    "            print(\"returning x of shape ...\" + str(x[:,offset:int(offset+config.rate*self.min_length)].shape))\n",
    "        \n",
    "        return (x[:,offset:int(offset+config.rate*self.min_length)],self.audio_df.loc[idx]['specie_ind'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db486d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take the model checkpoint as a parameter as input\n",
    "# read the val df\n",
    "#get the tensor rep for the offset.\n",
    "#pass it to the model get add get the prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8670ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "pred = []\n",
    "for i in range(10):\n",
    "    label.append(np.random.rand(9))\n",
    "    pred.append(np.random.rand(9))\n",
    "print(label)\n",
    "print(pred)\n",
    "print(classification_report(label, pred, target_names= classes, labels= classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca9ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.tensor(8, device = \"cuda\")\n",
    "print(label)\n",
    "label_cpu = label.cpu().detach()\n",
    "print(label_cpu)\n",
    "label_np = label_cpu.numpy()\n",
    "print(type(label_np))\n",
    "label_np_item = label_np.item()\n",
    "print(type(label_np_item))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352a8719",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.randn(4,9)\n",
    "y_pred.shape\n",
    "#y_pred_np = y_pred.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada2018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_np\n",
    "# y_pred_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c3be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.argmax(y_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84276b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39079b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc25a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,(x,y) in enumerate(test_loader):\n",
    "    print(\"idx = \" + str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4246698d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a585f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ccef7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
